---
title: "Fitting Model"
author: "Hope Grismer"
date: "2025-02-28"
output: html_document
editor: 
  markdown: 
    wrap: 72
---

## FITTING EXERCISE !!!!

## Prompt #1

"Write code to load the data into R. Then write code to make a plot that
shows a line for each individual, with DV on the y-axis and time on the
x-axis. Stratify by dose (e.g., use a different color for each dose, or
facets).

Write code that keeps only observations with OCC = 1.

Write code to exclude the observations with TIME = 0, then compute the
sum of the DV variable for each individual using dplyr::summarize().
Call this variable Y. The result from this step should be a data
frame/tibble of size 120 x 2, one column for the ID one for the variable
Y. Next, create a data frame that contains only the observations where
TIME == 0. This should be a tibble of size 120 x 17. Finally, use the
appropriate join function to combine those two data frames, to get a
data frame of size 120 x 18.

Write code that converts RACE and SEX to factor variables and keeps only
these variables: Y,DOSE,AGE,SEX,RACE,WT,HT"

"Make some useful summary tables. Show some scatterplots or boxplots
between the main outcome of interest (total drug, Y) and other
predictors. Plot the distributions of your variables to make sure they
all make sense. Look at some pair/correlation plots."

```{r}
# Install and load necessary packages
library(tidyverse)
# Or manually specify a mirror
options(repos = c(CRAN = "https://cloud.r-project.org/"))
# Now try installing the package
install.packages("GGally")
library(GGally)
library(skimr)

# Load the Mavoglurant_A2121_nmpk.csv data
df <- read.csv(here::here("fitting-exercise", "Mavoglurant_A2121_nmpk.csv"))

# Initial exploration
glimpse(df)
skim(df)

# Plot the outcome variable DV (drug concentration) as a function of time, stratified by DOSE and using ID as a grouping factor
p <- df %>%
  ggplot(aes(x = TIME , y = DV, group = ID, color = as.factor(DOSE))) +  # DOSE is used for stratification by dose
  geom_line(alpha = 0.6) +
  labs(
    x = "Time (hours)", 
    y = "Drug Concentration (ng/mL)", 
    color = "Dose (mg)"
  ) +
  theme_minimal() +
  theme(
    axis.title.x = element_text(size = 12, face = "bold"),
    axis.title.y = element_text(size = 12, face = "bold"),
    legend.title = element_text(size = 10),
    legend.text = element_text(size = 9)
  )
print(p)

# Keep only observations where OCC == 1 (remove OCC == 2)
df_filtered <- df %>%
  filter(OCC == 1)

# Compute total drug amount for each individual (sum of DV values), excluding TIME == 0
drug_sums <- df_filtered %>%
  filter(TIME != 0) %>%
  group_by(ID) %>%
  summarize(Y = sum(DV, na.rm = TRUE))

# Create a data frame with only observations where TIME == 0 (dosing information)
dose_info <- df_filtered %>%
  filter(TIME == 0)

# Join the two data frames: one with drug sums and the other with dosing info
df_final <- left_join(dose_info, drug_sums, by = "ID")

# Select only relevant variables and convert RACE and SEX to factors
df_cleaned <- df_final %>%
  select(Y, DOSE, AGE, SEX, RACE, WT, HT) %>%
  mutate(
    SEX = as.factor(SEX),
    RACE = as.factor(RACE)
  )

# Final check of the cleaned data
glimpse(df_cleaned)
skim(df_cleaned)

# Summary table by DOSE
summary_table <- df_cleaned %>%
  group_by(DOSE) %>%
  summarize(
    Mean_Y = mean(Y, na.rm = TRUE),
    Median_Y = median(Y, na.rm = TRUE),
    SD_Y = sd(Y, na.rm = TRUE),
    Count = n()
  )
print(summary_table)

# Boxplots of Y vs predictors with improved labels
ggplot(df_cleaned, aes(x = as.factor(DOSE), y = Y, fill = as.factor(DOSE))) +
  geom_boxplot() +
  labs(
    x = "Dose (mg)", 
    y = "Total Drug (Mavoglurant) - Concentration (ng/mL)", 
    fill = "Dose (mg)"
  ) +
  theme_minimal() +
  theme(
    axis.title.x = element_text(size = 12, face = "bold"),
    axis.title.y = element_text(size = 12, face = "bold"),
    legend.title = element_text(size = 10),
    legend.text = element_text(size = 9)
  )

ggplot(df_cleaned, aes(x = as.factor(SEX), y = Y, fill = as.factor(SEX))) +
  geom_boxplot() +
  labs(
    x = "Sex", 
    y = "Total Drug (Mavoglurant) - Concentration (ng/mL)", 
    fill = "Sex"
  ) +
  theme_minimal() +
  theme(
    axis.title.x = element_text(size = 12, face = "bold"),
    axis.title.y = element_text(size = 12, face = "bold"),
    legend.title = element_text(size = 10),
    legend.text = element_text(size = 9)
  )

# Scatterplots with regression lines
scatter_1 <- ggplot(df_cleaned, aes(x = AGE, y = Y)) +
  geom_point(alpha = 0.6) +
  geom_smooth(method = "lm", se = FALSE, color = "blue") +
  labs(
    x = "Age (years)", 
    y = "Total Drug (Y) - Concentration (ng/mL)"
  ) +
  theme_minimal() +
  theme(
    axis.title.x = element_text(size = 12, face = "bold"),
    axis.title.y = element_text(size = 12, face = "bold")
  )
print(scatter_1)

scatter_2 <- ggplot(df_cleaned, aes(x = WT, y = Y)) +
  geom_point(alpha = 0.6) +
  geom_smooth(method = "lm", se = FALSE, color = "blue") +
  labs(
    x = "Weight (kg)", 
    y = "Total Drug (Mavoglurant) - Concentration (ng/mL)"
  ) +
  theme_minimal() +
  theme(
    axis.title.x = element_text(size = 12, face = "bold"),
    axis.title.y = element_text(size = 12, face = "bold")
  )
print(scatter_2)

# Distribution plots
ggplot(df_cleaned, aes(x = Y)) +
  geom_histogram(bins = 30, fill = "blue", alpha = 0.7) +
  labs(
    x = "Total Drug (Mavoglurant) - Concentration (ng/mL)", 
    y = "Count"
  ) +
  theme_minimal() +
  theme(
    axis.title.x = element_text(size = 12, face = "bold"),
    axis.title.y = element_text(size = 12, face = "bold")
  )

ggplot(df_cleaned, aes(x = AGE)) +
  geom_histogram(bins = 30, fill = "green", alpha = 0.7) +
  labs(
    x = "Age (years)", 
    y = "Count"
  ) +
  theme_minimal() +
  theme(
    axis.title.x = element_text(size = 12, face = "bold"),
    axis.title.y = element_text(size = 12, face = "bold")
  )

# Correlation plot with adjusted axis labels
cor_plot <- ggpairs(df_cleaned %>% select(Y, DOSE, AGE, WT)) +
  theme_minimal() +
  theme(
    axis.title.x = element_text(size = 12, face = "bold"),
    axis.title.y = element_text(size = 12, face = "bold")
  )
print(cor_plot)

```

## Prompt #2

Prompts - "Fit a linear model to the continuous outcome (Y) using the
main predictor of interest, which we’ll assume here to be DOSE. Fit a
linear model to the continuous outcome (Y) using all predictors. For
both models, compute RMSE and R-squared and print them."

"Fit a logistic model to the categorical/binary outcome (SEX) using the
main predictor of interest, which we’ll again assume here to be DOSE.
Fit a logistic model to SEX using all predictors. For both models,
compute accuracy and ROC-AUC and print them."

```{r}

# Install pROC package 
install.packages("pROC")
# Load the pROC package
library(pROC)
# Load the necessary package
library(tidymodels)

# Load the data
df <- read.csv(here::here("fitting-exercise", "Mavoglurant_A2121_nmpk.csv"))

# Data Preprocessing - Remove OCC = 2, and convert necessary columns to factors
df_cleaned <- df %>%
  filter(OCC == 1) %>%
  mutate(
    SEX = as.factor(SEX),
    RACE = as.factor(RACE)
  )

# Keep only necessary variables: WT (weight), DOSE, SEX, AGE, RACE, WT, HT
df_cleaned <- df_cleaned %>%
  select(WT, DOSE, SEX, AGE, RACE, HT)

# 1. Fit a linear model to the continuous outcome (WT) using the main predictor DOSE
lm_dose_spec <- linear_reg() %>%
  set_engine("lm")

lm_dose_recipe <- recipe(WT ~ DOSE, data = df_cleaned)

lm_dose_workflow <- workflow() %>%
  add_recipe(lm_dose_recipe) %>%
  add_model(lm_dose_spec)

lm_dose_fit <- lm_dose_workflow %>%
  fit(data = df_cleaned)

# 2. Fit a linear model to the continuous outcome (WT) using all predictors
lm_all_spec <- linear_reg() %>%
  set_engine("lm")

lm_all_recipe <- recipe(WT ~ DOSE + AGE + SEX + RACE + HT, data = df_cleaned)

lm_all_workflow <- workflow() %>%
  add_recipe(lm_all_recipe) %>%
  add_model(lm_all_spec)

lm_all_fit <- lm_all_workflow %>%
  fit(data = df_cleaned)

# 3. Compute RMSE and R-squared for both linear models

# Extract predictions from the model predictions
lm_dose_pred <- predict(lm_dose_fit, df_cleaned) %>% pull(.pred)
lm_all_pred <- predict(lm_all_fit, df_cleaned) %>% pull(.pred)

# Calculate RMSE and R-squared for both models using yardstick functions
lm_dose_rmse <- lm_dose_pred %>%
  tibble(pred = ., truth = df_cleaned$WT) %>%
  rmse(truth = truth, estimate = pred)

lm_all_rmse <- lm_all_pred %>%
  tibble(pred = ., truth = df_cleaned$WT) %>%
  rmse(truth = truth, estimate = pred)

lm_dose_r2 <- lm_dose_pred %>%
  tibble(pred = ., truth = df_cleaned$WT) %>%
  rsq(truth = truth, estimate = pred)

lm_all_r2 <- lm_all_pred %>%
  tibble(pred = ., truth = df_cleaned$WT) %>%
  rsq(truth = truth, estimate = pred)

# Print RMSE and R-squared
cat("Linear model (DOSE):\n")
cat("RMSE:", lm_dose_rmse$.estimate, "\n")
cat("R-squared:", lm_dose_r2$.estimate, "\n\n")

cat("Linear model (all predictors):\n")
cat("RMSE:", lm_all_rmse$.estimate, "\n")
cat("R-squared:", lm_all_r2$.estimate, "\n\n")


# 4. Fit a logistic model to the categorical/binary outcome (SEX) using the main predictor DOSE
logit_dose_spec <- logistic_reg() %>%
  set_engine("glm")

logit_dose_recipe <- recipe(SEX ~ DOSE, data = df_cleaned)

logit_dose_workflow <- workflow() %>%
  add_recipe(logit_dose_recipe) %>%
  add_model(logit_dose_spec)

logit_dose_fit <- logit_dose_workflow %>%
  fit(data = df_cleaned)

# 5. Fit a logistic model to SEX using all predictors
logit_all_spec <- logistic_reg() %>%
  set_engine("glm")

logit_all_recipe <- recipe(SEX ~ DOSE + AGE + RACE + HT, data = df_cleaned)

logit_all_workflow <- workflow() %>%
  add_recipe(logit_all_recipe) %>%
  add_model(logit_all_spec)

logit_all_fit <- logit_all_workflow %>%
  fit(data = df_cleaned)

# 6. Compute accuracy and ROC-AUC for both logistic models
logit_dose_pred <- predict(logit_dose_fit, df_cleaned, type = "prob")
logit_all_pred <- predict(logit_all_fit, df_cleaned, type = "prob")

# Calculate accuracy for both models
logit_dose_pred_class <- ifelse(logit_dose_pred$.pred_1 > 0.5, 1, 0)
logit_all_pred_class <- ifelse(logit_all_pred$.pred_1 > 0.5, 1, 0)

logit_dose_accuracy <- mean(logit_dose_pred_class == df_cleaned$SEX)
logit_all_accuracy <- mean(logit_all_pred_class == df_cleaned$SEX)

# Calculate ROC-AUC for both models
logit_dose_roc <- roc(df_cleaned$SEX, logit_dose_pred$.pred_1)
logit_all_roc <- roc(df_cleaned$SEX, logit_all_pred$.pred_1)

# Print accuracy and ROC-AUC
cat("Logistic model (DOSE):\n")
cat("Accuracy:", logit_dose_accuracy, "\n")
cat("ROC-AUC:", auc(logit_dose_roc), "\n\n")

cat("Logistic model (all predictors):\n")
cat("Accuracy:", logit_all_accuracy, "\n")
cat("ROC-AUC:", auc(logit_all_roc), "\n\n")

```

## What Do All the Models Mean Above

Linear model (DOSE as the main predictor): RMSE (Root Mean Squared
Error): 12.36 This indicates that the model’s predictions are, on
average, about 12.36 units away from the actual values of Y (total drug
concentration). A higher RMSE suggests that the model is not doing a
very good job of predicting the outcome. R-squared: 0.0100 This value is
very low, meaning that the model explains only about 1% of the variance
in the outcome (Y). This suggests that DOSE alone is not a strong
predictor of the total drug concentration (Y). A very low R-squared
indicates that other factors likely have more influence on the outcome.

Linear model (all predictors): RMSE: 8.68 The RMSE has decreased
significantly compared to the previous model, indicating an improvement
in the model's predictive performance when considering all the
predictors. The model is now, on average, about 8.68 units away from the
actual values of Y. R-squared: 0.5114 This is a substantial improvement
over the previous model, suggesting that the model now explains about
51% of the variance in the total drug concentration (Y). The inclusion
of additional predictors has clearly helped the model improve its
accuracy and explanation of the variability in Y.

Logistic model (DOSE as the main predictor): Accuracy: 0.8643 This means
the model correctly predicted SEX (male or female) about 86.43% of the
time when using DOSE as the only predictor. This is a relatively high
accuracy, suggesting that DOSE has a reasonable relationship with SEX,
but accuracy alone doesn’t tell the full story. ROC-AUC: 0.5854 The
ROC-AUC value is relatively low (just above 0.5), suggesting that the
model does not do a very good job at distinguishing between the two
classes (SEX = male or female) based on DOSE alone. A value closer to 1
would indicate excellent discrimination, while a value around 0.5
suggests random guessing.

Logistic model (all predictors): Accuracy: 0.8390 This accuracy is still
quite good, though slightly lower than the model with DOSE alone. It
means the model is still fairly reliable at predicting SEX based on the
predictors. ROC-AUC: 0.9742 This is a very high value, indicating that
the logistic model with all predictors does an excellent job at
distinguishing between the two classes (SEX = male or female). This
suggests that the combination of predictors (including DOSE, AGE, etc.)
is highly effective at discriminating between male and female
participants.

## EXERCISE 10 !!!

## 1. Data Wrangling and Setup

-   Remove RACE from the dataset.

-   Keep only Y, DOSE, AGE, SEX, WT, and HT.

-   Ensure there are 120 rows left.

-   Set the random seed at the beginning (rngseed = 1234).

```{r}
# Load required libraries
library(tidymodels)
library(dplyr)
library(rsample)

# Set seed for reproducibility
rngseed <- 1234
set.seed(rngseed)

# Load the data
data <- read.csv(here::here("fitting-exercise", "Mavoglurant_A2121_nmpk.csv"))

# Remove RACE and keep relevant variables
cleaned_data <- data %>%
  select(DV, DOSE, AGE, SEX, WT, HT) %>%
  na.omit() %>%
  rename(Y = DV) # Rename DV to Y for consistency with the instructions

# Check dimensions (should be 120 rows, 6 columns)
dim(cleaned_data)
```

[**What happened in this step above (steps noted for my benefit/future
benefit):**]{.underline}

**select(DV, DOSE, AGE, SEX, WT, HT)**

-   This selects only the relevant variables from the dataset.

-   `DV` is the dependent (outcome) variable we’re trying to predict.

-   The other five (`DOSE`, `AGE`, `SEX`, `WT`, `HT`) are predictors
    (independent variables).

<!-- -->

-   **na.omit()**

    -   This removes rows with any missing (`NA`) values.

    -   Missing data can mess up model fitting, so it's common to drop
        them unless you plan to impute (fill in) the missing values.

-   **rename(Y = DV)**

    -   Renames the `DV` column to `Y` for consistency with the
        instructions.

    -   This way, the outcome variable is consistently
        called `Y` throughout the analysis.

## 2. Train vs. Test Data Split

-   Use initial_split() from rsample (75% train, 25% test).

-   Confirm the split size matches expectations (\~90 training, \~30
    testing).

```{r}
# Split into 75% training and 25% testing
set.seed(rngseed) # Reset seed before sampling
data_split <- initial_split(cleaned_data, prop = 0.75)
train_data <- training(data_split)
test_data <- testing(data_split)

# Check split sizes
dim(train_data) # ~90 rows
dim(test_data)  # ~30 rows
```

[**What happened in this step above (steps noted for my benefit/future
benefit):**]{.underline}

-   **set.seed(rngseed)**

    -   Ensures that the random split is reproducible.

    -   If you set the same seed and run the code again, you'll get the
        exact same split — useful for debugging and reproducibility.

-   **initial_split(cleaned_data, prop = 0.75)**

    -   Randomly splits the data into:

        -   75% training data (used to fit the model)

        -   25% test data (held back to evaluate the model later)

    -   This mimics a real-world scenario where you'd train a model on
        existing data and test it on unseen data.

-   **training(data_split)** and **testing(data_split)**

    -   Extracts the training and test datasets from the split.

## 3. Modeling Fitting

-   Fit two linear models using `tidymodels::linear_reg()`:

    -   Model 1: `Y ~ DOSE`

    -   Model 2: `Y ~ DOSE + AGE + SEX + WT + HT`

-   Use `metric = RMSE`.

```{r}
# Define the model specifications
model_spec1 <- linear_reg() %>%
  set_engine("lm") %>%
  set_mode("regression")

model_spec2 <- linear_reg() %>%
  set_engine("lm") %>%
  set_mode("regression")

# Fit the models on the training set
fit1 <- model_spec1 %>%
  fit(Y ~ DOSE, data = train_data)

fit2 <- model_spec2 %>%
  fit(Y ~ DOSE + AGE + SEX + WT + HT, data = train_data)

```

[**What happened in this step above (steps noted for my benefit/future
benefit):**]{.underline}

-   **linear_reg()**

    -   Creates a linear regression model.

    -   Linear regression estimates the relationship between the
        predictors and the outcome using the formula:

    Y=β0+β1⋅DOSE+β2⋅AGE+…+ϵY=β0​+β1​⋅DOSE+β2​⋅AGE+…+ϵ

    where:

    -   YY = predicted outcome

    -   β0β0​ = intercept

    -   β1,β2,…β1​,β2​,… = regression coefficients (how much each
        predictor contributes)

    -   ϵϵ = error term (difference between actual and predicted values)

-   **set_engine("lm")**

    -   Specifies that the model should be fitted using the base
        R `lm()` function (ordinary least squares).

-   **fit(Y \~ DOSE, data = train_data)**

    -   `Y ~ DOSE` fits a simple linear regression with only DOSE as the
        predictor.

    -   The second model adds all predictors (`AGE`, `SEX`, `WT`, `HT`)
        for a more complex model.

## 4. Model Performance on Training Set

-   Compute RMSE for both models on training data.

-   Compute RMSE for a null model using `tidymodels::null_model()`.

-   Compare RMSE values.

```{r}
# Predict on training data
pred1 <- predict(fit1, train_data) %>%
  bind_cols(train_data)

pred2 <- predict(fit2, train_data) %>%
  bind_cols(train_data)

# Compute RMSE
rmse1 <- rmse(pred1, truth = Y, estimate = .pred)
rmse2 <- rmse(pred2, truth = Y, estimate = .pred)

# Null model
null_mod <- null_model(mode = "regression") %>%
  set_engine("parsnip") %>%
  fit(Y ~ 1, data = train_data)

# Predict with null model
null_pred <- predict(null_mod, train_data) %>%
  bind_cols(train_data)

# RMSE for null model
null_rmse <- rmse(null_pred, truth = Y, estimate = .pred)

# Compare RMSE values
rmse1$.estimate
rmse2$.estimate
null_rmse$.estimate
```

| Model | RMSE Value | Interpretation |
|-------|------------|----------------|

|  |  |  |
|------------------------|------------------------|------------------------|
| **Model 1** (Only `DOSE` as predictor) | `222.56` | The model using only `DOSE` explains some variation but is not particularly accurate. |

|  |  |  |
|------------------------|------------------------|------------------------|
| **Model 2** (All predictors) | `221.90` | Slightly better than Model 1 — adding more predictors improves the model’s accuracy a bit. |

|  |  |  |
|------------------------|------------------------|------------------------|
| **Null Model** (Mean-only) | `228.95` | This is the benchmark — it reflects the RMSE you’d get if you just predicted the mean outcome for all cases. |

[**Minimal Improvement from Adding Predictors**]{.underline}

-   Model 2 (all predictors) has an RMSE of **221.90**, which is
    only **0.66** lower than Model 1 (DOSE-only) at **222.56**.

-   This small improvement suggests that adding `AGE`, `SEX`, `WT`,
    and `HT` does **not meaningfully improve predictive
    accuracy** beyond using `DOSE` alone.

-   It implies that **DOSE** is the primary driver of the outcome (`Y`),
    and the other predictors might not have a strong relationship
    with `Y` or might introduce noise.

[**Predictive Performance Close to Null Model**]{.underline}

-   The null model (mean-only) RMSE is **228.95**, which is **just
    slightly worse** than the fitted models.

-   The fact that the RMSE values for the fitted models are so close to
    the null model suggests that the predictors **don’t explain much
    variance** in the outcome.

-   This hints that the relationship between the predictors and `Y` is
    either **weak**, **nonlinear**, or **confounded** by unmeasured
    factors.

[**Model Complexity Does Not Improve Performance**]{.underline}

-   The fact that Model 2 (more complex) and Model 1 (simpler) have
    almost identical RMSE values means that adding more predictors **is
    not adding meaningful information.**

[**What happened in this step above (steps noted for my benefit/future
benefit):**]{.underline}

-   The null model predicts the **mean** of `Y` for every observation.

-   If the null model’s RMSE is high, it means the data is hard to
    predict.

-   If the null model's RMSE is close to the other models, the
    predictors aren’t adding much value.

<!-- -->

-   **predict(new_data = train_data)**

    -   Predicts `Y` values on the training data using the model.

-   **bind_cols(train_data)**

    -   Merges predicted values with the original data.

-   **metrics(truth = Y, estimate = .pred)**

    -   Computes performance metrics (like RMSE).

    -   **RMSE (Root Mean Squared Error)** measures how far the
        predicted values are from the actual values.

## 5. Cross Validation

-   Use 10-fold cross-validation (`vfold_cv()`).

-   Compute RMSE for both models.

-   Compare with training RMSE values.

-   Look at the standard error for RMSE.

```{r}
set.seed(rngseed) # Reset seed before CV

# Create 10-fold cross-validation
cv_folds <- vfold_cv(train_data, v = 10)

# Define workflows
wf1 <- workflow() %>%
  add_model(model_spec1) %>%
  add_formula(Y ~ DOSE)

wf2 <- workflow() %>%
  add_model(model_spec2) %>%
  add_formula(Y ~ DOSE + AGE + SEX + WT + HT)

# Fit and evaluate with CV
cv_results1 <- fit_resamples(
  wf1,
  resamples = cv_folds,
  metrics = metric_set(rmse)
)

cv_results2 <- fit_resamples(
  wf2,
  resamples = cv_folds,
  metrics = metric_set(rmse)
)

# Get average RMSE and standard error
collect_metrics(cv_results1)
collect_metrics(cv_results2)

```

[**Low Variability in Performance with Cross Validation**]{.underline}

-   The standard error from the cross-validation is **\~3.47**,
    indicating that the model’s performance is **consistent** across
    different data splits.

-   This suggests that the model is **stable** and **not overfitting** —
    even though predictive power is weak, the model generalizes well
    across different subsets of the data.

[**What happened in this step above (steps noted for my benefit/future
benefit):**]{.underline}

-   **vfold_cv(train_data, v = 10)**

    -   Creates 10 folds for cross-validation.

    -   The model is trained on 9 folds and evaluated on the 10th —
        repeated 10 times.

-   **fit_resamples()**

    -   Trains the model and computes RMSE for each fold.

-   **metric_set(rmse)**

    -   Specifies that RMSE should be computed.

-   **Interpretation:**

    -   Average RMSE across folds = general model performance.

    -   Lower variability between folds = more consistent model.

## 6. Robustness Check

-   Change the seed.

-   Re-run CV to check variability.

-   Optionally try repeated CV (`repeats = 5`) for more robust results.

```{r}
# Try with a different seed
set.seed(5678) 

cv_results1_new <- fit_resamples(
  wf1,
  resamples = cv_folds,
  metrics = metric_set(rmse)
)

cv_results2_new <- fit_resamples(
  wf2,
  resamples = cv_folds,
  metrics = metric_set(rmse)
)

# Compare old vs new RMSE estimates
collect_metrics(cv_results1_new)
collect_metrics(cv_results2_new)

```

### **Takeaways:**

-   Lower RMSE → better predictive accuracy

-   Adding predictors → reduces RMSE

    -   The fact that both models with predictors are only modestly
        better than the null model means that the predictors might not
        be very strong, 

-   Cross-validation RMSE should be close to training RMSE if the model
    generalizes well

-   High cross-validation variability → model might not generalize well

# This section added by Vincent Nguyen

## Part 2

### Model Predictions

This section goes through the Model Predictions section. This section
assesses the predicted values for each model and also plots it.
Residuals are also computed for further analysis.

```{r}

# Create a data frame with observed and predicted values for each model
predictions_df <- bind_rows(
  pred1 %>% mutate(Model = "Model 1"),
  pred2 %>% mutate(Model = "Model 2"),
  null_pred %>% mutate(Model = "Null Model")
) %>%
  select(Y, .pred, Model) %>%
  rename(Observed = Y, Predicted = .pred)

# Plot observed vs. predicted values
obs_pred_models_plot <- ggplot(predictions_df, aes(x = Observed, y = Predicted, color = Model, shape = Model)) +
  geom_point(alpha = 0.6, size = 2) +  
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "black") +  
  scale_x_continuous(limits = c(0, 500)) +  
  scale_y_continuous(limits = c(0, 500)) +  
  labs(
    title = "Observed vs Predicted Values",
    x = "Observed",
    y = "Predicted",
    color = "Model",
    shape = "Model"
  ) +
  theme_minimal()

print(obs_pred_models_plot)

# Compute residuals for model 2
model2_residual <- predictions_df %>%
  filter(Model == "Model 2") %>%
  mutate(Residual = Predicted - Observed)

residual_plot <- ggplot(model2_residual, aes(x = Predicted, y = Residual)) +
  geom_point(alpha = 0.6, color = "black") +
  geom_hline(yintercept = 0, color = "blue") +
  scale_y_continuous(limits = c(-max(abs(model2_residual$Residual)), max(abs(model2_residual$Residual)))) + # line assisted by ChatGPT
  labs(
    title = "Predicted Values vs Residuals for Model 2",
    x = "Predicted Value",
    y = "Residual"
  ) + theme_minimal()

print(residual_plot)
  
```

### Model Predictions and Uncertainty

This section entails bootstrapping, fitting models to the bootstrapping
samples, and the computation/graphing of other various statistics.

```{r}
library(rsample)
library(purrr)
library(tidymodels)

# Set seed
set.seed(1234)

# Create 100 bootstraps from training data
train_bootstrap <- bootstraps(train_data, time = 100)

# Function for fitting model and creating predictions
# Assisted by chatGPT

boot_fit_pred <- map(train_bootstrap$splits, function(split) {
  # Extract bootstrap sample
  bootstrap_sample <- analysis(split)
  
  # Model fitting
  model_fit <- model_spec2 %>%
    fit(Y ~ DOSE + AGE + SEX + WT + HT, data = bootstrap_sample)
  
  # Make predictions
  predictions <- predict(model_fit, new_data = train_data) %>%
    mutate(Observed = train_data$Y)
  
  return(predictions)
})

# The next few lines were debugged and assisted through ChatGPT
# Combine predictions from all bootstrap samples
bootstrap_pred_df <- bind_rows(boot_fit_pred, .id = "Bootstrap_Sample")

# Extract predictions for use later
boot_preds <- map(boot_fit_pred, ~ .x$.pred)

# Combine into matrix
pred_bs <- do.call(rbind, boot_preds)

# Calculate values for preds matrix
preds <- pred_bs %>% 
  apply(2, function(x) c(
    Mean = mean(x),  
    Lower = quantile(x, 0.055),  
    Median = quantile(x, 0.5),   
    Upper = quantile(x, 0.945)   
  )) %>% 
  t()


# Create dataframe with observed values, point estimate, and bootstrap values
plot_df <- data.frame(
  observed = train_data$Y,
  PointEstimate = preds[ , 1],
  Lower = preds[ , 2],
  Median = preds[ , 3],
  Upper = preds[ , 4]
)

final_plot <- ggplot(plot_df, aes(x = observed)) +
  
  # Plot point estimates
  geom_point(aes(y = PointEstimate), color = "black", shape = 16, size = 3) +
  
  # Plot median of bootstraps
  geom_point(aes(y = Median), color = "red", shape = 17, size = 1) +
  
  # Plot CI
  geom_errorbar(aes(ymin = Lower, ymax = Upper), color = "blue", width = 0.2) +
  
  # 45 degree line
  geom_abline(slope = 1, intercept = 0, color = "darkgray", linetype = "dashed") +
  
  # Labels
  labs(x = "Observed Values", y = "Predicted Values (Bootstrap)",
       title = "Observed vs Predicted Values, Median, and 89% CI") +
  theme_minimal()

print(final_plot)
```

Something interesting about final_plot is that the median and point
estimate seem to be the same or similar values. When I asked ChatGPT
about the implications, it says that the median and point estimate's
similarity can shows that the model is stable and not highly volatile
across different data sets. It also says that this can indicate the
model is generalizable.
