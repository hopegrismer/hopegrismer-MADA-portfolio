[
  {
    "objectID": "starter-analysis-exercise/data/readme.html",
    "href": "starter-analysis-exercise/data/readme.html",
    "title": "Hope Grismer's Data Analysis Portfolio",
    "section": "",
    "text": "The folders inside this folder should contain all data at various stages.\nThis data is being loaded/manipulated/changed/saved with code from the code folders.\nYou should place the raw data in the raw_data folder and not edit it. Ever!\nIdeally, load the raw data into R and do all changes there with code, so everything is automatically reproducible and documented.\nSometimes, you need to edit the files in the format you got. For instance, Excel files are sometimes so poorly formatted that it’s close to impossible to read them into R, or the persons you got the data from used color to code some information, which of course won’t import into R. In those cases, you might have to make modifications in a software other than R. If you need to make edits in whatever format you got the data (e.g. Excel), make a copy and place those copies in a separate folder, AND ONLY EDIT THOSE COPIES. Also, write down somewhere the edits you made.\nAdd as many sub-folders as suitable. If you only have a single processing step, one sub-folder for processed data is enough. If you have multiple stages of cleaning and processing, additional sub-folders might be useful. Adjust based on the complexity of your project.\nI suggest you save your processed and cleaned data as RDS or RDA/Rdata files. This preserves coding like factors, characters, numeric, etc. If you save as CSV, that information would get lost. However, CSV is better for sharing with others since it’s plain text. If you do CSV, you might want to write down somewhere what each variable is.\nSee here for some suggestions on how to store your processed data:\nhttp://www.sthda.com/english/wiki/saving-data-into-r-data-format-rds-and-rdata"
  },
  {
    "objectID": "starter-analysis-exercise/results/figures/readme.html",
    "href": "starter-analysis-exercise/results/figures/readme.html",
    "title": "Hope Grismer's Data Analysis Portfolio",
    "section": "",
    "text": "Folder for all figures.\nYou can create further sub-folders if that makes sense."
  },
  {
    "objectID": "starter-analysis-exercise/results/tables-files/readme.html",
    "href": "starter-analysis-exercise/results/tables-files/readme.html",
    "title": "Hope Grismer's Data Analysis Portfolio",
    "section": "",
    "text": "Folder for all tables (generally stored as Rds files) and other files.\nYou can create further sub-folders if that makes sense."
  },
  {
    "objectID": "starter-analysis-exercise/code/processing-code/readme.html",
    "href": "starter-analysis-exercise/code/processing-code/readme.html",
    "title": "Hope Grismer's Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains code for processing data.\nCurrently, there is just a single Quarto file to illustrate how the processing can look like.\nInstead of a Quarto file that contains code, it is also possible to use R scripts or a combination of R scripts and Quarto code. Those approaches are illustrated in the full dataanalysis-template repository."
  },
  {
    "objectID": "starter-analysis-exercise/code/eda-code/readme.html",
    "href": "starter-analysis-exercise/code/eda-code/readme.html",
    "title": "Hope Grismer's Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains code to do some simple exploratory data analysis (EDA) on the processed/cleaned data. The code produces a few tables and figures, which are saved in the appropriate results sub-folder."
  },
  {
    "objectID": "starter-analysis-exercise/code/eda-code/eda2.html",
    "href": "starter-analysis-exercise/code/eda-code/eda2.html",
    "title": "An example exploratory analysis script",
    "section": "",
    "text": "This Quarto file loads the cleaned data and does some exploring.\nI’m only showing it the way where the code is included in the file. As described in the processing_code materials, I currently prefer the approach of having R code in a separate file and pulling it in.\nBut I already had this written and haven’t yet re-done it that way. Feel free to redo and send a pull request on GitHub :)\nAgain, it is largely a matter of preference and what makes the most sense to decide if one wants to have code inside Quarto files, or as separate R files. And sometimes, an R script with enough comments is good enough and one doesn’t need a Quarto file.\nAlso note that while here I split cleaning and exploring, this is iterative. You saw that as part of the processing, we already had to explore the data somewhat to understand how to clean it. In general, as you explore, you’ll find things that need cleaning. As you clean, you can explore more. Therefore, at times it might make more sense to combine the cleaning and exploring code parts into a single R or Quarto file. Or split things in any other logical way.\nAs part of the exploratory analysis, you should produce plots or tables or other summary quantities for the most interesting/important quantities in your data. Depending on the total number of variables in your dataset, explore all or some of the others. Figures produced here might be histograms or density plots, correlation plots, etc. Tables might summarize your data.\nStart by exploring one variable at a time. Then continue by creating plots or tables of the outcome(s) of interest and the predictor/exposure/input variables you are most interested in. If your dataset is small, you can do that for all variables.\nPlots produced here can be scatterplots, boxplots, violinplots, etc. Tables can be simple 2x2 tables or larger ones.\n\nSetup\n\n#load needed packages. make sure they are installed.\nlibrary(here) #for data loading/saving\n\nhere() starts at /Users/hopegrismercomputer/Desktop/MADA-course/hopegrismer-MADA-portfolio\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(skimr)\nlibrary(ggplot2)\n\nLoad the data.\n\n#Path to data. Note the use of the here() package and not absolute paths\ndata_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"processed-data\",\"processeddata2.rds\")\n#load data\nmydata &lt;- readRDS(data_location)\n\n\n\nData exploration through tables\nShowing a bit of code to produce and save a summary table.\n\nsummary_df = skimr::skim(mydata)\nprint(summary_df)\n\n── Data Summary ────────────────────────\n                           Values\nName                       mydata\nNumber of rows             9     \nNumber of columns          5     \n_______________________          \nColumn type frequency:           \n  character                1     \n  factor                   1     \n  numeric                  3     \n________________________         \nGroup variables            None  \n\n── Variable type: character ────────────────────────────────────────────────────\n  skim_variable                 n_missing complete_rate min max empty n_unique\n1 Favorite Typical Pizza Flavor         0             1   6   9     0        3\n  whitespace\n1          0\n\n── Variable type: factor ───────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate ordered n_unique top_counts      \n1 Gender                0             1 FALSE          3 M: 4, F: 3, O: 2\n\n── Variable type: numeric ──────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate   mean    sd  p0 p25 p50 p75 p100 hist \n1 Height                0             1 166.   16.0  133 156 166 178  183 ▂▁▃▃▇\n2 Weight                0             1  70.1  21.2   45  55  70  80  110 ▇▂▃▂▂\n3 Shoe Size             0             1   7.89  2.09   5   7   8   9   11 ▇▇▃▇▇\n\n# save to file\nsummarytable_file = here(\"starter-analysis-exercise\",\"results\", \"tables-files\", \"summarytable.rds\")\nsaveRDS(summary_df, file = summarytable_file)\n\nWe are saving the results to the results/tables folder. Structure the folders inside results such that they make sense for your specific analysis. Provide enough documentation that someone can understand what you are doing and what goes where. readme.md files inside each folder are a good idea.\n\n\nData exploration through figures\nHistogram plots for the continuous outcomes.\nHeight first.\n\np1 &lt;- mydata %&gt;% ggplot(aes(x=Height)) + geom_histogram() \nplot(p1)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-distribution.png\")\nggsave(filename = figure_file, plot=p1) \n\nSaving 7 x 5 in image\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nNow weights.\n\np2 &lt;- mydata %&gt;% ggplot(aes(x=Weight)) + geom_histogram() \nplot(p2)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"weight-distribution.png\")\nggsave(filename = figure_file, plot=p2) \n\nSaving 7 x 5 in image\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nNow height as function of weight.\n\np3 &lt;- mydata %&gt;% ggplot(aes(x=Height, y=Weight)) + geom_point() + geom_smooth(method='lm')\nplot(p3)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-weight.png\")\nggsave(filename = figure_file, plot=p3) \n\nSaving 7 x 5 in image\n`geom_smooth()` using formula = 'y ~ x'\n\n\nOnce more height as function of weight, stratified by gender. Note that there is so little data, it’s a bit silly. But we’ll plot it anyway.\n\np4 &lt;- mydata %&gt;% ggplot(aes(x=Height, y=Weight, color = Gender)) + geom_point() + geom_smooth(method='lm')\nplot(p4)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning in qt((1 - level)/2, df): NaNs produced\n\n\nWarning in max(ids, na.rm = TRUE): no non-missing arguments to max; returning\n-Inf\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-weight-stratified.png\")\nggsave(filename = figure_file, plot=p4) \n\nSaving 7 x 5 in image\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning in qt((1 - level)/2, df): NaNs produced\nWarning in qt((1 - level)/2, df): no non-missing arguments to max; returning\n-Inf\n\n\n\n\nNotes\nFor your own explorations, tables and figures can be “quick and dirty”. As long as you can see what’s going on, there is no need to polish them. That’s in contrast to figures you’ll produce for your final products (paper, report, presentation, website, etc.). Those should look as nice, polished and easy to understand as possible.\nGuozheng’s work:\nBelow is a boxplot of height grouped by favorite typical pizza flavor.\n\np5 &lt;- mydata %&gt;% ggplot(aes(x=`Favorite Typical Pizza Flavor`, y=Height)) + \n  geom_boxplot(color=\"firebrick3\") + \n  theme_bw() +\n  theme(axis.title.x = element_text(size=20, face=\"bold\"),\n        axis.title.y = element_text(size=20, face=\"bold\"),\n        axis.text.x = element_text(size=15),\n        axis.text.y = element_text(size=15))\nplot(p5)\n\n\n\n\n\n\n\nfigure_file=here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-pizza-boxplot.png\")\nggsave(filename = figure_file, plot=p5) \n\nSaving 7 x 5 in image\n\n\nBelow is a scatterplot of shoe size ~ weight.\n\np6 &lt;- mydata %&gt;% ggplot(aes(x=Weight, y=`Shoe Size`)) + \n  geom_point(size=2, color=\"firebrick3\") + \n  theme_bw() +\n  theme(axis.title.x = element_text(size=20, face=\"bold\"),\n        axis.title.y = element_text(size=20, face=\"bold\"),\n        axis.text.x = element_text(size=15),\n        axis.text.y = element_text(size=15))\nplot(p6)\n\n\n\n\n\n\n\nfigure_file=here(\"starter-analysis-exercise\",\"results\",\"figures\",\"shoesize-weight-scatterplot.png\")\nggsave(filename = figure_file, plot=p6) \n\nSaving 7 x 5 in image"
  },
  {
    "objectID": "starter-analysis-exercise/code/analysis-code/readme.html",
    "href": "starter-analysis-exercise/code/analysis-code/readme.html",
    "title": "Hope Grismer's Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains code to do some simple exploratory analysis and statistical analysis on the processed/cleaned data. The code produces a few tables and figures, which are saved in the results folder.\nIt’s the same code done 3 times:\n\nFirst, there is an R script that you can run which does all the computations.\nSecond, there is a Quarto file which contains exactly the same code as the R script.\nThird, my current favorite, is a Quarto file with an approach where the code is pulled in from the R script and run.\n\nThe last version has the advantage of having code in one place for easy writing/debugging, and then being able to pull the code into the Quarto file for a nice combination of text/commentary and code.\nEach way of doing this is a reasonable approach, pick whichever one you prefer or makes the most sense for your setup. Whichever approach you choose, add ample documentation/commentary so you and others can easily understand what’s going on and what is done."
  },
  {
    "objectID": "starter-analysis-exercise/products/readme.html",
    "href": "starter-analysis-exercise/products/readme.html",
    "title": "Hope Grismer's Data Analysis Portfolio",
    "section": "",
    "text": "The folders inside this folder should contain all the products of your project.\nFor a classical academic project, this will be a peer-reviewed manuscript, and should be placed into a manuscript folder.\nFor our case, since we’ll want to put it on the website, we call it a report.\nOften you need a library of references in bibtex format, as well as a CSL style file that determines reference formatting. Since those files might be used by several of the products, I’m placing them in the main products folder. Feel free to re-organize."
  },
  {
    "objectID": "presentation-exercise/fight-songs/presentation-exercise.html",
    "href": "presentation-exercise/fight-songs/presentation-exercise.html",
    "title": "Fight Songs",
    "section": "",
    "text": "College fight songs are a key part of the game-day experience. The most memorable songs are celebrated and shouted during iconic events like the Heisman Trophy presentation, while the less notable fade into obscurity.\nIn this analysis and displayed in the graph on “Five Thirty Eight”, we collected fight songs from 65 schools across the Power Five conferences (ACC, Big Ten, Big 12, Pac-12, SEC) and Notre Dame. I found this to be super interesting as it examined common lyrical elements, such as chants and spelling, CLICHES because we know how common those are!!, as well as analyzed the tempo and duration of each song (based on available Spotify versions) to understand how these schools are musically propelled to victory.\n# Load the knitr package\nlibrary(knitr)\n\n# Load the here package\nlibrary(here)\n\nhere() starts at /Users/hopegrismercomputer/Desktop/MADA-course/hopegrismer-MADA-portfolio\n\n# Use include_graphics to display the PNG image with correct path using here()\n\ninclude_graphics(here(\"presentation-exercise\", \"fight-songs\", \"presentation-results-exercise\", \"presentation-results-plots\", \"Graph_of_Fight_Songs.png\"))\nAbove is the graph that I attempted to recreate.\n# Load and install required libraries\nlibrary(ggplot2)\ninstall.packages(\"here\")\n\n\nThe downloaded binary packages are in\n    /var/folders/29/tsfhn2w952x9r8f27_hpzq900000gn/T//RtmpdITUq8/downloaded_packages\n\nlibrary(here)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggrepel)  # Load the ggrepel package\n\n# Set the correct relative path to the CSV file inside the 'fight-songs' subfolder\ndata &lt;- read.csv(here(\"presentation-exercise\", \"fight-songs\", \"fight-songs.csv\"))\nI was able to download the original data. I recalculated the average valuesfor duration and BPM. I knew this was an important first step as these values are displayed as the quadrant boundaries on both the x and y axes.\n# Calculate average values for duration and BPM\navg_duration &lt;- mean(data$sec_duration, na.rm = TRUE)\navg_bpm &lt;- mean(data$bpm, na.rm = TRUE)\nNext, the four-quadrant plot was created. Many iterations of this code was requested from the AI because of the difficulties with school labels overlapping with the quadrant labels. This is something that the original data was able to avoid by creating an interactive overlay so labels are only shown if one’s cursor selects it. This is outside my current skills, but I am interested in whether this can be done within R or requires additional tech/platforms/extensions etc. I ended up switching the orientation of the labels to try to accomodate the right side.\n# Create the four-quadrant plot \nplot = ggplot(data, aes(x = sec_duration, y = bpm, label = school)) +  # Adjusted column names\n  geom_point(aes(color = school), size = 3, alpha = 0.7) +  # Add alpha for transparency, points for each university team\n  geom_hline(yintercept = avg_bpm, linetype = \"dashed\", color = \"lightgrey\") +  # Light grey horizontal line for average BPM\n  geom_vline(xintercept = avg_duration, linetype = \"dashed\", color = \"lightgrey\") +  # Light grey vertical line for average duration\n  geom_text_repel(aes(label = school), size = 3, max.overlaps = Inf, box.padding = 0.5, force = 10) +  # Repel text labels with more space\n  labs(\n    title = \"Fight Songs Quadrant Plot\",\n    x = \"Song Duration (seconds)\",\n    y = \"Beats Per Minute (BPM)\"\n  ) +\n  theme_minimal(base_size = 14) +  # Increase base font size for better readability\n  theme(\n    legend.position = \"none\",  # Remove legend for cleaner look\n    plot.margin = margin(10, 10, 10, 10),  # Increase margins to avoid cutting off labels\n    plot.title = element_text(hjust = 0.5)  # Center the title\n  ) +\n  # Annotate the quadrant labels vertically in the corners, adjusted laterally\n  # Top-left quadrant (\"Fast and Short\") - moved up a bit\n  annotate(\"text\", x = avg_duration * 0.15, y = 120, label = \"Fast and Short\", color = \"blue\", size = 4, angle = 0, fontface = \"bold\") +  # Moved up slightly\n  # Top-right quadrant (\"Fast but Long\") - moved slightly further right\n  annotate(\"text\", x = avg_duration * 2.3, y = avg_bpm * 1.1, label = \"Fast but Long\", color = \"blue\", size = 4, angle = 90, fontface = \"bold\") +  # Bolded text\n  # Bottom-left quadrant (\"Short but Slow\")\n  annotate(\"text\", x = avg_duration * 0.05, y = avg_bpm * 0.7, label = \"Short but Slow\", color = \"blue\", size = 4, angle = 90, fontface = \"bold\") +  # Bolded text\n  # Bottom-right quadrant (\"Slow and Long\")\n  annotate(\"text\", x = avg_duration * 1.95, y = avg_bpm * 0.7, label = \"Slow and Long\", color = \"blue\", size = 4, angle = 90, fontface = \"bold\")  # Bolded text\nHere is the quadrant plot I was able to generate below.\n# Print the plot\nprint(plot)\n\n\n\n\n\n\n\n# Save the plot into the \"fight-songs\" folder with a white background\nggsave(\"./presentation-results-exercise/presentation-results-plots/fight_song_quadrant_plot.png\", plot = plot, width = 10, height = 8, dpi = 300, bg = \"white\")\nI ended up putting my final code back into AI to see what prompt it thinks would generate my final version. This was the output:\nAI Prompt: Generate a Quarto document that does the following: Introduction: Write an introduction about college fight songs, explaining how they are an integral part of the college experience and how the best ones are celebrated while the less memorable ones fade into obscurity. Include a section about an analysis of 65 school fight songs, collected from the Power Five conferences (ACC, Big Ten, Big 12, Pac-12, SEC) and Notre Dame. The analysis includes an exploration of common lyrical elements (such as chants and spelling) and an examination of the tempo and duration of the songs (based on Spotify data). Graph Section: Display a PNG image, titled “Graph of Fight Songs,” that is located in a specific path (presentation-results-plots folder). Use the knitr package to include the image with the correct file path using the here() function. Data Loading and Preparation: Load necessary libraries (e.g., ggplot2, dplyr, ggrepel, here). Load the data from a CSV file that is located in the presentation-exercise/fight-songs folder, using the read.csv() function and the here() package for path management. Analysis: Calculate the average duration and beats per minute (BPM) for the songs. Use the calculated averages for plotting the data, showing the boundaries for the quadrants on the x and y axes. Quadrant Plot Creation: Create a four-quadrant plot using ggplot2. The plot should display each song with its duration (x-axis) and BPM (y-axis). Include repelling text labels to avoid overlap. Annotate the quadrants (e.g., “Fast and Short,” “Fast but Long,” etc.) with text labels. Add dashed lines for the average BPM and duration. Saving the Plot: Save the generated plot in the presentation-results-exercise/presentation-results-plots folder with a white background. Make sure the explanations and commentary about the analysis are clearly written for non-technical readers.\nI found feeding back the code to AI and asking what would have been efficient in generating the output is an interesting loop to pursue."
  },
  {
    "objectID": "presentation-exercise/fight-songs/presentation-exercise.html#part-two---creating-a-high-quality-table",
    "href": "presentation-exercise/fight-songs/presentation-exercise.html#part-two---creating-a-high-quality-table",
    "title": "Fight Songs",
    "section": "Part Two - Creating a High-Quality Table",
    "text": "Part Two - Creating a High-Quality Table\nI am genuinely proud of my table. I started with a general table output via a AI template structure from the following prompt: Use the same dataset you used for the graph (or a subset) to create a table that is representative of the data set. Make sure it includes meaningful metrics like song duration and beats per minute (BPM) and BPM Comparison to correspond with the quadrants.\nThe orignal table had column headings that matched the raw data’s table’s headings. I renamed each to best display the data more professionally.\n\n# Load necessary libraries\nlibrary(kableExtra)\n\n\nAttaching package: 'kableExtra'\n\n\nThe following object is masked from 'package:dplyr':\n\n    group_rows\n\nlibrary(dplyr)\n\n# Assuming you have a dataset 'data' with the necessary columns\n\n# Calculate summary values (if you haven't done it yet)\nsummary_table &lt;- data %&gt;%\n  mutate(\n    avg_duration = mean(sec_duration, na.rm = TRUE),\n    avg_bpm = mean(bpm, na.rm = TRUE),\n    bpm_comparison = ifelse(bpm &gt; avg_bpm, \"Above Average\", \"Below Average\"),\n    duration_diff = sec_duration - avg_duration,\n    bpm_diff = bpm - avg_bpm\n  )\n\n# Select only the relevant columns for the table\nsummary_table &lt;- summary_table %&gt;%\n  select(school, bpm_comparison, duration_diff, bpm_diff)\n\n# Rename the columns for clarity\ncolnames(summary_table) &lt;- c(\"School\", \n                             \"BPM Comparison (Above/Below Average)\", \n                             \"Difference from Mean Duration (Seconds)\", \n                             \"Difference from Mean BPM\")\n\n# Create and style the table using kable and kableExtra\nsummary_table %&gt;%\n  kable(\"html\", caption = \"Summary of Fight Songs with Differences from the Mean\") %&gt;%\n  kable_styling(full_width = TRUE, position = \"center\", font_size = 12) %&gt;%\n  column_spec(1, bold = TRUE, color = \"white\", background = \"#0073e6\") %&gt;%\n  column_spec(2, color = \"darkblue\") %&gt;%\n  column_spec(3, color = \"green\") %&gt;%\n  column_spec(4, color = \"orange\") %&gt;%\n  row_spec(0, bold = TRUE, background = \"#d9d9d9\") %&gt;%\n  row_spec(1:nrow(summary_table), color = \"black\", \n           background = ifelse(summary_table$`BPM Comparison (Above/Below Average)` == \"Above Average\", \"#e6f7ff\", \"#ffe6e6\")) %&gt;%\n  footnote(general = \"The above table summarizes key metrics of the fight songs, including their duration, BPM, and how they compare to the average-- both categorically and quantitatively.\", \n           general_title = \"Footnote\", \n           footnote_as_chunk = TRUE)\n\n\nSummary of Fight Songs with Differences from the Mean\n\n\nSchool\nBPM Comparison (Above/Below Average)\nDifference from Mean Duration (Seconds)\nDifference from Mean BPM\n\n\n\n\nNotre Dame\nAbove Average\n-7.9076923\n23.2\n\n\nBaylor\nBelow Average\n27.0923077\n-52.8\n\n\nIowa State\nAbove Average\n-16.9076923\n26.2\n\n\nKansas\nAbove Average\n-9.9076923\n8.2\n\n\nKansas State\nBelow Average\n-4.9076923\n-48.8\n\n\nOklahoma\nAbove Average\n-34.9076923\n24.2\n\n\nOklahoma State\nAbove Average\n-42.9076923\n51.2\n\n\nTexas\nBelow Average\n-6.9076923\n-47.8\n\n\nTCU\nAbove Average\n-24.9076923\n20.2\n\n\nTexas Tech\nAbove Average\n-17.9076923\n30.2\n\n\nWest Virginia\nAbove Average\n20.0923077\n23.2\n\n\nIllinois\nAbove Average\n-11.9076923\n33.2\n\n\nIndiana\nAbove Average\n-8.9076923\n21.2\n\n\nIowa\nAbove Average\n0.0923077\n21.2\n\n\nMaryland\nAbove Average\n-15.9076923\n13.2\n\n\nMichigan\nBelow Average\n50.0923077\n-45.8\n\n\nMichigan State\nAbove Average\n40.0923077\n16.2\n\n\nMinnesota\nAbove Average\n-11.9076923\n22.2\n\n\nNebraska\nAbove Average\n22.0923077\n7.2\n\n\nNorthwestern\nAbove Average\n-14.9076923\n11.2\n\n\nOhio State\nAbove Average\n17.0923077\n49.2\n\n\nPenn State\nAbove Average\n9.0923077\n16.2\n\n\nPurdue\nAbove Average\n14.0923077\n31.2\n\n\nRutgers\nBelow Average\n2.0923077\n-53.8\n\n\nWisconsin\nAbove Average\n62.0923077\n3.2\n\n\nArizona\nAbove Average\n11.0923077\n15.2\n\n\nArizona State\nAbove Average\n34.0923077\n21.2\n\n\nCalifornia\nAbove Average\n16.0923077\n5.2\n\n\nColorado\nAbove Average\n-44.9076923\n22.2\n\n\nOregon\nBelow Average\n8.0923077\n-63.8\n\n\nOregon State\nAbove Average\n-8.9076923\n8.2\n\n\nStanford\nAbove Average\n-8.9076923\n6.2\n\n\nUCLA\nBelow Average\n-2.9076923\n-56.8\n\n\nUSC\nBelow Average\n-11.9076923\n-53.8\n\n\nUtah\nAbove Average\n-5.9076923\n39.2\n\n\nWashington\nAbove Average\n17.0923077\n10.2\n\n\nWashington State\nBelow Average\n-7.9076923\n-47.8\n\n\nAlabama\nAbove Average\n-12.9076923\n24.2\n\n\nArkansas\nBelow Average\n16.0923077\n-51.8\n\n\nAuburn\nBelow Average\n-44.9076923\n-51.8\n\n\nFlorida\nBelow Average\n-5.9076923\n-57.8\n\n\nGeorgia\nAbove Average\n-30.9076923\n33.2\n\n\nKentucky\nBelow Average\n-19.9076923\n-49.8\n\n\nLSU\nAbove Average\n11.0923077\n39.2\n\n\nMississippi\nAbove Average\n-6.9076923\n24.2\n\n\nMississippi State\nAbove Average\n-17.9076923\n17.2\n\n\nMissouri\nAbove Average\n14.0923077\n9.2\n\n\nSouth Carolina\nAbove Average\n-7.9076923\n24.2\n\n\nTennessee\nAbove Average\n-18.9076923\n20.2\n\n\nTexas A&M\nBelow Average\n100.0923077\n-11.8\n\n\nVanderbilt\nBelow Average\n-26.9076923\n-56.8\n\n\nBoston College\nAbove Average\n12.0923077\n18.2\n\n\nClemson\nAbove Average\n6.0923077\n16.2\n\n\nDuke\nAbove Average\n-13.9076923\n10.2\n\n\nFlorida State\nAbove Average\n20.0923077\n6.2\n\n\nGeorgia Tech\nAbove Average\n-3.9076923\n19.2\n\n\nLouisville\nAbove Average\n8.0923077\n10.2\n\n\nMiami\nBelow Average\n1.0923077\n-53.8\n\n\nNorth Carolina\nAbove Average\n-33.9076923\n29.2\n\n\nNorth Carolina State\nBelow Average\n10.0923077\n-38.8\n\n\nPitt\nBelow Average\n38.0923077\n-49.8\n\n\nSyracuse\nAbove Average\n13.0923077\n8.2\n\n\nVirginia\nBelow Average\n-29.9076923\n-16.8\n\n\nVirginia Tech\nAbove Average\n-0.9076923\n4.2\n\n\nWake Forest\nAbove Average\n-2.9076923\n17.2\n\n\n\nFootnote  The above table summarizes key metrics of the fight songs, including their duration, BPM, and how they compare to the average-- both categorically and quantitatively.\n\n\n\n\n\n\n\n\n\n\n\nLet’s start over!!!!\nTrying again!!!!\nI tried again on this exercise after reading how other students prompted their AI using an image upload of the orginal graph. I ended up prompting the AI model to stylistically copy the uploaded image. I simultaneously uploaded the above code and noted in the prompt that the code was partially correct but did not appropriate match the style. Below is my updated output, which was much closer in style to the original!\n\n\n\n\n\n\n\n\n\n\nSummary of Fight Songs\n\n\nSchool\nBPM Comparison\nDiff from Mean Duration\nDiff from Mean BPM\n\n\n\n\nNotre Dame\nAbove Average\n-7.9076923\n23.2\n\n\nBaylor\nBelow Average\n27.0923077\n-52.8\n\n\nIowa State\nAbove Average\n-16.9076923\n26.2\n\n\nKansas\nAbove Average\n-9.9076923\n8.2\n\n\nKansas State\nBelow Average\n-4.9076923\n-48.8\n\n\nOklahoma\nAbove Average\n-34.9076923\n24.2\n\n\nOklahoma State\nAbove Average\n-42.9076923\n51.2\n\n\nTexas\nBelow Average\n-6.9076923\n-47.8\n\n\nTCU\nAbove Average\n-24.9076923\n20.2\n\n\nTexas Tech\nAbove Average\n-17.9076923\n30.2\n\n\nWest Virginia\nAbove Average\n20.0923077\n23.2\n\n\nIllinois\nAbove Average\n-11.9076923\n33.2\n\n\nIndiana\nAbove Average\n-8.9076923\n21.2\n\n\nIowa\nAbove Average\n0.0923077\n21.2\n\n\nMaryland\nAbove Average\n-15.9076923\n13.2\n\n\nMichigan\nBelow Average\n50.0923077\n-45.8\n\n\nMichigan State\nAbove Average\n40.0923077\n16.2\n\n\nMinnesota\nAbove Average\n-11.9076923\n22.2\n\n\nNebraska\nAbove Average\n22.0923077\n7.2\n\n\nNorthwestern\nAbove Average\n-14.9076923\n11.2\n\n\nOhio State\nAbove Average\n17.0923077\n49.2\n\n\nPenn State\nAbove Average\n9.0923077\n16.2\n\n\nPurdue\nAbove Average\n14.0923077\n31.2\n\n\nRutgers\nBelow Average\n2.0923077\n-53.8\n\n\nWisconsin\nAbove Average\n62.0923077\n3.2\n\n\nArizona\nAbove Average\n11.0923077\n15.2\n\n\nArizona State\nAbove Average\n34.0923077\n21.2\n\n\nCalifornia\nAbove Average\n16.0923077\n5.2\n\n\nColorado\nAbove Average\n-44.9076923\n22.2\n\n\nOregon\nBelow Average\n8.0923077\n-63.8\n\n\nOregon State\nAbove Average\n-8.9076923\n8.2\n\n\nStanford\nAbove Average\n-8.9076923\n6.2\n\n\nUCLA\nBelow Average\n-2.9076923\n-56.8\n\n\nUSC\nBelow Average\n-11.9076923\n-53.8\n\n\nUtah\nAbove Average\n-5.9076923\n39.2\n\n\nWashington\nAbove Average\n17.0923077\n10.2\n\n\nWashington State\nBelow Average\n-7.9076923\n-47.8\n\n\nAlabama\nAbove Average\n-12.9076923\n24.2\n\n\nArkansas\nBelow Average\n16.0923077\n-51.8\n\n\nAuburn\nBelow Average\n-44.9076923\n-51.8\n\n\nFlorida\nBelow Average\n-5.9076923\n-57.8\n\n\nGeorgia\nAbove Average\n-30.9076923\n33.2\n\n\nKentucky\nBelow Average\n-19.9076923\n-49.8\n\n\nLSU\nAbove Average\n11.0923077\n39.2\n\n\nMississippi\nAbove Average\n-6.9076923\n24.2\n\n\nMississippi State\nAbove Average\n-17.9076923\n17.2\n\n\nMissouri\nAbove Average\n14.0923077\n9.2\n\n\nSouth Carolina\nAbove Average\n-7.9076923\n24.2\n\n\nTennessee\nAbove Average\n-18.9076923\n20.2\n\n\nTexas A&M\nBelow Average\n100.0923077\n-11.8\n\n\nVanderbilt\nBelow Average\n-26.9076923\n-56.8\n\n\nBoston College\nAbove Average\n12.0923077\n18.2\n\n\nClemson\nAbove Average\n6.0923077\n16.2\n\n\nDuke\nAbove Average\n-13.9076923\n10.2\n\n\nFlorida State\nAbove Average\n20.0923077\n6.2\n\n\nGeorgia Tech\nAbove Average\n-3.9076923\n19.2\n\n\nLouisville\nAbove Average\n8.0923077\n10.2\n\n\nMiami\nBelow Average\n1.0923077\n-53.8\n\n\nNorth Carolina\nAbove Average\n-33.9076923\n29.2\n\n\nNorth Carolina State\nBelow Average\n10.0923077\n-38.8\n\n\nPitt\nBelow Average\n38.0923077\n-49.8\n\n\nSyracuse\nAbove Average\n13.0923077\n8.2\n\n\nVirginia\nBelow Average\n-29.9076923\n-16.8\n\n\nVirginia Tech\nAbove Average\n-0.9076923\n4.2\n\n\nWake Forest\nAbove Average\n-2.9076923\n17.2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nI included the original here to compare with the second replication above."
  },
  {
    "objectID": "cdcdata-exercise/statistical-analysis-cdcdata-exercise.html",
    "href": "cdcdata-exercise/statistical-analysis-cdcdata-exercise.html",
    "title": "CDC Data Analysis Exercise",
    "section": "",
    "text": "# Install the 'here' package \n# Set CRAN mirror globally\noptions(repos = c(CRAN = \"https://cloud.r-project.org/\"))\ninstall.packages(\"here\")\n\n\nThe downloaded binary packages are in\n    /var/folders/29/tsfhn2w952x9r8f27_hpzq900000gn/T//Rtmp1il05Y/downloaded_packages\n\n# Load necessary libraries\nlibrary(ggplot2)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(here)  # Load the 'here' package\n\nhere() starts at /Users/hopegrismercomputer/Desktop/MADA-course/hopegrismer-MADA-portfolio\n\n# Load the dataset using here::here() for the file path\ndata &lt;- read.csv(here(\"cdcdata-exercise\", \"Weekly_Influenza_Vaccination_Coverage_Among_Pregnant_Women_18-49_Years__by_Race_and_Ethnicity_20250205.csv\"))\n\n\n# Visualize vaccination coverage by race and ethnicity\nbarplot1iteration = ggplot(data, aes(x = Percent, fill = Race.and.Ethnicity)) + \n    geom_histogram(binwidth = 100, color = \"black\", position = \"dodge\") +\n    labs(title = \"Vaccination Coverage Distribution by Race and Ethnicity - First Iteration\",\n         x = \"Vaccination Coverage (%)\",\n         y = \"Frequency\") +\n    theme_minimal()\n\n# Save first iteration of barplot\nfigure_file = here(\"cdcdata-exercise\",\"cdcdata-exercise-tables-graphs\",\"barplot-coverage-by-race-first-iteration.png\")\nggsave(filename = figure_file, plot=barplot1iteration)\n\nSaving 7 x 5 in image\n\nbarplot1iteration\n\n\n\n\n\n\n\n# Print  first iteration of barplot\nprint(\"barplot-coverage-by-race-first-iteration.png\")\n\n[1] \"barplot-coverage-by-race-first-iteration.png\"\n\n\n\n# There is negative vaccination coverage (like -50%) appearing on histogram which doesn’t make sense in context. \n# Check for any negative values in the Percent column\nnegative_values &lt;- data %&gt;% filter(Percent &lt; 0)\nprint(negative_values)\n\n[1] Week_Ending_Date   Race.and.Ethnicity Percent            Flu.Season        \n[5] Denominator        Date.Order         Race.Sort.Order   \n&lt;0 rows&gt; (or 0-length row.names)\n\n# Visualize vaccination coverage by race and ethnicity with updated BINS\nupdatedbins &lt;- ggplot(data, aes(x = Percent, fill = Race.and.Ethnicity)) + \n    geom_histogram(binwidth = 10, color = \"black\", position = \"dodge\") +\n    labs(title = \"Vaccination Coverage Distribution by Race and Ethnicity\",\n         x = \"Vaccination Coverage (%)\",\n         y = \"Frequency\") +\n    theme_minimal()\n\nupdatedbins\n\n\n\n\n\n\n\n\n\n#### ------- Making the Distributions ----- ###\n\n# Summarize vaccination coverage by race and ethnicity\ndata_summary &lt;- data %&gt;%\n    group_by(Race.and.Ethnicity) %&gt;%\n    summarize(avg_vaccination_coverage = mean(Percent, na.rm = TRUE))\n\n# View the summary data\nprint(data_summary)\n\n# A tibble: 9 × 2\n  Race.and.Ethnicity                     avg_vaccination_coverage\n  &lt;chr&gt;                                                     &lt;dbl&gt;\n1 American Indian / Alaska Native, NH                        28.2\n2 Asian, NH                                                  42.9\n3 Black, NH                                                  22.4\n4 Hispanic/Latino                                            32.2\n5 Multiple/Other, NH                                         32.4\n6 Native Hawaiian / Pacific Islander, NH                     30.3\n7 Overall                                                    32.6\n8 Unknown                                                    25.0\n9 White, NH                                                  33.2\n\n# Boxplot of vaccination coverage by race and ethnicity\nboxplot1 = ggplot(data, aes(x = Race.and.Ethnicity, y = Percent, fill = Race.and.Ethnicity)) + \n    geom_boxplot() + \n    labs(title = \"Vaccination Coverage by Race and Ethnicity\",\n         x = \"Race and Ethnicity\",\n         y = \"Vaccination Coverage (%)\") +\n    theme_minimal() +\n    theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Rotate x labels for better readability +\ntheme(\n    panel.background = element_rect(fill = \"white\"),  # White background for the plot panel\n    plot.background = element_rect(fill = \"white\")    # White background for the overall plot\n)\n\nList of 2\n $ panel.background:List of 5\n  ..$ fill         : chr \"white\"\n  ..$ colour       : NULL\n  ..$ linewidth    : NULL\n  ..$ linetype     : NULL\n  ..$ inherit.blank: logi FALSE\n  ..- attr(*, \"class\")= chr [1:2] \"element_rect\" \"element\"\n $ plot.background :List of 5\n  ..$ fill         : chr \"white\"\n  ..$ colour       : NULL\n  ..$ linewidth    : NULL\n  ..$ linetype     : NULL\n  ..$ inherit.blank: logi FALSE\n  ..- attr(*, \"class\")= chr [1:2] \"element_rect\" \"element\"\n - attr(*, \"class\")= chr [1:2] \"theme\" \"gg\"\n - attr(*, \"complete\")= logi FALSE\n - attr(*, \"validate\")= logi TRUE\n\nboxplot1\n\n\n\n\n\n\n\n#Save bloxplot1 \nfigure_file = here(\"cdcdata-exercise\",\"cdcdata-exercise-tables-graphs\",\"boxplot-coverage-by-race.png\")\nggsave(filename = figure_file, plot=boxplot1)\n\nSaving 7 x 5 in image\n\n\n\n# Summarize the data by Race and Ethnicity to calculate the mean and standard deviation\ndata_summary &lt;- data %&gt;%\n    group_by(Race.and.Ethnicity) %&gt;%\n    summarize(\n        avg_vaccination_coverage = mean(Percent, na.rm = TRUE),\n        sd_vaccination_coverage = sd(Percent, na.rm = TRUE)\n    )\n\n# View the summary data\nprint(data_summary)\n\n# A tibble: 9 × 3\n  Race.and.Ethnicity               avg_vaccination_cove…¹ sd_vaccination_cover…²\n  &lt;chr&gt;                                             &lt;dbl&gt;                  &lt;dbl&gt;\n1 American Indian / Alaska Native…                   28.2                   15.9\n2 Asian, NH                                          42.9                   22.7\n3 Black, NH                                          22.4                   12.6\n4 Hispanic/Latino                                    32.2                   17.8\n5 Multiple/Other, NH                                 32.4                   17.6\n6 Native Hawaiian / Pacific Islan…                   30.3                   17.4\n7 Overall                                            32.6                   18.0\n8 Unknown                                            25.0                   14.7\n9 White, NH                                          33.2                   18.8\n# ℹ abbreviated names: ¹​avg_vaccination_coverage, ²​sd_vaccination_coverage\n\n\n\n# Load necessary libraries\nlibrary(ggplot2)\n\n# Create a bar plot showing the mean vaccination coverage with standard deviation as error bars\nbarplot1 = ggplot(data_summary, aes(x = Race.and.Ethnicity, y = avg_vaccination_coverage, fill = Race.and.Ethnicity)) +\n    geom_bar(stat = \"identity\", color = \"black\", show.legend = FALSE) +  # Bar plot\n    geom_errorbar(aes(ymin = avg_vaccination_coverage - sd_vaccination_coverage, \n                      ymax = avg_vaccination_coverage + sd_vaccination_coverage), \n                  width = 0.2) +  # Add error bars for SD\n    labs(title = \"Mean Vaccination Coverage by Race and Ethnicity with Standard Deviation\",\n         x = \"Race and Ethnicity\",\n         y = \"Vaccination Coverage (%)\") +\n    theme_minimal() +\n    theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Rotate x-axis labels for readability\n\nbarplot1\n\n\n\n\n\n\n\n#Save barplot1 showing the mean vaccination coverage with sd error bars\nfigure_file = here(\"cdcdata-exercise\",\"cdcdata-exercise-tables-graphs\",\"barplot-meanwithsd-by-race.png\")\nggsave(filename = figure_file, plot=barplot1)\n\nSaving 7 x 5 in image\n\n\n\n#### To visualize the distribution of vaccination coverage (like a bell curve or a smooth curve) for each race/ethnicity group\n# Load necessary libraries\nlibrary(ggplot2)\nlibrary(dplyr)\n\n# Create a density plot for vaccination coverage by race/ethnicity\ndensityplot1 = ggplot(data, aes(x = Percent, fill = Race.and.Ethnicity, color = Race.and.Ethnicity)) +\n    geom_density(alpha = 0.4) +  # alpha for transparency\n    labs(title = \"Density Plot of Vaccination Coverage by Race and Ethnicity\",\n         x = \"Vaccination Coverage (%)\",\n         y = \"Density\") +\n    theme_minimal() +\n    theme(axis.text.x = element_text(angle = 45, hjust = 1)) +  # Rotate x labels for readability\n    scale_fill_brewer(palette = \"Spectral\") +  # Custom color palette\n    scale_color_brewer(palette = \"Spectral\")  # Custom color palette for lines\n\ndensityplot1\n\n\n\n\n\n\n\n#Save density plot for vaccination coverage by race/ethnicity\nfigure_file = here(\"cdcdata-exercise\",\"cdcdata-exercise-tables-graphs\",\"density-plot-vaxcov-by-race.png\")\nggsave(filename = figure_file, plot=densityplot1)\n\nSaving 7 x 5 in image\n\n\nThis was added by Vincent Nguyen. Weekly Influenza Vaccination Coverage, Pregnant Women 18-49 Years Old by Race and Ethnicity\nWeekly influenza vaccination coverage estimates for pregnant women 18–49 years are based on electronic health record (EHR) data from the Vaccine Safety Datalink (VSD) (https://www.cdc.gov/vaccine-safety-systems/vsd/), a collaboration between CDC’s Immunization Safety Office and multiple integrated health care organizations.\n\n\n\nThe downloaded binary packages are in\n    /var/folders/29/tsfhn2w952x9r8f27_hpzq900000gn/T//Rtmp1il05Y/downloaded_packages\n\n\nFirst, I visualized the vaccination coverage data using race and ethnicity as indicators/variables.\n\n\n\n\n\n\n\n\n\nSaving 7 x 5 in image\n\n\nIn my first iteration of this graph, however, there was negative vaccination coverage (like -50%) appearing on histogram. This representation did not which doesn’t make sense in context. I looked for outliers in the data or blanks and found none. I determined the cause of the misrepresentation was the BIN size. So, I adjusted the BINS to properly display the data.\n\n\n[1] Week_Ending_Date   Race.and.Ethnicity Percent            Flu.Season        \n[5] Denominator        Date.Order         Race.Sort.Order   \n&lt;0 rows&gt; (or 0-length row.names)\n\n\n\n\n\n\n\n\n\nSaving 7 x 5 in image\n\n\nNext, I summarized vaccination coverage by race and ethnicity in table form before generating a corresponding boxplot of vaccination coverage by race and ethnicity. This boxplot showed some interesting trends and the general variation/range within each group.\n\n\n# A tibble: 9 × 2\n  Race.and.Ethnicity                     avg_vaccination_coverage\n  &lt;chr&gt;                                                     &lt;dbl&gt;\n1 American Indian / Alaska Native, NH                        28.2\n2 Asian, NH                                                  42.9\n3 Black, NH                                                  22.4\n4 Hispanic/Latino                                            32.2\n5 Multiple/Other, NH                                         32.4\n6 Native Hawaiian / Pacific Islander, NH                     30.3\n7 Overall                                                    32.6\n8 Unknown                                                    25.0\n9 White, NH                                                  33.2\n\n\nList of 2\n $ panel.background:List of 5\n  ..$ fill         : chr \"white\"\n  ..$ colour       : NULL\n  ..$ linewidth    : NULL\n  ..$ linetype     : NULL\n  ..$ inherit.blank: logi FALSE\n  ..- attr(*, \"class\")= chr [1:2] \"element_rect\" \"element\"\n $ plot.background :List of 5\n  ..$ fill         : chr \"white\"\n  ..$ colour       : NULL\n  ..$ linewidth    : NULL\n  ..$ linetype     : NULL\n  ..$ inherit.blank: logi FALSE\n  ..- attr(*, \"class\")= chr [1:2] \"element_rect\" \"element\"\n - attr(*, \"class\")= chr [1:2] \"theme\" \"gg\"\n - attr(*, \"complete\")= logi FALSE\n - attr(*, \"validate\")= logi TRUE\n\n\n\n\n\n\n\n\n\nSaving 7 x 5 in image\n\n\nThese means and standard deviations can be seen here as well in the data summary table.\n\n\n# A tibble: 9 × 3\n  Race.and.Ethnicity               avg_vaccination_cove…¹ sd_vaccination_cover…²\n  &lt;chr&gt;                                             &lt;dbl&gt;                  &lt;dbl&gt;\n1 American Indian / Alaska Native…                   28.2                   15.9\n2 Asian, NH                                          42.9                   22.7\n3 Black, NH                                          22.4                   12.6\n4 Hispanic/Latino                                    32.2                   17.8\n5 Multiple/Other, NH                                 32.4                   17.6\n6 Native Hawaiian / Pacific Islan…                   30.3                   17.4\n7 Overall                                            32.6                   18.0\n8 Unknown                                            25.0                   14.7\n9 White, NH                                          33.2                   18.8\n# ℹ abbreviated names: ¹​avg_vaccination_coverage, ²​sd_vaccination_coverage\n\n\nI also create a bar plot showing the mean vaccination coverage with standard deviation as error bars. Similar to the other bar plot, this displays the ranges and variation within each group vaccine coveraage. However, this graphic specifically includes each groups’ mean and standard deviation (error bars).\n\n\nSaving 7 x 5 in image\n\n\n\n\n\n\n\n\n\nFinally, to visualize the distribution of vaccination coverage (like a bell curve or a smooth curve) for each race/ethnicity group, I created a density plot for vaccination coverage by race/ethnicity. This was my favorite graphic of this exercise. It shows very explicitly where vaccine coverage for the majority of each group for the majority of the weeks recorded is falling. Such a graphic shows unfortunate trends such as the high density of American Indian/Native American vaccine coverage data plots falling in the 20-25% coverage range.\n\n\nSaving 7 x 5 in image\n\n\n\n\n\n\n\n\n\nThis portion of the assignment was contributed by Vincent Nguyen.\n\nFor this section, I had trouble replicating the exact vaccination coverage over time; I did ask ChatGPT for help on this but it became complicated really fast so I chose to start the data set in November to ensure the coverage made somewhat logical sense. There were tons of prompts as I was trying to create logisic growth curves and everything but I decided to just make it more simple. I asked ChatGPT to help introduce some noise and helped ensure the vaccination coverage increased over time according to the set mean and SD. The issue arises because the mean and standard deviation do not capture the actual spread of vaccination coverage, which increased over time rather than being randomly distributed.\n\n# First, need to understand distribution of race/ethcnitiy to replicate the denominator\ndenom_stats &lt;- data %&gt;%\n  group_by(Race.and.Ethnicity) %&gt;%\n  summarise(\n    mean_denominator = mean(Denominator, na.rm = TRUE),\n    sd_denominator = sd(Denominator, na.rm = TRUE)\n  )\n\n\n# Define race/ethnicity groups\nrace_ethnicity &lt;- c(\n  \"American Indian / Alaska Native, NH\", \"Asian, NH\", \"Black, NH\", \n  \"Hispanic/Latino\", \"Multiple/Other, NH\", \"Native Hawaiian / Pacific Islander, NH\", \n  \"Overall\", \"Unknown\", \"White, NH\"\n)\n\n# Define the summary stats for mean and SD of Denominator (based on denom_stats)\nsummary_stats_denominator &lt;- data.frame(\n  Race.Ethnicity = c(\n    \"American Indian / Alaska Native, NH\", \"Asian, NH\", \"Black, NH\", \n    \"Hispanic/Latino\", \"Multiple/Other, NH\", \"Native Hawaiian / Pacific Islander, NH\", \n    \"Overall\", \"Unknown\", \"White, NH\"\n  ),\n  mean_denominator = c(403.0788, 23625.3990, 15415.2266, 54375.3350, 7427.5222, 800.7389, 164107.4581, 11195.0443, 50865.1133),\n  sd_denominator = c(86.13829, 4898.93053, 3671.98063, 11469.75194, 1540.13275, 169.41929, 34907.95658, 2441.36489, 11256.72535)\n)\n\n# Define the summary stats for vaccination coverage (avg and sd) (based on data_summary)\nsummary_stats_vaccination &lt;- data.frame(\n  Race.Ethnicity = c(\n    \"American Indian / Alaska Native, NH\", \"Asian, NH\", \"Black, NH\", \n    \"Hispanic/Latino\", \"Multiple/Other, NH\", \"Native Hawaiian / Pacific Islander, NH\", \n    \"Overall\", \"Unknown\", \"White, NH\"\n  ),\n  avg_vaccination_coverage = c(28.22759, 42.87931, 22.42167, 32.17389, 32.35961, 30.25074, 32.58966, 25.03744, 33.15911),\n  sd_vaccination_coverage = c(15.92859, 22.72488, 12.61283, 17.83555, 17.62178, 17.41049, 18.04427, 14.71800, 18.81666)\n)\n\n# First, create date range from late November to early February\nflu_season_dates &lt;- seq(as.Date(\"2023-11-01\"), as.Date(\"2024-02-01\"), by = \"week\")\n\n# Create an empty data frame for synthetic data\nn_observations &lt;- length(flu_season_dates) * length(race_ethnicity)\nsynthetic_data &lt;- data.frame(\n  Vaccination.Coverage = numeric(n_observations),  \n  Flu.Season = as.Date(rep(NA, n_observations)),   \n  Denominator = numeric(n_observations),           \n  Race.Ethnicity = character(n_observations)       \n)\n\n# Populate the synthetic data frame with combinations of flu season dates and race/ethnicity\nrow_index &lt;- 1\n\n# Set seed for reproducibility\nset.seed(123) \n\nfor (week_date in flu_season_dates) {\n  for (race in race_ethnicity) {\n    # Get the mean and SD for the current race/ethnicity for Denominator\n    race_stats_denominator &lt;- summary_stats_denominator[summary_stats_denominator$Race.Ethnicity == race, ]\n    \n    # Generate a random population count from the normal distribution for Denominator\n    denominator_value &lt;- max(0, rnorm(1, race_stats_denominator$mean_denominator, race_stats_denominator$sd_denominator))\n    \n    # Get the mean and SD for the current race/ethnicity for Vaccination Coverage\n    race_stats_vaccination &lt;- summary_stats_vaccination[summary_stats_vaccination$Race.Ethnicity == race, ]\n    \n    # Calculate the week number (starting from 1 for the first week in the narrowed date range)\n    week_number &lt;- which(flu_season_dates == week_date)\n    \n    # Gradual increase for vaccination coverage across weeks\n    # Calculate the incremental change based on the week number\n    vaccination_coverage_growth &lt;- race_stats_vaccination$avg_vaccination_coverage + (week_number - 1) * (100 - race_stats_vaccination$avg_vaccination_coverage) / (length(flu_season_dates) - 1)\n    \n    # Add noise based on the mean and standard deviation for vaccination coverage\n    vaccination_coverage &lt;- rnorm(1, vaccination_coverage_growth, race_stats_vaccination$sd_vaccination_coverage)\n    \n    # Ensure vaccination coverage stays between 0% and 100%\n    vaccination_coverage &lt;- min(max(0, vaccination_coverage), 100)\n    \n    # Assign values to the synthetic data frame\n    synthetic_data[row_index, \"Flu.Season\"] &lt;- week_date\n    synthetic_data[row_index, \"Race.Ethnicity\"] &lt;- race\n    synthetic_data[row_index, \"Denominator\"] &lt;- denominator_value\n    synthetic_data[row_index, \"Vaccination.Coverage\"] &lt;- vaccination_coverage\n    \n    row_index &lt;- row_index + 1\n  }\n}\n\n# View the first few rows of the synthetic data frame\nhead(synthetic_data)\n\n  Vaccination.Coverage Flu.Season Denominator\n1            24.561187 2023-11-01    354.8004\n2            44.481605 2023-11-01  31261.4027\n3            44.053493 2023-11-01  15889.9687\n4             9.610827 2023-11-01  59661.9295\n5            24.506253 2023-11-01   6369.6776\n6            36.515275 2023-11-01   1008.1220\n                          Race.Ethnicity\n1    American Indian / Alaska Native, NH\n2                              Asian, NH\n3                              Black, NH\n4                        Hispanic/Latino\n5                     Multiple/Other, NH\n6 Native Hawaiian / Pacific Islander, NH\n\n\nCreation of Summary Stats and Graphs\nFor this section, I chose to mimic Hope’s original graphs and just change the inputs to use the synthetic data set instead. Some graphs do look reasonably similar but are skewed more positively because of the inclusion of certain dates only.\n\n# Create summary stats\ndata_synthetic_summary &lt;- synthetic_data %&gt;%\n    group_by(Race.Ethnicity) %&gt;%\n    summarize(\n        avg_vaccination_coverage = mean(Vaccination.Coverage, na.rm = TRUE),\n        sd_vaccination_coverage = sd(Vaccination.Coverage, na.rm = TRUE)\n    )\n\nhead(data_synthetic_summary)\n\n# A tibble: 6 × 3\n  Race.Ethnicity                   avg_vaccination_cove…¹ sd_vaccination_cover…²\n  &lt;chr&gt;                                             &lt;dbl&gt;                  &lt;dbl&gt;\n1 American Indian / Alaska Native…                   64.4                   24.3\n2 Asian, NH                                          71.8                   20.7\n3 Black, NH                                          59.3                   25.5\n4 Hispanic/Latino                                    66.8                   29.7\n5 Multiple/Other, NH                                 59.0                   24.6\n6 Native Hawaiian / Pacific Islan…                   67.5                   21.7\n# ℹ abbreviated names: ¹​avg_vaccination_coverage, ²​sd_vaccination_coverage\n\n# Visualize vaccination coverage by race and ethnicity with updated BINS\ngraph2 = ggplot(synthetic_data, aes(x = Vaccination.Coverage, fill = Race.Ethnicity)) + \n    geom_histogram(binwidth = 10, color = \"black\", position = \"dodge\") +\n    labs(title = \"Vaccination Coverage Distribution by Race and Ethnicity\",\n         x = \"Vaccination Coverage (%)\",\n         y = \"Frequency\") +\n    theme_minimal()\n\n#Display \nprint(graph2)\n\n\n\n\n\n\n\n# Create a bar plot showing the mean vaccination coverage with standard deviation as error bars\nbarplot2 = ggplot(data_synthetic_summary, aes(x = Race.Ethnicity, y = avg_vaccination_coverage, fill = Race.Ethnicity)) +\n    geom_bar(stat = \"identity\", color = \"black\", show.legend = FALSE) +  \n  \n    geom_errorbar(aes(ymin = avg_vaccination_coverage - sd_vaccination_coverage, \n                      ymax = avg_vaccination_coverage + sd_vaccination_coverage), \n                  width = 0.2) +  \n    labs(title = \"Mean Vaccination Coverage by Race and Ethnicity with Standard Deviation\",\n         x = \"Race and Ethnicity\",\n         y = \"Vaccination Coverage (%)\") +\n    theme_minimal() +\n    theme(axis.text.x = element_text(angle = 45, hjust = 1))  \n#Display\nprint(barplot2)\n\n\n\n\n\n\n\n# Create graph like how it was for the real data\ndensityplot2 = ggplot(synthetic_data, aes(x = Vaccination.Coverage, fill = Race.Ethnicity, color = Race.Ethnicity)) +\n  geom_density(alpha = 0.4) +  \n  labs(title = \"Density Plot of Vaccination Coverage by Race and Ethnicity (Synthetic Data)\",\n       x = \"Vaccination Coverage (%)\",\n       y = \"Density\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +  \n  scale_fill_brewer(palette = \"Spectral\") +  \n  scale_color_brewer(palette = \"Spectral\")  \n\n#Display \nprint(densityplot2)"
  },
  {
    "objectID": "aboutme.html",
    "href": "aboutme.html",
    "title": "About me",
    "section": "",
    "text": "My name is Hope. I am originally from Sioux Falls, South Dakota (FAQs: yes, we do have -50 degree wind chill; no, I did not own any livestock), but have lived in Georgia for the last five years.\nI’m a final semester (woo woo!) Master of Public Health student in the Environmental Health Science department. I am very passionate about all things at the intersection between the environmental science and human health, and applied to medical school this year which starts this July! In undergrad here at UGA, I was heavily involved in research within a reproductive toxicology lab. However, I have recently shifted gears slightly during my Masters education. This semester, I am currently completing an internship with the Southeastern Center of Excellence in Vector-Bourne Diseases’ CDC-funded project which focuses on vector-bourne disease transmission– more specifically, Lyme Disease and Alpha-Gal Syndrome.\n\n\n\nMe!"
  },
  {
    "objectID": "aboutme.html#hey-again",
    "href": "aboutme.html#hey-again",
    "title": "About me",
    "section": "",
    "text": "My name is Hope. I am originally from Sioux Falls, South Dakota (FAQs: yes, we do have -50 degree wind chill; no, I did not own any livestock), but have lived in Georgia for the last five years.\nI’m a final semester (woo woo!) Master of Public Health student in the Environmental Health Science department. I am very passionate about all things at the intersection between the environmental science and human health, and applied to medical school this year which starts this July! In undergrad here at UGA, I was heavily involved in research within a reproductive toxicology lab. However, I have recently shifted gears slightly during my Masters education. This semester, I am currently completing an internship with the Southeastern Center of Excellence in Vector-Bourne Diseases’ CDC-funded project which focuses on vector-bourne disease transmission– more specifically, Lyme Disease and Alpha-Gal Syndrome.\n\n\n\nMe!"
  },
  {
    "objectID": "data-exercise/data-exercise.html",
    "href": "data-exercise/data-exercise.html",
    "title": "Data Exercise - Synthetic Data",
    "section": "",
    "text": "# Set CRAN mirror (before installing any packages)\noptions(repos = c(CRAN = \"https://cran.rstudio.com\"))\n\n# Now install the package\ninstall.packages(\"simstudy\")\n\n\nThe downloaded binary packages are in\n    /var/folders/29/tsfhn2w952x9r8f27_hpzq900000gn/T//RtmpK3SRwu/downloaded_packages\n\n# Load the package after installation\nlibrary(simstudy)\n\n\n# Load necessary libraries\nlibrary(tidyverse)  # For data manipulation and visualization\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(ggplot2)      \n\n\n# Creation of Synthetic Data Set Relating Diet to Adverse Events & --------\n\n# Set a random seed for reproducibility\nset.seed(42)\n\n# Number of patients per diet group\nn_patients_per_group &lt;- 50  # 50 patients in each diet group\n\n# Total number of observations (patients x 10 days)\nn_patients &lt;- n_patients_per_group * 2  # Two diet groups\nn_days &lt;- 10  # 10 days of sampling per patient\n\n# Diet group assignments\ndiet_groups &lt;- rep(c(\"High-Sodium Diet\", \"Mediterranean Diet\"), each = n_patients_per_group)\n\n# Generate the fake patient data\npatient_data &lt;- data.frame(\n  patient_id = rep(1:n_patients, each = n_days),  # Unique patient ID\n  diet_group = rep(diet_groups, each = n_days),\n  day = rep(1:n_days, times = n_patients),  # Day of sample (1 to 10)\n  cholesterol = NA,  # Placeholder for cholesterol levels\n  systolic_bp = NA,  # Placeholder for systolic blood pressure\n  diastolic_bp = NA,  # Placeholder for diastolic blood pressure\n  drug_concentration = NA,  # Placeholder for drug concentrations\n  adverse_event = NA  # Placeholder for adverse events\n)\n\n# Simulate baseline cholesterol levels (random, with some variation between patients)\npatient_data$cholesterol &lt;- rnorm(n_patients, mean = 200, sd = 25)[patient_data$patient_id]\n\n# Blood pressure varies by diet group and is slightly elevated in the High-Sodium Diet group\npatient_data$systolic_bp &lt;- ifelse(patient_data$diet_group == \"High-Sodium Diet\", \n                                   rnorm(n_patients, mean = 145, sd = 10)[patient_data$patient_id],\n                                   rnorm(n_patients, mean = 130, sd = 8)[patient_data$patient_id])\n\npatient_data$diastolic_bp &lt;- ifelse(patient_data$diet_group == \"High-Sodium Diet\", \n                                    rnorm(n_patients, mean = 95, sd = 8)[patient_data$patient_id],\n                                    rnorm(n_patients, mean = 85, sd = 7)[patient_data$patient_id])\n\n# Drug concentrations decay over time, with higher concentration for Mediterranean Diet group initially\npatient_data$drug_concentration &lt;- ifelse(patient_data$diet_group == \"Mediterranean Diet\",\n                                          40 * exp(-0.08 * patient_data$day),  # Slower decay for Mediterranean diet\n                                          50 * exp(-0.1 * patient_data$day))   # Faster decay for High-Sodium Diet\n\n# Define a function to simulate adverse events based on drug concentration\nsimulate_adverse_event &lt;- function(concentration, diet_group) {\n  # Base probability of an adverse event as a function of drug concentration\n  base_prob &lt;- min(0.1 + (concentration / 100), 0.8)  # Capped at 80%\n  \n  # Diet type influences the adverse event rate: High-Sodium Diet may have more adverse events\n  if (diet_group == \"High-Sodium Diet\") {\n    base_prob &lt;- min(base_prob + 0.15, 0.90)  # Increase risk for High-Sodium Diet group\n  }\n  \n  rbinom(1, 1, base_prob)  # Simulate a binary outcome (0 = no event, 1 = event)\n}\n\n\n# Apply adverse event simulation to each observation (per patient, per day)\npatient_data$adverse_event &lt;- mapply(simulate_adverse_event, \n                                     patient_data$drug_concentration, \n                                     patient_data$diet_group)\n\n# Simulate some minor changes in cholesterol and blood pressure based on drug adherence and diet\n# Let's assume cholesterol improves slightly for Mediterranean diet over time and worsens for High-Sodium\n\npatient_data$cholesterol &lt;- ifelse(patient_data$diet_group == \"Mediterranean Diet\",\n                                   patient_data$cholesterol - (patient_data$day * 0.2),  # Slow reduction\n                                   patient_data$cholesterol + (patient_data$day * 0.3))  # Slow increase for High-Sodium\n\npatient_data$systolic_bp &lt;- patient_data$systolic_bp - (patient_data$day * 0.5)  # Systolic BP drops with time\npatient_data$diastolic_bp &lt;- patient_data$diastolic_bp - (patient_data$day * 0.3)  # Diastolic BP drops with time\n\n# Ensure BP stays within reasonable ranges\npatient_data$systolic_bp &lt;- pmax(patient_data$systolic_bp, 120)  # Prevent systolic BP from dropping too low\npatient_data$diastolic_bp &lt;- pmax(patient_data$diastolic_bp, 70)  # Prevent diastolic BP from dropping too low\n\n# Preview the first few rows of the dataset\nhead(patient_data)\n\n  patient_id       diet_group day cholesterol systolic_bp diastolic_bp\n1          1 High-Sodium Diet   1     234.574    156.5097     94.66303\n2          1 High-Sodium Diet   2     234.874    156.0097     94.36303\n3          1 High-Sodium Diet   3     235.174    155.5097     94.06303\n4          1 High-Sodium Diet   4     235.474    155.0097     93.76303\n5          1 High-Sodium Diet   5     235.774    154.5097     93.46303\n6          1 High-Sodium Diet   6     236.074    154.0097     93.16303\n  drug_concentration adverse_event\n1           45.24187             0\n2           40.93654             1\n3           37.04091             0\n4           33.51600             1\n5           30.32653             1\n6           27.44058             1\n\n# Optionally: Save the dataset to a CSV file\nwrite.csv(patient_data, \"synthetic_hypertension_lifestyle_data.csv\", row.names = FALSE)\n\n# Final output summary\nsummary(patient_data)  # Check summary statistics of the dataset\n\n   patient_id      diet_group             day        cholesterol   \n Min.   :  1.00   Length:1000        Min.   : 1.0   Min.   :123.2  \n 1st Qu.: 25.75   Class :character   1st Qu.: 3.0   1st Qu.:185.3  \n Median : 50.50   Mode  :character   Median : 5.5   Median :201.9  \n Mean   : 50.50                      Mean   : 5.5   Mean   :201.1  \n 3rd Qu.: 75.25                      3rd Qu.: 8.0   3rd Qu.:217.3  \n Max.   :100.00                      Max.   :10.0   Max.   :260.2  \n  systolic_bp     diastolic_bp    drug_concentration adverse_event \n Min.   :120.0   Min.   : 70.00   Min.   :17.97      Min.   :0.00  \n 1st Qu.:126.8   1st Qu.: 82.11   1st Qu.:22.12      1st Qu.:0.00  \n Median :133.7   Median : 87.55   Median :27.13      Median :0.00  \n Mean   :134.4   Mean   : 87.76   Mean   :28.25      Mean   :0.46  \n 3rd Qu.:140.5   3rd Qu.: 93.61   3rd Qu.:33.66      3rd Qu.:1.00  \n Max.   :171.5   Max.   :109.18   Max.   :45.24      Max.   :1.00  \n\n# Create a copy of the original dataset - did not konw that you could make a copy to play with within the same script\npatient_data_copy &lt;- patient_data\n\n\n# Making Fake Data More Real - 4 ways -------------------------------------\n\n# Add random noise to drug concentration (e.g., small fluctuations)\npatient_data_copy$drug_concentration &lt;- patient_data_copy$drug_concentration + rnorm(nrow(patient_data_copy), mean = 0, sd = 2)\n\n# Increase cholesterol levels by 5% for all patients (simulating a small change)\npatient_data_copy$cholesterol &lt;- patient_data_copy$cholesterol * 1.05\n\n# Change blood pressure by adding a constant value to systolic and diastolic\npatient_data_copy$systolic_bp &lt;- patient_data_copy$systolic_bp + 3  # Increase systolic BP by 3 mmHg\npatient_data_copy$diastolic_bp &lt;- patient_data_copy$diastolic_bp + 2  # Increase diastolic BP by 2 mmHg\n\n# Randomly alter the treatment group for some patients (e.g., swap a small percentage)\nset.seed(123)  # For reproducibility\nswap_indices &lt;- sample(1:nrow(patient_data_copy), size = round(0.1 * nrow(patient_data_copy)), replace = FALSE)  # 10% of patients\npatient_data_copy$diet_group[swap_indices] &lt;- ifelse(patient_data_copy$diet_group[swap_indices] == \"High-Sodium Diet\", \n                                                     \"Mediterranean Diet\", \n                                                     \"High-Sodium Diet\")\n# Check the first few rows of the modified copy\nhead(patient_data_copy)\n\n  patient_id       diet_group day cholesterol systolic_bp diastolic_bp\n1          1 High-Sodium Diet   1    246.3027    159.5097     96.66303\n2          1 High-Sodium Diet   2    246.6177    159.0097     96.36303\n3          1 High-Sodium Diet   3    246.9327    158.5097     96.06303\n4          1 High-Sodium Diet   4    247.2477    158.0097     95.76303\n5          1 High-Sodium Diet   5    247.5627    157.5097     95.46303\n6          1 High-Sodium Diet   6    247.8777    157.0097     95.16303\n  drug_concentration adverse_event\n1           49.89199             0\n2           41.98478             1\n3           38.98238             0\n4           34.26995             1\n5           28.33467             1\n6           26.24562             1\n\n\n\n# Creating The Graphs -----------------------------------------------------\n\n\n# Plot for the diet study hypothetical - the original ---------------------\n\n# Load necessary libraries\nlibrary(ggplot2)\nlibrary(dplyr)\n\np2 &lt;- ggplot(patient_data, aes(x = as.factor(adverse_event), y = drug_concentration, \n                               fill = diet_group)) +\n  # Boxplot showing the distribution of drug concentration by adverse event and diet\n  geom_boxplot(width = 0.7, position = position_dodge(width = 0.8), color = \"black\") +\n  \n  # Overlay raw data points on the boxplot\n  geom_point(aes(color = diet_group), position = position_dodge(width = 0.8), \n             size = 3, shape = 16) + \n  \n  # Add axis labels and a title\n  labs(\n    x = \"Adverse Events\",   # X-axis label\n    y = \"Drug Concentration\",   # Y-axis label\n    title = \"Drug Concentration by Adverse Events and Diet Type\"   # Plot title\n  ) +\n  \n  # Custom color scale for diet groups\n  scale_color_manual(values = c(\"High-Sodium Diet\" = \"red\", \"Mediterranean Diet\" = \"blue\")) +  # Customize color for each diet group\n  scale_fill_manual(values = c(\"High-Sodium Diet\" = \"red\", \"Mediterranean Diet\" = \"blue\")) +  # Fill color for boxplot\n  \n  # Set the theme for minimal styling\n  theme_minimal() +\n  \n  # Adjust legend and other aesthetic settings\n  theme(legend.position = \"top\", \n        legend.title = element_blank(),\n        legend.text = element_text(size = 12),\n        axis.text = element_text(size = 12),\n        axis.title = element_text(size = 14))\n\n# Plot the graphic\nplot(p2)\n\n\n\n\n\n\n\n# Creating a Copy of the Data, Adding Noise, and Changing Conditions --------\n\n# Create a copy of the original dataset\npatient_data_copy &lt;- patient_data\n\n# Example 1: Add random noise to drug concentration (e.g., small fluctuations)\npatient_data_copy$drug_concentration &lt;- patient_data_copy$drug_concentration + rnorm(nrow(patient_data_copy), mean = 0, sd = 2)\n\n# Example 2: Increase cholesterol levels by 5% for all patients (simulating a small change)\npatient_data_copy$cholesterol &lt;- patient_data_copy$cholesterol * 1.05\n\n# Example 3: Change blood pressure by adding a constant value to systolic and diastolic\npatient_data_copy$systolic_bp &lt;- patient_data_copy$systolic_bp + 3  # Increase systolic BP by 3 mmHg\npatient_data_copy$diastolic_bp &lt;- patient_data_copy$diastolic_bp + 2  # Increase diastolic BP by 2 mmHg\n\n# Example 4: Randomly alter the treatment group for some patients (e.g., swap a small percentage)\nset.seed(123)  # For reproducibility\nswap_indices &lt;- sample(1:nrow(patient_data_copy), size = round(0.1 * nrow(patient_data_copy)), replace = FALSE)  # 10% of patients\npatient_data_copy$diet_group[swap_indices] &lt;- ifelse(patient_data_copy$diet_group[swap_indices] == \"High-Sodium Diet\", \n                                                     \"Mediterranean Diet\", \n                                                     \"High-Sodium Diet\")\n# Check the first few rows of the modified copy\nhead(patient_data_copy)\n\n  patient_id       diet_group day cholesterol systolic_bp diastolic_bp\n1          1 High-Sodium Diet   1    246.3027    159.5097     96.66303\n2          1 High-Sodium Diet   2    246.6177    159.0097     96.36303\n3          1 High-Sodium Diet   3    246.9327    158.5097     96.06303\n4          1 High-Sodium Diet   4    247.2477    158.0097     95.76303\n5          1 High-Sodium Diet   5    247.5627    157.5097     95.46303\n6          1 High-Sodium Diet   6    247.8777    157.0097     95.16303\n  drug_concentration adverse_event\n1           44.20013             0\n2           44.18294             1\n3           34.90077             0\n4           36.88778             1\n5           29.84315             1\n6           26.50418             1\n\n\n\n# Creating Second Plot with Added Noise  ----------------------------------\n\np3 &lt;- ggplot(patient_data_copy, aes(x = as.factor(adverse_event), y = drug_concentration, \n                               fill = diet_group)) +\n  # Boxplot showing the distribution of drug concentration by adverse event and diet\n  geom_boxplot(width = 0.7, position = position_dodge(width = 0.8), color = \"black\") +\n  \n  # Overlay raw data points on the boxplot\n  geom_point(aes(color = diet_group), position = position_dodge(width = 0.8), \n             size = 3, shape = 16) + \n  \n  # Add axis labels and a title\n  labs(\n    x = \"Adverse Events\",   # X-axis label\n    y = \"Drug Concentration\",   # Y-axis label\n    title = \"Drug Concentration by Adverse Events and Diet Type with Complications\"   # Plot title\n  ) +\n  \n  # Custom color scale for diet groups\n  scale_color_manual(values = c(\"High-Sodium Diet\" = \"red\", \"Mediterranean Diet\" = \"blue\")) +  # Customize color for each diet group\n  scale_fill_manual(values = c(\"High-Sodium Diet\" = \"red\", \"Mediterranean Diet\" = \"blue\")) +  # Fill color for boxplot\n  \n  # Set the theme for minimal styling\n  theme_minimal() +\n  \n  # Adjust legend and other aesthetic settings\n  theme(legend.position = \"top\", \n        legend.title = element_blank(),\n        legend.text = element_text(size = 12),\n        axis.text = element_text(size = 12),\n        axis.title = element_text(size = 14))\n\n# Plot the graphic\nplot(p3)\n\n\n\n\n\n\n\n\n\n# Inserting Realism through the SimStudy Package --------------------------\n\n\n# Install and load simstudy package if not already installed\ninstall.packages(\"simstudy\")\n\n\nThe downloaded binary packages are in\n    /var/folders/29/tsfhn2w952x9r8f27_hpzq900000gn/T//RtmpK3SRwu/downloaded_packages\n\nlibrary(simstudy)\n\n# Set seed for reproducibility\nset.seed(42)\n\n# Introduce extreme cholesterol outliers\ncholesterol_outliers &lt;- sample(1:nrow(patient_data), size = 6)  # Randomly pick 5 rows\npatient_data$cholesterol[cholesterol_outliers] &lt;- patient_data$cholesterol[cholesterol_outliers] + rnorm(5, mean = 100, sd = 20)  # Adding extreme high values\n\nWarning in patient_data$cholesterol[cholesterol_outliers] + rnorm(5, mean =\n100, : longer object length is not a multiple of shorter object length\n\ncholesterol_outliers_data = patient_data$cholesterol[cholesterol_outliers] \n\n# Create Plots with Extreme Patients Added In -----------------------------\n\n# Create a boxplot for drug concentration with the outliers in cholesterol\np4 &lt;- ggplot(patient_data, aes(x = factor(cholesterol &gt; 300), y = drug_concentration, fill = cholesterol &gt; 300)) +\n  geom_boxplot() +\n  labs(x = \"Cholesterol (Outliers vs Normal)\", \n       y = \"Drug Concentration\", \n       title = \"Boxplot of Drug Concentration with Cholesterol Outliers\") +\n  scale_fill_manual(values = c(\"red\", \"blue\")) +\n  scale_x_discrete(labels = c(\"FALSE\" = \"Normal\", \"TRUE\" = \"Outlier\")) +  # Change labels for cholesterol categories\n  # Different color and different labels for data with outliers included v. normal (excluded)\n  theme_minimal()\n\n# Plot the graphic\nplot(p4)"
  },
  {
    "objectID": "coding-exercise/coding-exercise.html",
    "href": "coding-exercise/coding-exercise.html",
    "title": "R Coding Exercise",
    "section": "",
    "text": "# To load the dslabs package\nlibrary(dslabs)\n# Look at help file for gapminder data\nhelp(gapminder)\n# Get an overview of the data structure \nstr(gapminder)\n\n'data.frame':   10545 obs. of  9 variables:\n $ country         : Factor w/ 185 levels \"Albania\",\"Algeria\",..: 1 2 3 4 5 6 7 8 9 10 ...\n $ year            : int  1960 1960 1960 1960 1960 1960 1960 1960 1960 1960 ...\n $ infant_mortality: num  115.4 148.2 208 NA 59.9 ...\n $ life_expectancy : num  62.9 47.5 36 63 65.4 ...\n $ fertility       : num  6.19 7.65 7.32 4.43 3.11 4.55 4.82 3.45 2.7 5.57 ...\n $ population      : num  1636054 11124892 5270844 54681 20619075 ...\n $ gdp             : num  NA 1.38e+10 NA NA 1.08e+11 ...\n $ continent       : Factor w/ 5 levels \"Africa\",\"Americas\",..: 4 1 1 2 2 3 2 5 4 3 ...\n $ region          : Factor w/ 22 levels \"Australia and New Zealand\",..: 19 11 10 2 15 21 2 1 22 21 ...\n\n# Generate a summary of the data \nsummary(gapminder)\n\n                country           year      infant_mortality life_expectancy\n Albania            :   57   Min.   :1960   Min.   :  1.50   Min.   :13.20  \n Algeria            :   57   1st Qu.:1974   1st Qu.: 16.00   1st Qu.:57.50  \n Angola             :   57   Median :1988   Median : 41.50   Median :67.54  \n Antigua and Barbuda:   57   Mean   :1988   Mean   : 55.31   Mean   :64.81  \n Argentina          :   57   3rd Qu.:2002   3rd Qu.: 85.10   3rd Qu.:73.00  \n Armenia            :   57   Max.   :2016   Max.   :276.90   Max.   :83.90  \n (Other)            :10203                  NA's   :1453                    \n   fertility       population             gdp               continent   \n Min.   :0.840   Min.   :3.124e+04   Min.   :4.040e+07   Africa  :2907  \n 1st Qu.:2.200   1st Qu.:1.333e+06   1st Qu.:1.846e+09   Americas:2052  \n Median :3.750   Median :5.009e+06   Median :7.794e+09   Asia    :2679  \n Mean   :4.084   Mean   :2.701e+07   Mean   :1.480e+11   Europe  :2223  \n 3rd Qu.:6.000   3rd Qu.:1.523e+07   3rd Qu.:5.540e+10   Oceania : 684  \n Max.   :9.220   Max.   :1.376e+09   Max.   :1.174e+13                  \n NA's   :187     NA's   :185         NA's   :2972                       \n             region    \n Western Asia   :1026  \n Eastern Africa : 912  \n Western Africa : 912  \n Caribbean      : 741  \n South America  : 684  \n Southern Europe: 684  \n (Other)        :5586\n# Determine the type of object gapminder is\nclass(gapminder)\n\n[1] \"data.frame\"\n\n# Subset the dataset to only include African countries\nafricadata &lt;- gapminder[gapminder$continent == \"Africa\", ]\n# Check the structure of the new africadata object\nstr(africadata)\n\n'data.frame':   2907 obs. of  9 variables:\n $ country         : Factor w/ 185 levels \"Albania\",\"Algeria\",..: 2 3 18 22 26 27 29 31 32 33 ...\n $ year            : int  1960 1960 1960 1960 1960 1960 1960 1960 1960 1960 ...\n $ infant_mortality: num  148 208 187 116 161 ...\n $ life_expectancy : num  47.5 36 38.3 50.3 35.2 ...\n $ fertility       : num  7.65 7.32 6.28 6.62 6.29 6.95 5.65 6.89 5.84 6.25 ...\n $ population      : num  11124892 5270844 2431620 524029 4829291 ...\n $ gdp             : num  1.38e+10 NA 6.22e+08 1.24e+08 5.97e+08 ...\n $ continent       : Factor w/ 5 levels \"Africa\",\"Americas\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ region          : Factor w/ 22 levels \"Australia and New Zealand\",..: 11 10 20 17 20 5 10 20 10 10 ...\n\n# Check a summary of the new africadata object\nsummary(africadata)\n\n         country          year      infant_mortality life_expectancy\n Algeria     :  57   Min.   :1960   Min.   : 11.40   Min.   :13.20  \n Angola      :  57   1st Qu.:1974   1st Qu.: 62.20   1st Qu.:48.23  \n Benin       :  57   Median :1988   Median : 93.40   Median :53.98  \n Botswana    :  57   Mean   :1988   Mean   : 95.12   Mean   :54.38  \n Burkina Faso:  57   3rd Qu.:2002   3rd Qu.:124.70   3rd Qu.:60.10  \n Burundi     :  57   Max.   :2016   Max.   :237.40   Max.   :77.60  \n (Other)     :2565                  NA's   :226                     \n   fertility       population             gdp               continent   \n Min.   :1.500   Min.   :    41538   Min.   :4.659e+07   Africa  :2907  \n 1st Qu.:5.160   1st Qu.:  1605232   1st Qu.:8.373e+08   Americas:   0  \n Median :6.160   Median :  5570982   Median :2.448e+09   Asia    :   0  \n Mean   :5.851   Mean   : 12235961   Mean   :9.346e+09   Europe  :   0  \n 3rd Qu.:6.860   3rd Qu.: 13888152   3rd Qu.:6.552e+09   Oceania :   0  \n Max.   :8.450   Max.   :182201962   Max.   :1.935e+11                  \n NA's   :51      NA's   :51          NA's   :637                        \n                       region   \n Eastern Africa           :912  \n Western Africa           :912  \n Middle Africa            :456  \n Northern Africa          :342  \n Southern Africa          :285  \n Australia and New Zealand:  0  \n (Other)                  :  0\n# Determine the type of object gapminder is\nclass(gapminder)\n\n[1] \"data.frame\"\n\n# Subset the dataset to only include African countries\nafricadata &lt;- gapminder[gapminder$continent == \"Africa\", ]\n# Check the structure of the new africadata object\nstr(africadata)\n\n'data.frame':   2907 obs. of  9 variables:\n $ country         : Factor w/ 185 levels \"Albania\",\"Algeria\",..: 2 3 18 22 26 27 29 31 32 33 ...\n $ year            : int  1960 1960 1960 1960 1960 1960 1960 1960 1960 1960 ...\n $ infant_mortality: num  148 208 187 116 161 ...\n $ life_expectancy : num  47.5 36 38.3 50.3 35.2 ...\n $ fertility       : num  7.65 7.32 6.28 6.62 6.29 6.95 5.65 6.89 5.84 6.25 ...\n $ population      : num  11124892 5270844 2431620 524029 4829291 ...\n $ gdp             : num  1.38e+10 NA 6.22e+08 1.24e+08 5.97e+08 ...\n $ continent       : Factor w/ 5 levels \"Africa\",\"Americas\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ region          : Factor w/ 22 levels \"Australia and New Zealand\",..: 11 10 20 17 20 5 10 20 10 10 ...\n\n# Check a summary of the new africadata object\nsummary(africadata)\n\n         country          year      infant_mortality life_expectancy\n Algeria     :  57   Min.   :1960   Min.   : 11.40   Min.   :13.20  \n Angola      :  57   1st Qu.:1974   1st Qu.: 62.20   1st Qu.:48.23  \n Benin       :  57   Median :1988   Median : 93.40   Median :53.98  \n Botswana    :  57   Mean   :1988   Mean   : 95.12   Mean   :54.38  \n Burkina Faso:  57   3rd Qu.:2002   3rd Qu.:124.70   3rd Qu.:60.10  \n Burundi     :  57   Max.   :2016   Max.   :237.40   Max.   :77.60  \n (Other)     :2565                  NA's   :226                     \n   fertility       population             gdp               continent   \n Min.   :1.500   Min.   :    41538   Min.   :4.659e+07   Africa  :2907  \n 1st Qu.:5.160   1st Qu.:  1605232   1st Qu.:8.373e+08   Americas:   0  \n Median :6.160   Median :  5570982   Median :2.448e+09   Asia    :   0  \n Mean   :5.851   Mean   : 12235961   Mean   :9.346e+09   Europe  :   0  \n 3rd Qu.:6.860   3rd Qu.: 13888152   3rd Qu.:6.552e+09   Oceania :   0  \n Max.   :8.450   Max.   :182201962   Max.   :1.935e+11                  \n NA's   :51      NA's   :51          NA's   :637                        \n                       region   \n Eastern Africa           :912  \n Western Africa           :912  \n Middle Africa            :456  \n Northern Africa          :342  \n Southern Africa          :285  \n Australia and New Zealand:  0  \n (Other)                  :  0\n# Create an object containing only the 'infant_mortality' variable\nafricadata_infant &lt;- africadata[, \"infant_mortality\"]\n# Check the structure of the new object\nstr(africadata_infant)\n\n num [1:2907] 148 208 187 116 161 ...\n\n# Check a summary of the new infant mortality object\nsummary(africadata_infant)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  11.40   62.20   93.40   95.12  124.70  237.40     226\n# Create an object containing only the 'infant_mortality' variable\nafricadata_infant &lt;- africadata[, \"infant_mortality\"]\n# Check the structure of the new object\nstr(africadata_infant)\n\n num [1:2907] 148 208 187 116 161 ...\n\n# Check a summary of the new infant mortality object\nsummary(africadata_infant)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  11.40   62.20   93.40   95.12  124.70  237.40     226\n# Create an object containing only the 'life_expectancy' variable\nafricadata_life &lt;- africadata[, \"life_expectancy\"]\n# Check the structure of the new life ex. object\nstr(africadata_life)\n\n num [1:2907] 47.5 36 38.3 50.3 35.2 ...\n\n# Check a summary of the new life ex. object\nsummary(africadata_life)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  13.20   48.23   53.98   54.38   60.10   77.60\n# Create an object containing only the 'life_expectancy' variable\nafricadata_life &lt;- africadata[, \"life_expectancy\"]\n# Check the structure of the new life ex. object\nstr(africadata_life)\n\n num [1:2907] 47.5 36 38.3 50.3 35.2 ...\n\n# Check a summary of the new life ex. object\nsummary(africadata_life)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  13.20   48.23   53.98   54.38   60.10   77.60\n# Load the plotting tools library through ggplot package\nlibrary(ggplot2)\n\n# Plot life expectancy as a function of infant mortality + theme tool is something found in use of AI for my plots \nggplot(africadata, aes(x = infant_mortality, y = life_expectancy)) +\n  geom_point() +  \n\n# Plot above data as points\n  labs(\n    title = \"Life Expectancy vs. Infant Mortality\",\n    x = \"Infant Mortality (per 1000 live births)\",\n    y = \"Life Expectancy (years)\"\n  ) +\n  theme_minimal()\n\nWarning: Removed 226 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n# Plot life expectancy as a function of population size with log scale on x-axis, using the + theme tool is something found in use of AI for my plots \nggplot(africadata, aes(x = population, y = life_expectancy)) +\n  geom_point() +  \n# Plot above data as points\n  scale_x_log10() +  \n# Set x-axis to log scale\n  labs(\n    title = \"Life Expectancy vs. Population Size\",\n    x = \"Population Size (log scale)\",\n    y = \"Life Expectancy (years)\"\n  ) +\n  theme_minimal()\n\nWarning: Removed 51 rows containing missing values or values outside the scale range\n(`geom_point()`)."
  },
  {
    "objectID": "coding-exercise/coding-exercise.html#the-data-streaks-are-likely-due-to-the-large-number-of-data-plots-that-are-close-in-relative-number-especially-on-a-log-scale.-this-makes-the-data-appear-to-be-in-streaks-rather-than-in-individual-points.",
    "href": "coding-exercise/coding-exercise.html#the-data-streaks-are-likely-due-to-the-large-number-of-data-plots-that-are-close-in-relative-number-especially-on-a-log-scale.-this-makes-the-data-appear-to-be-in-streaks-rather-than-in-individual-points.",
    "title": "R Coding Exercise",
    "section": "The data streaks are likely due to the large number of data plots that are close in relative number, especially on a log scale. This makes the data appear to be in streaks rather than in individual points.",
    "text": "The data streaks are likely due to the large number of data plots that are close in relative number, especially on a log scale. This makes the data appear to be in streaks rather than in individual points.\n\n# Identify rows where infant_mortality is missing (NA)\nmissing_infant_mortality &lt;- africadata[is.na(africadata$infant_mortality), ]\n# Extract the unique years with missing infant mortality data\nmissing_years &lt;- unique(missing_infant_mortality$year)\n# Print the result\nmissing_years\n\n [1] 1960 1961 1962 1963 1964 1965 1966 1967 1968 1969 1970 1971 1972 1973 1974\n[16] 1975 1976 1977 1978 1979 1980 1981 2016\n\n# Subset the africadata object to only include data for the year 2000\nafricadata_2000 &lt;- africadata[africadata$year == 2000, ]\n# Check the structure of the new object\nstr(africadata_2000)\n\n'data.frame':   51 obs. of  9 variables:\n $ country         : Factor w/ 185 levels \"Albania\",\"Algeria\",..: 2 3 18 22 26 27 29 31 32 33 ...\n $ year            : int  2000 2000 2000 2000 2000 2000 2000 2000 2000 2000 ...\n $ infant_mortality: num  33.9 128.3 89.3 52.4 96.2 ...\n $ life_expectancy : num  73.3 52.3 57.2 47.6 52.6 46.7 54.3 68.4 45.3 51.5 ...\n $ fertility       : num  2.51 6.84 5.98 3.41 6.59 7.06 5.62 3.7 5.45 7.35 ...\n $ population      : num  31183658 15058638 6949366 1736579 11607944 ...\n $ gdp             : num  5.48e+10 9.13e+09 2.25e+09 5.63e+09 2.61e+09 ...\n $ continent       : Factor w/ 5 levels \"Africa\",\"Americas\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ region          : Factor w/ 22 levels \"Australia and New Zealand\",..: 11 10 20 17 20 5 10 20 10 10 ...\n\n# Check a summary of the new object\nsummary(africadata_2000)\n\n         country        year      infant_mortality life_expectancy\n Algeria     : 1   Min.   :2000   Min.   : 12.30   Min.   :37.60  \n Angola      : 1   1st Qu.:2000   1st Qu.: 60.80   1st Qu.:51.75  \n Benin       : 1   Median :2000   Median : 80.30   Median :54.30  \n Botswana    : 1   Mean   :2000   Mean   : 78.93   Mean   :56.36  \n Burkina Faso: 1   3rd Qu.:2000   3rd Qu.:103.30   3rd Qu.:60.00  \n Burundi     : 1   Max.   :2000   Max.   :143.30   Max.   :75.00  \n (Other)     :45                                                  \n   fertility       population             gdp               continent \n Min.   :1.990   Min.   :    81154   Min.   :2.019e+08   Africa  :51  \n 1st Qu.:4.150   1st Qu.:  2304687   1st Qu.:1.274e+09   Americas: 0  \n Median :5.550   Median :  8799165   Median :3.238e+09   Asia    : 0  \n Mean   :5.156   Mean   : 15659800   Mean   :1.155e+10   Europe  : 0  \n 3rd Qu.:5.960   3rd Qu.: 17391242   3rd Qu.:8.654e+09   Oceania : 0  \n Max.   :7.730   Max.   :122876723   Max.   :1.329e+11                \n                                                                      \n                       region  \n Eastern Africa           :16  \n Western Africa           :16  \n Middle Africa            : 8  \n Northern Africa          : 6  \n Southern Africa          : 5  \n Australia and New Zealand: 0  \n (Other)                  : 0  \n\n\n\n# Load necessary libraries again \nlibrary(ggplot2)\n\n# Make Plot 1: Life Expectancy vs. Infant Mortality for the year 2000\nggplot(africadata_2000, aes(x = infant_mortality, y = life_expectancy)) +\n  geom_point() +  \n\n# Plot data as points\n  labs(\n    title = \"Life Expectancy vs. Infant Mortality (Year 2000)\",\n    x = \"Infant Mortality (per 1000 live births)\",\n    y = \"Life Expectancy (years)\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n# Plot 2: Life Expectancy vs. Population Size (Log scale) for the year 2000\nggplot(africadata_2000, aes(x = population, y = life_expectancy)) +\n  geom_point() +  \n\n# Plot data as points\n  scale_x_log10() +  \n\n# Set x-axis to log scale\n  labs(\n    title = \"Life Expectancy vs. Population Size (Year 2000)\",\n    x = \"Population Size (log scale)\",\n    y = \"Life Expectancy (years)\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n# Fit a simple linear model with life expectancy as the outcome and infant mortality as the predictor\nfit1 &lt;- lm(life_expectancy ~ infant_mortality, data = africadata_2000)\n# Apply the summary to fit1 to display the results\nsummary(fit1)\n\n\nCall:\nlm(formula = life_expectancy ~ infant_mortality, data = africadata_2000)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-22.6651  -3.7087   0.9914   4.0408   8.6817 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      71.29331    2.42611  29.386  &lt; 2e-16 ***\ninfant_mortality -0.18916    0.02869  -6.594 2.83e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.221 on 49 degrees of freedom\nMultiple R-squared:  0.4701,    Adjusted R-squared:  0.4593 \nF-statistic: 43.48 on 1 and 49 DF,  p-value: 2.826e-08\n\n# Fit a linear model with life expectancy as the outcome and population size as the predictor\nfit2 &lt;- lm(life_expectancy ~ population, data = africadata_2000)\n# Apply the summary to fit2 to display the results\nsummary(fit2)\n\n\nCall:\nlm(formula = life_expectancy ~ population, data = africadata_2000)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-18.429  -4.602  -2.568   3.800  18.802 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 5.593e+01  1.468e+00  38.097   &lt;2e-16 ***\npopulation  2.756e-08  5.459e-08   0.505    0.616    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.524 on 49 degrees of freedom\nMultiple R-squared:  0.005176,  Adjusted R-squared:  -0.01513 \nF-statistic: 0.2549 on 1 and 49 DF,  p-value: 0.6159"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My website and data analysis portfolio",
    "section": "",
    "text": "Hello! My name is Hope. :)\nWelcome to my website and data analysis portfolio.\n\nPlease use the Menu Bar above to look around.\nSee you soon!"
  },
  {
    "objectID": "fitting-exercise/fitting-model.html#prompt-1",
    "href": "fitting-exercise/fitting-model.html#prompt-1",
    "title": "Fitting Model",
    "section": "Prompt #1",
    "text": "Prompt #1\n“Write code to load the data into R. Then write code to make a plot that shows a line for each individual, with DV on the y-axis and time on the x-axis. Stratify by dose (e.g., use a different color for each dose, or facets).\nWrite code that keeps only observations with OCC = 1.\nWrite code to exclude the observations with TIME = 0, then compute the sum of the DV variable for each individual using dplyr::summarize(). Call this variable Y. The result from this step should be a data frame/tibble of size 120 x 2, one column for the ID one for the variable Y. Next, create a data frame that contains only the observations where TIME == 0. This should be a tibble of size 120 x 17. Finally, use the appropriate join function to combine those two data frames, to get a data frame of size 120 x 18.\nWrite code that converts RACE and SEX to factor variables and keeps only these variables: Y,DOSE,AGE,SEX,RACE,WT,HT”\n“Make some useful summary tables. Show some scatterplots or boxplots between the main outcome of interest (total drug, Y) and other predictors. Plot the distributions of your variables to make sure they all make sense. Look at some pair/correlation plots.”\n\n# Install and load necessary packages\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n# Or manually specify a mirror\noptions(repos = c(CRAN = \"https://cloud.r-project.org/\"))\n# Now try installing the package\ninstall.packages(\"GGally\")\n\n\nThe downloaded binary packages are in\n    /var/folders/29/tsfhn2w952x9r8f27_hpzq900000gn/T//Rtmpzywett/downloaded_packages\n\nlibrary(GGally)\n\nRegistered S3 method overwritten by 'GGally':\n  method from   \n  +.gg   ggplot2\n\nlibrary(skimr)\n\n# Load the Mavoglurant_A2121_nmpk.csv data\ndf &lt;- read.csv(here::here(\"fitting-exercise\", \"Mavoglurant_A2121_nmpk.csv\"))\n\n# Initial exploration\nglimpse(df)\n\nRows: 2,678\nColumns: 17\n$ ID   &lt;int&gt; 793, 793, 793, 793, 793, 793, 793, 793, 793, 793, 793, 793, 793, …\n$ CMT  &lt;int&gt; 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2,…\n$ EVID &lt;int&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,…\n$ EVI2 &lt;int&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,…\n$ MDV  &lt;int&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,…\n$ DV   &lt;dbl&gt; 0.00, 491.00, 605.00, 556.00, 310.00, 237.00, 147.00, 101.00, 72.…\n$ LNDV &lt;dbl&gt; 0.000, 6.196, 6.405, 6.321, 5.737, 5.468, 4.990, 4.615, 4.282, 3.…\n$ AMT  &lt;dbl&gt; 25, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 25, 0, 0, 0, 0, …\n$ TIME &lt;dbl&gt; 0.000, 0.200, 0.250, 0.367, 0.533, 0.700, 1.200, 2.200, 3.200, 4.…\n$ DOSE &lt;dbl&gt; 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 2…\n$ OCC  &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ RATE &lt;int&gt; 75, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 150, 0, 0, 0, 0,…\n$ AGE  &lt;int&gt; 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 2…\n$ SEX  &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ RACE &lt;int&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,…\n$ WT   &lt;dbl&gt; 94.3, 94.3, 94.3, 94.3, 94.3, 94.3, 94.3, 94.3, 94.3, 94.3, 94.3,…\n$ HT   &lt;dbl&gt; 1.769997, 1.769997, 1.769997, 1.769997, 1.769997, 1.769997, 1.769…\n\nskim(df)\n\n\nData summary\n\n\nName\ndf\n\n\nNumber of rows\n2678\n\n\nNumber of columns\n17\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n17\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nID\n0\n1\n858.81\n34.08\n793.00\n832.00\n860.00\n888.00\n915.00\n▅▆▇▇▇\n\n\nCMT\n0\n1\n1.93\n0.26\n1.00\n2.00\n2.00\n2.00\n2.00\n▁▁▁▁▇\n\n\nEVID\n0\n1\n0.07\n0.26\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\nEVI2\n0\n1\n0.16\n0.70\n0.00\n0.00\n0.00\n0.00\n4.00\n▇▁▁▁▁\n\n\nMDV\n0\n1\n0.09\n0.29\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\nDV\n0\n1\n179.93\n226.28\n0.00\n23.52\n74.20\n283.00\n1730.00\n▇▂▁▁▁\n\n\nLNDV\n0\n1\n4.08\n1.88\n0.00\n3.16\n4.31\n5.64\n7.46\n▃▃▇▇▅\n\n\nAMT\n0\n1\n2.76\n10.32\n0.00\n0.00\n0.00\n0.00\n50.00\n▇▁▁▁▁\n\n\nTIME\n0\n1\n5.85\n8.91\n0.00\n0.58\n2.25\n6.36\n48.22\n▇▁▁▁▁\n\n\nDOSE\n0\n1\n37.37\n12.05\n25.00\n25.00\n37.50\n50.00\n50.00\n▇▁▁▁▇\n\n\nOCC\n0\n1\n1.38\n0.49\n1.00\n1.00\n1.00\n2.00\n2.00\n▇▁▁▁▅\n\n\nRATE\n0\n1\n16.55\n61.88\n0.00\n0.00\n0.00\n0.00\n300.00\n▇▁▁▁▁\n\n\nAGE\n0\n1\n32.90\n8.87\n18.00\n26.00\n31.00\n40.00\n50.00\n▆▇▅▅▅\n\n\nSEX\n0\n1\n1.13\n0.33\n1.00\n1.00\n1.00\n1.00\n2.00\n▇▁▁▁▁\n\n\nRACE\n0\n1\n7.41\n21.97\n1.00\n1.00\n1.00\n2.00\n88.00\n▇▁▁▁▁\n\n\nWT\n0\n1\n83.16\n12.48\n56.60\n73.30\n82.60\n90.60\n115.30\n▂▇▇▅▁\n\n\nHT\n0\n1\n1.76\n0.08\n1.52\n1.71\n1.78\n1.82\n1.93\n▁▃▆▇▃\n\n\n\n\n# Plot the outcome variable DV (drug concentration) as a function of time, stratified by DOSE and using ID as a grouping factor\np &lt;- df %&gt;%\n  ggplot(aes(x = TIME , y = DV, group = ID, color = as.factor(DOSE))) +  # DOSE is used for stratification by dose\n  geom_line(alpha = 0.6) +\n  labs(\n    x = \"Time (hours)\", \n    y = \"Drug Concentration (ng/mL)\", \n    color = \"Dose (mg)\"\n  ) +\n  theme_minimal() +\n  theme(\n    axis.title.x = element_text(size = 12, face = \"bold\"),\n    axis.title.y = element_text(size = 12, face = \"bold\"),\n    legend.title = element_text(size = 10),\n    legend.text = element_text(size = 9)\n  )\nprint(p)\n\n\n\n\n\n\n\n# Keep only observations where OCC == 1 (remove OCC == 2)\ndf_filtered &lt;- df %&gt;%\n  filter(OCC == 1)\n\n# Compute total drug amount for each individual (sum of DV values), excluding TIME == 0\ndrug_sums &lt;- df_filtered %&gt;%\n  filter(TIME != 0) %&gt;%\n  group_by(ID) %&gt;%\n  summarize(Y = sum(DV, na.rm = TRUE))\n\n# Create a data frame with only observations where TIME == 0 (dosing information)\ndose_info &lt;- df_filtered %&gt;%\n  filter(TIME == 0)\n\n# Join the two data frames: one with drug sums and the other with dosing info\ndf_final &lt;- left_join(dose_info, drug_sums, by = \"ID\")\n\n# Select only relevant variables and convert RACE and SEX to factors\ndf_cleaned &lt;- df_final %&gt;%\n  select(Y, DOSE, AGE, SEX, RACE, WT, HT) %&gt;%\n  mutate(\n    SEX = as.factor(SEX),\n    RACE = as.factor(RACE)\n  )\n\n# Final check of the cleaned data\nglimpse(df_cleaned)\n\nRows: 120\nColumns: 7\n$ Y    &lt;dbl&gt; 2690.52, 2638.81, 2149.61, 1788.89, 3126.37, 2336.89, 3007.20, 27…\n$ DOSE &lt;dbl&gt; 25.0, 25.0, 25.0, 25.0, 25.0, 25.0, 25.0, 25.0, 25.0, 25.0, 25.0,…\n$ AGE  &lt;int&gt; 42, 24, 31, 46, 41, 27, 23, 20, 23, 28, 46, 22, 43, 50, 19, 26, 3…\n$ SEX  &lt;fct&gt; 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ RACE &lt;fct&gt; 2, 2, 1, 1, 2, 2, 1, 88, 2, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1…\n$ WT   &lt;dbl&gt; 94.3, 80.4, 71.8, 77.4, 64.3, 74.1, 87.9, 61.9, 65.3, 103.5, 83.0…\n$ HT   &lt;dbl&gt; 1.769997, 1.759850, 1.809847, 1.649993, 1.560052, 1.829862, 1.850…\n\nskim(df_cleaned)\n\n\nData summary\n\n\nName\ndf_cleaned\n\n\nNumber of rows\n120\n\n\nNumber of columns\n7\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n2\n\n\nnumeric\n5\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nSEX\n0\n1\nFALSE\n2\n1: 104, 2: 16\n\n\nRACE\n0\n1\nFALSE\n4\n1: 74, 2: 36, 88: 8, 7: 2\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nY\n0\n1\n2445.41\n961.64\n826.43\n1700.53\n2349.14\n3050.22\n5606.58\n▆▇▆▂▁\n\n\nDOSE\n0\n1\n36.46\n11.86\n25.00\n25.00\n37.50\n50.00\n50.00\n▇▁▂▁▆\n\n\nAGE\n0\n1\n33.00\n8.98\n18.00\n26.00\n31.00\n40.25\n50.00\n▅▇▃▅▅\n\n\nWT\n0\n1\n82.55\n12.52\n56.60\n73.18\n82.10\n90.10\n115.30\n▂▇▇▅▁\n\n\nHT\n0\n1\n1.76\n0.09\n1.52\n1.70\n1.77\n1.81\n1.93\n▁▃▆▇▃\n\n\n\n\n# Save cleaned data \nsaveRDS(df_cleaned, file = here::here(\"fitting-exercise\", \"cleaned_data.rds\"))\n\n# Summary table by DOSE\nsummary_table &lt;- df_cleaned %&gt;%\n  group_by(DOSE) %&gt;%\n  summarize(\n    Mean_Y = mean(Y, na.rm = TRUE),\n    Median_Y = median(Y, na.rm = TRUE),\n    SD_Y = sd(Y, na.rm = TRUE),\n    Count = n()\n  )\nprint(summary_table)\n\n# A tibble: 3 × 5\n   DOSE Mean_Y Median_Y  SD_Y Count\n  &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n1  25    1783.    1666.  601.    59\n2  37.5  2464.    2388.  488.    12\n3  50    3239.    3194.  787.    49\n\n# Boxplots of Y vs predictors with improved labels\nggplot(df_cleaned, aes(x = as.factor(DOSE), y = Y, fill = as.factor(DOSE))) +\n  geom_boxplot() +\n  labs(\n    x = \"Dose (mg)\", \n    y = \"Total Drug (Mavoglurant) - Concentration (ng/mL)\", \n    fill = \"Dose (mg)\"\n  ) +\n  theme_minimal() +\n  theme(\n    axis.title.x = element_text(size = 12, face = \"bold\"),\n    axis.title.y = element_text(size = 12, face = \"bold\"),\n    legend.title = element_text(size = 10),\n    legend.text = element_text(size = 9)\n  )\n\n\n\n\n\n\n\nggplot(df_cleaned, aes(x = as.factor(SEX), y = Y, fill = as.factor(SEX))) +\n  geom_boxplot() +\n  labs(\n    x = \"Sex\", \n    y = \"Total Drug (Mavoglurant) - Concentration (ng/mL)\", \n    fill = \"Sex\"\n  ) +\n  theme_minimal() +\n  theme(\n    axis.title.x = element_text(size = 12, face = \"bold\"),\n    axis.title.y = element_text(size = 12, face = \"bold\"),\n    legend.title = element_text(size = 10),\n    legend.text = element_text(size = 9)\n  )\n\n\n\n\n\n\n\n# Scatterplots with regression lines\nscatter_1 &lt;- ggplot(df_cleaned, aes(x = AGE, y = Y)) +\n  geom_point(alpha = 0.6) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"blue\") +\n  labs(\n    x = \"Age (years)\", \n    y = \"Total Drug (Y) - Concentration (ng/mL)\"\n  ) +\n  theme_minimal() +\n  theme(\n    axis.title.x = element_text(size = 12, face = \"bold\"),\n    axis.title.y = element_text(size = 12, face = \"bold\")\n  )\nprint(scatter_1)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nscatter_2 &lt;- ggplot(df_cleaned, aes(x = WT, y = Y)) +\n  geom_point(alpha = 0.6) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"blue\") +\n  labs(\n    x = \"Weight (kg)\", \n    y = \"Total Drug (Mavoglurant) - Concentration (ng/mL)\"\n  ) +\n  theme_minimal() +\n  theme(\n    axis.title.x = element_text(size = 12, face = \"bold\"),\n    axis.title.y = element_text(size = 12, face = \"bold\")\n  )\nprint(scatter_2)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n# Distribution plots\nggplot(df_cleaned, aes(x = Y)) +\n  geom_histogram(bins = 30, fill = \"blue\", alpha = 0.7) +\n  labs(\n    x = \"Total Drug (Mavoglurant) - Concentration (ng/mL)\", \n    y = \"Count\"\n  ) +\n  theme_minimal() +\n  theme(\n    axis.title.x = element_text(size = 12, face = \"bold\"),\n    axis.title.y = element_text(size = 12, face = \"bold\")\n  )\n\n\n\n\n\n\n\nggplot(df_cleaned, aes(x = AGE)) +\n  geom_histogram(bins = 30, fill = \"green\", alpha = 0.7) +\n  labs(\n    x = \"Age (years)\", \n    y = \"Count\"\n  ) +\n  theme_minimal() +\n  theme(\n    axis.title.x = element_text(size = 12, face = \"bold\"),\n    axis.title.y = element_text(size = 12, face = \"bold\")\n  )\n\n\n\n\n\n\n\n# Correlation plot with adjusted axis labels\ncor_plot &lt;- ggpairs(df_cleaned %&gt;% select(Y, DOSE, AGE, WT)) +\n  theme_minimal() +\n  theme(\n    axis.title.x = element_text(size = 12, face = \"bold\"),\n    axis.title.y = element_text(size = 12, face = \"bold\")\n  )\nprint(cor_plot)"
  },
  {
    "objectID": "fitting-exercise/fitting-model.html#prompt-2",
    "href": "fitting-exercise/fitting-model.html#prompt-2",
    "title": "Fitting Model",
    "section": "Prompt #2",
    "text": "Prompt #2\nPrompts - “Fit a linear model to the continuous outcome (Y) using the main predictor of interest, which we’ll assume here to be DOSE. Fit a linear model to the continuous outcome (Y) using all predictors. For both models, compute RMSE and R-squared and print them.”\n“Fit a logistic model to the categorical/binary outcome (SEX) using the main predictor of interest, which we’ll again assume here to be DOSE. Fit a logistic model to SEX using all predictors. For both models, compute accuracy and ROC-AUC and print them.”\n\n# Install pROC package \ninstall.packages(\"pROC\")\n\n\nThe downloaded binary packages are in\n    /var/folders/29/tsfhn2w952x9r8f27_hpzq900000gn/T//Rtmpzywett/downloaded_packages\n\n# Load the pROC package\nlibrary(pROC)\n\nType 'citation(\"pROC\")' for a citation.\n\n\n\nAttaching package: 'pROC'\n\n\nThe following objects are masked from 'package:stats':\n\n    cov, smooth, var\n\n# Load the necessary package\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.3.0 ──\n\n\n✔ broom        1.0.7     ✔ rsample      1.2.1\n✔ dials        1.4.0     ✔ tune         1.3.0\n✔ infer        1.0.7     ✔ workflows    1.2.0\n✔ modeldata    1.4.0     ✔ workflowsets 1.1.0\n✔ parsnip      1.3.0     ✔ yardstick    1.3.2\n✔ recipes      1.1.1     \n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n\n# Load the data\ndf &lt;- read.csv(here::here(\"fitting-exercise\", \"Mavoglurant_A2121_nmpk.csv\"))\n\n# Data Preprocessing - Remove OCC = 2, and convert necessary columns to factors\ndf_cleaned &lt;- df %&gt;%\n  filter(OCC == 1) %&gt;%\n  mutate(\n    SEX = as.factor(SEX),\n    RACE = as.factor(RACE)\n  )\n\n# Keep only necessary variables: WT (weight), DOSE, SEX, AGE, RACE, WT, HT\ndf_cleaned &lt;- df_cleaned %&gt;%\n  select(WT, DOSE, SEX, AGE, RACE, HT)\n\n# 1. Fit a linear model to the continuous outcome (WT) using the main predictor DOSE\nlm_dose_spec &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\")\n\nlm_dose_recipe &lt;- recipe(WT ~ DOSE, data = df_cleaned)\n\nlm_dose_workflow &lt;- workflow() %&gt;%\n  add_recipe(lm_dose_recipe) %&gt;%\n  add_model(lm_dose_spec)\n\nlm_dose_fit &lt;- lm_dose_workflow %&gt;%\n  fit(data = df_cleaned)\n\n# 2. Fit a linear model to the continuous outcome (WT) using all predictors\nlm_all_spec &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\")\n\nlm_all_recipe &lt;- recipe(WT ~ DOSE + AGE + SEX + RACE + HT, data = df_cleaned)\n\nlm_all_workflow &lt;- workflow() %&gt;%\n  add_recipe(lm_all_recipe) %&gt;%\n  add_model(lm_all_spec)\n\nlm_all_fit &lt;- lm_all_workflow %&gt;%\n  fit(data = df_cleaned)\n\n# 3. Compute RMSE and R-squared for both linear models\n\n# Extract predictions from the model predictions\nlm_dose_pred &lt;- predict(lm_dose_fit, df_cleaned) %&gt;% pull(.pred)\nlm_all_pred &lt;- predict(lm_all_fit, df_cleaned) %&gt;% pull(.pred)\n\n# Calculate RMSE and R-squared for both models using yardstick functions\nlm_dose_rmse &lt;- lm_dose_pred %&gt;%\n  tibble(pred = ., truth = df_cleaned$WT) %&gt;%\n  rmse(truth = truth, estimate = pred)\n\nlm_all_rmse &lt;- lm_all_pred %&gt;%\n  tibble(pred = ., truth = df_cleaned$WT) %&gt;%\n  rmse(truth = truth, estimate = pred)\n\nlm_dose_r2 &lt;- lm_dose_pred %&gt;%\n  tibble(pred = ., truth = df_cleaned$WT) %&gt;%\n  rsq(truth = truth, estimate = pred)\n\nlm_all_r2 &lt;- lm_all_pred %&gt;%\n  tibble(pred = ., truth = df_cleaned$WT) %&gt;%\n  rsq(truth = truth, estimate = pred)\n\n# Print RMSE and R-squared\ncat(\"Linear model (DOSE):\\n\")\n\nLinear model (DOSE):\n\ncat(\"RMSE:\", lm_dose_rmse$.estimate, \"\\n\")\n\nRMSE: 12.35923 \n\ncat(\"R-squared:\", lm_dose_r2$.estimate, \"\\n\\n\")\n\nR-squared: 0.01004977 \n\ncat(\"Linear model (all predictors):\\n\")\n\nLinear model (all predictors):\n\ncat(\"RMSE:\", lm_all_rmse$.estimate, \"\\n\")\n\nRMSE: 8.682867 \n\ncat(\"R-squared:\", lm_all_r2$.estimate, \"\\n\\n\")\n\nR-squared: 0.5113958 \n\n# 4. Fit a logistic model to the categorical/binary outcome (SEX) using the main predictor DOSE\nlogit_dose_spec &lt;- logistic_reg() %&gt;%\n  set_engine(\"glm\")\n\nlogit_dose_recipe &lt;- recipe(SEX ~ DOSE, data = df_cleaned)\n\nlogit_dose_workflow &lt;- workflow() %&gt;%\n  add_recipe(logit_dose_recipe) %&gt;%\n  add_model(logit_dose_spec)\n\nlogit_dose_fit &lt;- logit_dose_workflow %&gt;%\n  fit(data = df_cleaned)\n\n# 5. Fit a logistic model to SEX using all predictors\nlogit_all_spec &lt;- logistic_reg() %&gt;%\n  set_engine(\"glm\")\n\nlogit_all_recipe &lt;- recipe(SEX ~ DOSE + AGE + RACE + HT, data = df_cleaned)\n\nlogit_all_workflow &lt;- workflow() %&gt;%\n  add_recipe(logit_all_recipe) %&gt;%\n  add_model(logit_all_spec)\n\nlogit_all_fit &lt;- logit_all_workflow %&gt;%\n  fit(data = df_cleaned)\n\n# 6. Compute accuracy and ROC-AUC for both logistic models\nlogit_dose_pred &lt;- predict(logit_dose_fit, df_cleaned, type = \"prob\")\nlogit_all_pred &lt;- predict(logit_all_fit, df_cleaned, type = \"prob\")\n\n# Calculate accuracy for both models\nlogit_dose_pred_class &lt;- ifelse(logit_dose_pred$.pred_1 &gt; 0.5, 1, 0)\nlogit_all_pred_class &lt;- ifelse(logit_all_pred$.pred_1 &gt; 0.5, 1, 0)\n\nlogit_dose_accuracy &lt;- mean(logit_dose_pred_class == df_cleaned$SEX)\nlogit_all_accuracy &lt;- mean(logit_all_pred_class == df_cleaned$SEX)\n\n# Calculate ROC-AUC for both models\nlogit_dose_roc &lt;- roc(df_cleaned$SEX, logit_dose_pred$.pred_1)\n\nSetting levels: control = 1, case = 2\n\n\nSetting direction: controls &gt; cases\n\nlogit_all_roc &lt;- roc(df_cleaned$SEX, logit_all_pred$.pred_1)\n\nSetting levels: control = 1, case = 2\nSetting direction: controls &gt; cases\n\n# Print accuracy and ROC-AUC\ncat(\"Logistic model (DOSE):\\n\")\n\nLogistic model (DOSE):\n\ncat(\"Accuracy:\", logit_dose_accuracy, \"\\n\")\n\nAccuracy: 0.8642643 \n\ncat(\"ROC-AUC:\", auc(logit_dose_roc), \"\\n\\n\")\n\nROC-AUC: 0.5853838 \n\ncat(\"Logistic model (all predictors):\\n\")\n\nLogistic model (all predictors):\n\ncat(\"Accuracy:\", logit_all_accuracy, \"\\n\")\n\nAccuracy: 0.839039 \n\ncat(\"ROC-AUC:\", auc(logit_all_roc), \"\\n\\n\")\n\nROC-AUC: 0.9741739"
  },
  {
    "objectID": "fitting-exercise/fitting-model.html#what-do-all-the-models-mean-above",
    "href": "fitting-exercise/fitting-model.html#what-do-all-the-models-mean-above",
    "title": "Fitting Model",
    "section": "What Do All the Models Mean Above",
    "text": "What Do All the Models Mean Above\nLinear model (DOSE as the main predictor): RMSE (Root Mean Squared Error): 12.36 This indicates that the model’s predictions are, on average, about 12.36 units away from the actual values of Y (total drug concentration). A higher RMSE suggests that the model is not doing a very good job of predicting the outcome. R-squared: 0.0100 This value is very low, meaning that the model explains only about 1% of the variance in the outcome (Y). This suggests that DOSE alone is not a strong predictor of the total drug concentration (Y). A very low R-squared indicates that other factors likely have more influence on the outcome.\nLinear model (all predictors): RMSE: 8.68 The RMSE has decreased significantly compared to the previous model, indicating an improvement in the model’s predictive performance when considering all the predictors. The model is now, on average, about 8.68 units away from the actual values of Y. R-squared: 0.5114 This is a substantial improvement over the previous model, suggesting that the model now explains about 51% of the variance in the total drug concentration (Y). The inclusion of additional predictors has clearly helped the model improve its accuracy and explanation of the variability in Y.\nLogistic model (DOSE as the main predictor): Accuracy: 0.8643 This means the model correctly predicted SEX (male or female) about 86.43% of the time when using DOSE as the only predictor. This is a relatively high accuracy, suggesting that DOSE has a reasonable relationship with SEX, but accuracy alone doesn’t tell the full story. ROC-AUC: 0.5854 The ROC-AUC value is relatively low (just above 0.5), suggesting that the model does not do a very good job at distinguishing between the two classes (SEX = male or female) based on DOSE alone. A value closer to 1 would indicate excellent discrimination, while a value around 0.5 suggests random guessing.\nLogistic model (all predictors): Accuracy: 0.8390 This accuracy is still quite good, though slightly lower than the model with DOSE alone. It means the model is still fairly reliable at predicting SEX based on the predictors. ROC-AUC: 0.9742 This is a very high value, indicating that the logistic model with all predictors does an excellent job at distinguishing between the two classes (SEX = male or female). This suggests that the combination of predictors (including DOSE, AGE, etc.) is highly effective at discriminating between male and female participants."
  },
  {
    "objectID": "fitting-exercise/fitting-model.html#exercise-10",
    "href": "fitting-exercise/fitting-model.html#exercise-10",
    "title": "Fitting Model",
    "section": "EXERCISE 10 !!!",
    "text": "EXERCISE 10 !!!"
  },
  {
    "objectID": "fitting-exercise/fitting-model.html#data-wrangling-and-setup",
    "href": "fitting-exercise/fitting-model.html#data-wrangling-and-setup",
    "title": "Fitting Model",
    "section": "1. Data Wrangling and Setup",
    "text": "1. Data Wrangling and Setup\n\nRemove RACE from the dataset.\nKeep only Y, DOSE, AGE, SEX, WT, and HT.\nEnsure there are 120 rows left.\nSet the random seed at the beginning (rngseed = 1234).\n\n\n# Load required libraries\nlibrary(tidymodels)\nlibrary(dplyr)\nlibrary(rsample)\n\n# Set seed for reproducibility\nrngseed &lt;- 1234\nset.seed(rngseed)\n\n# Load the data\ndata &lt;- read.csv(here::here(\"fitting-exercise\", \"Mavoglurant_A2121_nmpk.csv\"))\n\n# Remove RACE and keep relevant variables\ncleaned_data &lt;- data %&gt;%\n  select(DV, DOSE, AGE, SEX, WT, HT) %&gt;%\n  na.omit() %&gt;%\n  rename(Y = DV) # Rename DV to Y for consistency with the instructions\n\n# Check dimensions (should be 120 rows, 6 columns)\ndim(cleaned_data)\n\n[1] 2678    6\n\n\nWhat happened in this step above (steps noted for my benefit/future benefit):\nselect(DV, DOSE, AGE, SEX, WT, HT)\n\nThis selects only the relevant variables from the dataset.\nDV is the dependent (outcome) variable we’re trying to predict.\nThe other five (DOSE, AGE, SEX, WT, HT) are predictors (independent variables).\n\n\n\nna.omit()\n\nThis removes rows with any missing (NA) values.\nMissing data can mess up model fitting, so it’s common to drop them unless you plan to impute (fill in) the missing values.\n\nrename(Y = DV)\n\nRenames the DV column to Y for consistency with the instructions.\nThis way, the outcome variable is consistently called Y throughout the analysis."
  },
  {
    "objectID": "fitting-exercise/fitting-model.html#train-vs.-test-data-split",
    "href": "fitting-exercise/fitting-model.html#train-vs.-test-data-split",
    "title": "Fitting Model",
    "section": "2. Train vs. Test Data Split",
    "text": "2. Train vs. Test Data Split\n\nUse initial_split() from rsample (75% train, 25% test).\nConfirm the split size matches expectations (~90 training, ~30 testing).\n\n\n# Split into 75% training and 25% testing\nset.seed(rngseed) # Reset seed before sampling\ndata_split &lt;- initial_split(cleaned_data, prop = 0.75)\ntrain_data &lt;- training(data_split)\ntest_data &lt;- testing(data_split)\n\n# Check split sizes\ndim(train_data) # ~90 rows\n\n[1] 2008    6\n\ndim(test_data)  # ~30 rows\n\n[1] 670   6\n\n\nWhat happened in this step above (steps noted for my benefit/future benefit):\n\nset.seed(rngseed)\n\nEnsures that the random split is reproducible.\nIf you set the same seed and run the code again, you’ll get the exact same split — useful for debugging and reproducibility.\n\ninitial_split(cleaned_data, prop = 0.75)\n\nRandomly splits the data into:\n\n75% training data (used to fit the model)\n25% test data (held back to evaluate the model later)\n\nThis mimics a real-world scenario where you’d train a model on existing data and test it on unseen data.\n\ntraining(data_split) and testing(data_split)\n\nExtracts the training and test datasets from the split."
  },
  {
    "objectID": "fitting-exercise/fitting-model.html#modeling-fitting",
    "href": "fitting-exercise/fitting-model.html#modeling-fitting",
    "title": "Fitting Model",
    "section": "3. Modeling Fitting",
    "text": "3. Modeling Fitting\n\nFit two linear models using tidymodels::linear_reg():\n\nModel 1: Y ~ DOSE\nModel 2: Y ~ DOSE + AGE + SEX + WT + HT\n\nUse metric = RMSE.\n\n\n# Define the model specifications\nmodel_spec1 &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;%\n  set_mode(\"regression\")\n\nmodel_spec2 &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;%\n  set_mode(\"regression\")\n\n# Fit the models on the training set\nmodel1_train &lt;- model_spec1 %&gt;%\n  fit(Y ~ DOSE, data = train_data)\n\nmodel2_train &lt;- model_spec2 %&gt;%\n  fit(Y ~ DOSE + AGE + SEX + WT + HT, data = train_data)\n\nWhat happened in this step above (steps noted for my benefit/future benefit):\n\nlinear_reg()\n\nCreates a linear regression model.\nLinear regression estimates the relationship between the predictors and the outcome using the formula:\n\nY=β0+β1⋅DOSE+β2⋅AGE+…+ϵY=β0​+β1​⋅DOSE+β2​⋅AGE+…+ϵ\nwhere:\n\nYY = predicted outcome\nβ0β0​ = intercept\nβ1,β2,…β1​,β2​,… = regression coefficients (how much each predictor contributes)\nϵϵ = error term (difference between actual and predicted values)\n\nset_engine(“lm”)\n\nSpecifies that the model should be fitted using the base R lm() function (ordinary least squares).\n\nfit(Y ~ DOSE, data = train_data)\n\nY ~ DOSE fits a simple linear regression with only DOSE as the predictor.\nThe second model adds all predictors (AGE, SEX, WT, HT) for a more complex model."
  },
  {
    "objectID": "fitting-exercise/fitting-model.html#model-performance-on-training-set",
    "href": "fitting-exercise/fitting-model.html#model-performance-on-training-set",
    "title": "Fitting Model",
    "section": "4. Model Performance on Training Set",
    "text": "4. Model Performance on Training Set\n\nCompute RMSE for both models on training data.\nCompute RMSE for a null model using tidymodels::null_model().\nCompare RMSE values.\n\n\n# Predict on training data\npred1 &lt;- predict(model1_train, train_data) %&gt;%\n  bind_cols(train_data)\n\npred2 &lt;- predict(model2_train, train_data) %&gt;%\n  bind_cols(train_data)\n\n# Compute RMSE\nrmse1 &lt;- rmse(pred1, truth = Y, estimate = .pred)\nrmse2 &lt;- rmse(pred2, truth = Y, estimate = .pred)\n\n# Null model\nnull_mod &lt;- null_model(mode = \"regression\") %&gt;%\n  set_engine(\"parsnip\") %&gt;%\n  fit(Y ~ 1, data = train_data)\n\n# Predict with null model\nnull_pred &lt;- predict(null_mod, train_data) %&gt;%\n  bind_cols(train_data)\n\n# RMSE for null model\nnull_rmse &lt;- rmse(null_pred, truth = Y, estimate = .pred)\n\n# Compare RMSE values\nrmse1$.estimate\n\n[1] 222.5618\n\nrmse2$.estimate\n\n[1] 221.903\n\nnull_rmse$.estimate\n\n[1] 228.9459\n\n\n\n\n\nModel\nRMSE Value\nInterpretation\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel 1 (Only DOSE as predictor)\n222.56\nThe model using only DOSE explains some variation but is not particularly accurate.\n\n\n\n\n\n\n\n\n\n\n\nModel 2 (All predictors)\n221.90\nSlightly better than Model 1 — adding more predictors improves the model’s accuracy a bit.\n\n\n\n\n\n\n\n\n\n\n\nNull Model (Mean-only)\n228.95\nThis is the benchmark — it reflects the RMSE you’d get if you just predicted the mean outcome for all cases.\n\n\n\nMinimal Improvement from Adding Predictors\n\nModel 2 (all predictors) has an RMSE of 221.90, which is only 0.66 lower than Model 1 (DOSE-only) at 222.56.\nThis small improvement suggests that adding AGE, SEX, WT, and HT does not meaningfully improve predictive accuracy beyond using DOSE alone.\nIt implies that DOSE is the primary driver of the outcome (Y), and the other predictors might not have a strong relationship with Y or might introduce noise.\n\nPredictive Performance Close to Null Model\n\nThe null model (mean-only) RMSE is 228.95, which is just slightly worse than the fitted models.\nThe fact that the RMSE values for the fitted models are so close to the null model suggests that the predictors don’t explain much variance in the outcome.\nThis hints that the relationship between the predictors and Y is either weak, nonlinear, or confounded by unmeasured factors.\n\nModel Complexity Does Not Improve Performance\n\nThe fact that Model 2 (more complex) and Model 1 (simpler) have almost identical RMSE values means that adding more predictors is not adding meaningful information.\n\nWhat happened in this step above (steps noted for my benefit/future benefit):\n\nThe null model predicts the mean of Y for every observation.\nIf the null model’s RMSE is high, it means the data is hard to predict.\nIf the null model’s RMSE is close to the other models, the predictors aren’t adding much value.\n\n\n\npredict(new_data = train_data)\n\nPredicts Y values on the training data using the model.\n\nbind_cols(train_data)\n\nMerges predicted values with the original data.\n\nmetrics(truth = Y, estimate = .pred)\n\nComputes performance metrics (like RMSE).\nRMSE (Root Mean Squared Error) measures how far the predicted values are from the actual values."
  },
  {
    "objectID": "fitting-exercise/fitting-model.html#cross-validation",
    "href": "fitting-exercise/fitting-model.html#cross-validation",
    "title": "Fitting Model",
    "section": "5. Cross Validation",
    "text": "5. Cross Validation\n\nUse 10-fold cross-validation (vfold_cv()).\nCompute RMSE for both models.\nCompare with training RMSE values.\nLook at the standard error for RMSE.\n\n\nset.seed(rngseed) # Reset seed before CV\n\n# Create 10-fold cross-validation\ncv_folds &lt;- vfold_cv(train_data, v = 10)\n\n# Define workflows\nwf1 &lt;- workflow() %&gt;%\n  add_model(model_spec1) %&gt;%\n  add_formula(Y ~ DOSE)\n\nwf2 &lt;- workflow() %&gt;%\n  add_model(model_spec2) %&gt;%\n  add_formula(Y ~ DOSE + AGE + SEX + WT + HT)\n\n# Fit and evaluate with CV\ncv_results1 &lt;- fit_resamples(\n  wf1,\n  resamples = cv_folds,\n  metrics = metric_set(rmse)\n)\n\ncv_results2 &lt;- fit_resamples(\n  wf2,\n  resamples = cv_folds,\n  metrics = metric_set(rmse)\n)\n\n# Get average RMSE and standard error\ncollect_metrics(cv_results1)\n\n# A tibble: 1 × 6\n  .metric .estimator  mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard    222.    10    3.47 Preprocessor1_Model1\n\ncollect_metrics(cv_results2)\n\n# A tibble: 1 × 6\n  .metric .estimator  mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard    222.    10    3.29 Preprocessor1_Model1\n\n\nLow Variability in Performance with Cross Validation\n\nThe standard error from the cross-validation is ~3.47, indicating that the model’s performance is consistent across different data splits.\nThis suggests that the model is stable and not overfitting — even though predictive power is weak, the model generalizes well across different subsets of the data.\n\nWhat happened in this step above (steps noted for my benefit/future benefit):\n\nvfold_cv(train_data, v = 10)\n\nCreates 10 folds for cross-validation.\nThe model is trained on 9 folds and evaluated on the 10th — repeated 10 times.\n\nfit_resamples()\n\nTrains the model and computes RMSE for each fold.\n\nmetric_set(rmse)\n\nSpecifies that RMSE should be computed.\n\nInterpretation:\n\nAverage RMSE across folds = general model performance.\nLower variability between folds = more consistent model."
  },
  {
    "objectID": "fitting-exercise/fitting-model.html#robustness-check",
    "href": "fitting-exercise/fitting-model.html#robustness-check",
    "title": "Fitting Model",
    "section": "6. Robustness Check",
    "text": "6. Robustness Check\n\nChange the seed.\nRe-run CV to check variability.\nOptionally try repeated CV (repeats = 5) for more robust results.\n\n\n# Try with a different seed\nset.seed(5678) \n\ncv_results1_new &lt;- fit_resamples(\n  wf1,\n  resamples = cv_folds,\n  metrics = metric_set(rmse)\n)\n\ncv_results2_new &lt;- fit_resamples(\n  wf2,\n  resamples = cv_folds,\n  metrics = metric_set(rmse)\n)\n\n# Compare old vs new RMSE estimates\ncollect_metrics(cv_results1_new)\n\n# A tibble: 1 × 6\n  .metric .estimator  mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard    222.    10    3.47 Preprocessor1_Model1\n\ncollect_metrics(cv_results2_new)\n\n# A tibble: 1 × 6\n  .metric .estimator  mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard    222.    10    3.29 Preprocessor1_Model1\n\n\n\nTakeaways:\n\nLower RMSE → better predictive accuracy\nAdding predictors → reduces RMSE\n\nThe fact that both models with predictors are only modestly better than the null model means that the predictors might not be very strong, \n\nCross-validation RMSE should be close to training RMSE if the model generalizes well\nHigh cross-validation variability → model might not generalize well"
  },
  {
    "objectID": "fitting-exercise/fitting-model.html#part-2",
    "href": "fitting-exercise/fitting-model.html#part-2",
    "title": "Fitting Model",
    "section": "Part 2",
    "text": "Part 2\n\nModel Predictions\nThis section goes through the Model Predictions section. This section assesses the predicted values for each model and also plots it. Residuals are also computed for further analysis.\n\n# Create a data frame with observed and predicted values for each model\npredictions_df &lt;- bind_rows(\n  pred1 %&gt;% mutate(Model = \"Model 1\"),\n  pred2 %&gt;% mutate(Model = \"Model 2\"),\n  null_pred %&gt;% mutate(Model = \"Null Model\")\n) %&gt;%\n  select(Y, .pred, Model) %&gt;%\n  rename(Observed = Y, Predicted = .pred)\n\n# Plot observed vs. predicted values\nobs_pred_models_plot &lt;- ggplot(predictions_df, aes(x = Observed, y = Predicted, color = Model, shape = Model)) +\n  geom_point(alpha = 0.6, size = 2) +  \n  geom_abline(slope = 1, intercept = 0, linetype = \"dashed\", color = \"black\") +  \n  scale_x_continuous(limits = c(0, 500)) +  \n  scale_y_continuous(limits = c(0, 500)) +  \n  labs(\n    title = \"Observed vs Predicted Values\",\n    x = \"Observed\",\n    y = \"Predicted\",\n    color = \"Model\",\n    shape = \"Model\"\n  ) +\n  theme_minimal()\n\nprint(obs_pred_models_plot)\n\nWarning: Removed 579 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n# Compute residuals for model 2\nmodel2_residual &lt;- predictions_df %&gt;%\n  filter(Model == \"Model 2\") %&gt;%\n  mutate(Residual = Predicted - Observed)\n\nresidual_plot &lt;- ggplot(model2_residual, aes(x = Predicted, y = Residual)) +\n  geom_point(alpha = 0.6, color = \"black\") +\n  geom_hline(yintercept = 0, color = \"blue\") +\n  scale_y_continuous(limits = c(-max(abs(model2_residual$Residual)), max(abs(model2_residual$Residual)))) + # line assisted by ChatGPT\n  labs(\n    title = \"Predicted Values vs Residuals for Model 2\",\n    x = \"Predicted Value\",\n    y = \"Residual\"\n  ) + theme_minimal()\n\nprint(residual_plot)\n\n\n\n\n\n\n\n\n\n\nModel Predictions and Uncertainty\nThis section entails bootstrapping, fitting models to the bootstrapping samples, and the computation/graphing of other various statistics.\n\nlibrary(rsample)\nlibrary(purrr)\nlibrary(tidymodels)\n\n# Set seed\nset.seed(1234)\n\n# Create 100 bootstraps from training data\ntrain_bootstrap &lt;- bootstraps(train_data, time = 100)\n\n# Function for fitting model and creating predictions\n# Assisted by chatGPT\n\nboot_fit_pred &lt;- map(train_bootstrap$splits, function(split) {\n  # Extract bootstrap sample\n  bootstrap_sample &lt;- analysis(split)\n  \n  # Model fitting\n  model_fit &lt;- model_spec2 %&gt;%\n    fit(Y ~ DOSE + AGE + SEX + WT + HT, data = bootstrap_sample)\n  \n  # Make predictions\n  predictions &lt;- predict(model_fit, new_data = train_data) %&gt;%\n    mutate(Observed = train_data$Y)\n  \n  return(predictions)\n})\n\n# The next few lines were debugged and assisted through ChatGPT\n# Combine predictions from all bootstrap samples\nbootstrap_pred_df &lt;- bind_rows(boot_fit_pred, .id = \"Bootstrap_Sample\")\n\n# Extract predictions for use later\nboot_preds &lt;- map(boot_fit_pred, ~ .x$.pred)\n\n# Combine into matrix\npred_bs &lt;- do.call(rbind, boot_preds)\n\n# Calculate values for preds matrix\npreds &lt;- pred_bs %&gt;% \n  apply(2, function(x) c(\n    Mean = mean(x),  \n    Lower = quantile(x, 0.055),  \n    Median = quantile(x, 0.5),   \n    Upper = quantile(x, 0.945)   \n  )) %&gt;% \n  t()\n\n\n# Create dataframe with observed values, point estimate, and bootstrap values\nplot_df &lt;- data.frame(\n  observed = train_data$Y,\n  PointEstimate = preds[ , 1],\n  Lower = preds[ , 2],\n  Median = preds[ , 3],\n  Upper = preds[ , 4]\n)\n\nfinal_plot &lt;- ggplot(plot_df, aes(x = observed)) +\n  \n  # Plot point estimates\n  geom_point(aes(y = PointEstimate), color = \"black\", shape = 16, size = 3) +\n  \n  # Plot median of bootstraps\n  geom_point(aes(y = Median), color = \"red\", shape = 17, size = 1) +\n  \n  # Plot CI\n  geom_errorbar(aes(ymin = Lower, ymax = Upper), color = \"blue\", width = 0.2) +\n  \n  # 45 degree line\n  geom_abline(slope = 1, intercept = 0, color = \"darkgray\", linetype = \"dashed\") +\n  \n  # Labels\n  labs(x = \"Observed Values\", y = \"Predicted Values (Bootstrap)\",\n       title = \"Observed vs Predicted Values, Median, and 89% CI\") +\n  theme_minimal()\n\nprint(final_plot)\n\n\n\n\n\n\n\n\nSomething interesting about final_plot is that the median and point estimate seem to be the same or similar values. When I asked ChatGPT about the implications, it says that the median and point estimate’s similarity can shows that the model is stable and not highly volatile across different data sets. It also says that this can indicate the model is generalizable."
  },
  {
    "objectID": "fitting-exercise/fitting-model.html#part-3",
    "href": "fitting-exercise/fitting-model.html#part-3",
    "title": "Fitting Model",
    "section": "Part 3",
    "text": "Part 3\nNow I want to fit model 2 onto the test set. I will make a plot for predicted values versus observed values for the fit on both the training set and the test set."
  },
  {
    "objectID": "ml-models-exercise/ml-models-exercise.html",
    "href": "ml-models-exercise/ml-models-exercise.html",
    "title": "ml-models-exercise",
    "section": "",
    "text": "Write code that loads the packages & sets the random seed\n# Load necessary packages\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.3.0 ──\n\n\n✔ broom        1.0.7     ✔ recipes      1.1.1\n✔ dials        1.4.0     ✔ rsample      1.2.1\n✔ dplyr        1.1.4     ✔ tibble       3.2.1\n✔ ggplot2      3.5.1     ✔ tidyr        1.3.1\n✔ infer        1.0.7     ✔ tune         1.3.0\n✔ modeldata    1.4.0     ✔ workflows    1.2.0\n✔ parsnip      1.3.0     ✔ workflowsets 1.1.0\n✔ purrr        1.0.4     ✔ yardstick    1.3.2\n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ purrr::discard() masks scales::discard()\n✖ dplyr::filter()  masks stats::filter()\n✖ dplyr::lag()     masks stats::lag()\n✖ recipes::step()  masks stats::step()\n\nlibrary(ggplot2)\nlibrary(here)\n\nhere() starts at /Users/hopegrismercomputer/Desktop/MADA-course/hopegrismer-MADA-portfolio\n\nlibrary(GGally)\n\nRegistered S3 method overwritten by 'GGally':\n  method from   \n  +.gg   ggplot2\n\nlibrary(tune)\n\nsetwd(\"/Users/hopegrismercomputer/Desktop/MADA-course/hopegrismer-MADA-portfolio\")\nLoading the data from my folder that it is placed in\n# Load cleaned data from the specified folder\ncleaned_data &lt;- readRDS(here::here(\"ml-models-exercise\", \"cleaned_data.rds\"))\n\n# Preview the first few rows\nhead(cleaned_data)\n\n        Y DOSE AGE SEX RACE   WT       HT\n1 2690.52   25  42   1    2 94.3 1.769997\n2 2638.81   25  24   1    2 80.4 1.759850\n3 2149.61   25  31   1    1 71.8 1.809847\n4 1788.89   25  46   2    1 77.4 1.649993\n5 3126.37   25  41   2    2 64.3 1.560052\n6 2336.89   25  27   1    2 74.1 1.829862\n\ncleaned_data \n\n          Y DOSE AGE SEX RACE    WT       HT\n1   2690.52 25.0  42   1    2  94.3 1.769997\n2   2638.81 25.0  24   1    2  80.4 1.759850\n3   2149.61 25.0  31   1    1  71.8 1.809847\n4   1788.89 25.0  46   2    1  77.4 1.649993\n5   3126.37 25.0  41   2    2  64.3 1.560052\n6   2336.89 25.0  27   1    2  74.1 1.829862\n7   3007.20 25.0  23   1    1  87.9 1.850107\n8   2795.65 25.0  20   1   88  61.9 1.730095\n9   3865.79 25.0  23   1    2  65.3 1.649839\n10  1761.62 25.0  28   1    1 103.5 1.840020\n11  2548.98 25.0  46   1    1  83.0 1.779870\n12  1967.61 25.0  22   1    1  68.7 1.700058\n13  2352.78 37.5  43   2    1  64.4 1.560084\n14  1800.79 37.5  50   2    1  69.8 1.640057\n15  2009.16 37.5  19   1    2  86.1 1.910054\n16  2815.26 37.5  26   1    2  84.5 1.770060\n17  2008.52 37.5  39   1    1  99.1 1.809982\n18  2933.99 37.5  46   1    1  71.2 1.669993\n19  2748.86 37.5  41   1    1  82.6 1.830179\n20  2154.56 37.5  30   1    1  85.4 1.860186\n21  3462.59 37.5  49   1    1  76.1 1.700070\n22  2771.69 37.5  28   1    1  78.3 1.719903\n23  2423.89 37.5  25   1    1  73.3 1.690144\n24  2084.87 37.5  37   1    1 102.1 1.809858\n25  4984.57 50.0  47   1    1  79.5 1.749972\n26  2572.45 50.0  27   1    1  97.5 1.849933\n27  2667.02 50.0  45   2    1  80.7 1.659881\n28  3004.21 50.0  28   1    1  83.2 1.740016\n29  4834.65 50.0  42   2    1  58.0 1.580118\n30  5606.58 50.0  29   1    1  85.7 1.770157\n31  3408.61 50.0  39   1    1  74.2 1.749948\n32  4493.01 50.0  38   1    1  70.4 1.640153\n33  3513.71 50.0  32   1    2  78.9 1.740008\n34  3905.93 50.0  47   1    2  89.3 1.689903\n35  3644.37 50.0  28   1    2  96.8 1.900156\n36  2746.20 50.0  40   1    2  74.8 1.650143\n37  1424.00 25.0  44   2    1  85.4 1.640050\n38  1108.17 25.0  48   2    2  79.5 1.620071\n39  3104.70 50.0  45   1    1  99.1 1.809982\n40  2177.20 50.0  31   1   88  88.3 1.759875\n41  2193.20 25.0  30   1    2  91.6 1.740143\n42  1810.59 25.0  27   1    2  69.2 1.810115\n43  1666.10 25.0  45   1    2  92.0 1.690046\n44  2027.39 25.0  20   1    1  80.5 1.879841\n45  2345.50 50.0  31   1    1  85.9 1.719881\n46  3310.20 50.0  46   1    1  94.5 1.779874\n47  3777.20 50.0  37   1    1 101.8 1.829941\n48  2063.43 25.0  40   1    2 102.7 1.879912\n49  4378.37 50.0  18   1    2  56.6 1.740218\n50  1853.91 25.0  24   1   88  70.7 1.780164\n51  3774.00 50.0  35   1    2  81.6 1.650067\n52  1625.46 25.0  30   1    2  90.6 1.789882\n53  1044.07 25.0  40   2    1  83.8 1.680030\n54  1423.70 25.0  39   1    1  82.1 1.810010\n55  3037.39 50.0  24   1    1  78.7 1.790083\n56  2610.00 50.0  25   1    1  79.8 1.859849\n57  3193.98 50.0  37   1    1  74.9 1.760000\n58  1602.63 25.0  28   1    2  73.2 1.729925\n59  2457.68 50.0  30   1    2  85.2 1.809879\n60  1474.60 25.0  44   1    1  90.1 1.779908\n61   997.89 25.0  43   1    1  99.8 1.800072\n62  4451.84 50.0  24   1   88  58.4 1.660126\n63  3507.10 50.0  22   1    1  69.4 1.820068\n64  3332.16 50.0  18   1    1  73.1 1.749978\n65  3733.10 50.0  34   1    2  88.0 1.840086\n66  1886.48 25.0  47   1    1  80.3 1.699861\n67  1175.69 25.0  33   1    1  99.3 1.799946\n68  1517.24 25.0  18   1    1  96.0 1.909821\n69  2036.20 50.0  18   1    2 102.7 1.809949\n70  2532.10 25.0  30   1    2  71.4 1.679921\n71  1392.78 25.0  20   1    2  86.0 1.680014\n72  2372.70 50.0  39   1    1  89.3 1.719864\n73  3239.66 50.0  23   1    1  72.4 1.840202\n74  1935.24 25.0  24   1    1  70.4 1.740103\n75  1344.35 25.0  30   1    1  74.3 1.800082\n76  1411.57 25.0  26   1    1 102.5 1.909892\n77  1712.00 25.0  19   1    1  72.7 1.710081\n78  2978.20 50.0  49   1    1  97.3 1.800026\n79  1948.80 50.0  39   1    1  75.7 1.780081\n80  1346.62 25.0  41   1    2  81.0 1.749966\n81  1380.61 25.0  19   1    1  66.2 1.800092\n82  1214.97 25.0  49   1    1  80.9 1.810146\n83  3622.80 50.0  32   1    2  92.2 1.799898\n84  3751.90 50.0  42   1    1  92.9 1.760028\n85  2092.89 25.0  39   2   88  58.2 1.619872\n86  3458.43 50.0  39   2    1  69.8 1.520031\n87  2789.70 50.0  49   2    7  69.6 1.580004\n88  2303.58 25.0  28   2    1  62.3 1.750123\n89  2030.50 25.0  26   1   88  63.6 1.719924\n90  1439.57 25.0  27   1    2  75.1 1.850127\n91  2471.60 50.0  45   1    2  93.8 1.780146\n92  1097.60 25.0  28   1    1  85.9 1.870176\n93  1464.29 25.0  25   1    1  87.1 1.839879\n94  3243.29 50.0  21   1   88  81.9 1.770065\n95  2654.70 25.0  37   1    2  78.6 1.740039\n96  3609.33 50.0  29   1    1  68.8 1.810025\n97  3060.70 50.0  25   1    2  81.0 1.780172\n98  1374.48 25.0  25   1    1  84.6 1.710058\n99  1451.50 25.0  36   2    1  88.2 1.710089\n100 1503.55 25.0  35   2    1  90.0 1.679941\n101 2027.60 25.0  28   2    2  58.9 1.580133\n102 3046.72 50.0  24   1    1  90.1 1.799956\n103 2485.00 50.0  29   1    1 115.3 1.930120\n104 1731.80 25.0  26   1   88  72.2 1.700092\n105 1958.27 25.0  26   1    1  70.5 1.679958\n106 2996.40 50.0  26   1    1  82.1 1.780073\n107 1288.64 25.0  28   1    2  77.5 1.819881\n108 2353.40 50.0  37   1    2  99.1 1.779932\n109 3016.30 50.0  45   1    1  90.0 1.730034\n110 3306.15 50.0  38   1    1  83.4 1.819932\n111  826.43 25.0  30   1    2 105.1 1.879883\n112 1338.20 25.0  40   1    1  97.3 1.860153\n113 1490.93 25.0  41   1    7  85.8 1.789940\n114 1067.56 25.0  37   1    1  85.4 1.820067\n115 2472.90 50.0  36   1    1  84.4 1.730002\n116 1085.93 25.0  41   1    1  77.1 1.589927\n117 2278.97 50.0  28   1    1 113.2 1.909995\n118 1898.00 25.0  27   1    2  89.0 1.829859\n119 1208.74 25.0  31   1    1 110.8 1.869859\n120 3593.55 50.0  23   1    1  96.3 1.820081\n\n# Set a random seed for reproducibility\nset.seed(1234)\nfile.exists(here::here(\"ml-models-exercise\", \"cleaned-data.rds\"))\n\n[1] FALSE\n\ncleaned_data &lt;- cleaned_data %&gt;%\n  mutate(RACE = as.numeric(as.character(RACE))) %&gt;%  # Convert to numeric\n  mutate(RACE = case_when(\n    RACE == 7  ~ 3,\n    RACE == 88 ~ 3,\n    TRUE ~ RACE  # Keep other values unchanged\n  ))\n\n# Verify changes\ntable(cleaned_data$RACE)\n\n\n 1  2  3 \n74 36 10 \n\n# Verify changes\ntable(cleaned_data$RACE)\n\n\n 1  2  3 \n74 36 10 \n\n# Save the updated dataset back to the same folder\nsaveRDS(cleaned_data, here::here(\"ml-models-exercise\", \"cleaned_data-updated.rds\"))\nPairwise correlation for the continuous variables. If this were to reveal strong correlations, we would consider removal of them.\n# Select only the continuous variables\ncontinuous_vars &lt;- cleaned_data %&gt;%\n  select(where(is.numeric))  # Select numeric variables only\n\n# Create the correlation plot\nggpairs(continuous_vars)\nAs seen above in the correlation coefficients, there are some variables with relatively strong corrrelations like 0.600 for height and weight (this one makes intuitive sense, for instance). However, nothing seems excessive (e.g., above an absolute value of 0.9).\nSo, as a result, it would be concluded that we do not have too much of a problem with collinearity."
  },
  {
    "objectID": "ml-models-exercise/ml-models-exercise.html#feature-engineering---what-it-is",
    "href": "ml-models-exercise/ml-models-exercise.html#feature-engineering---what-it-is",
    "title": "ml-models-exercise",
    "section": "Feature Engineering - What It Is",
    "text": "Feature Engineering - What It Is\nA term for creating new variables/doing stuff to the current variables to make new variations of them. For example, above, combining the races into the same grouping is an example of feature engineering. Height and weight have a relatively high correlation. It is suggested to make them into BMI (below). It looks like the weight is in pounds and the height is in meters already.\n\ncleaned_data &lt;- cleaned_data %&gt;%\n  mutate(BMI = WT / HT ^ 2)\n\n# Check summary statistics to ensure reasonable values\nsummary(cleaned_data$BMI)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  18.69   24.54   26.38   26.63   29.70   32.21 \n\n\nThe normal BMI range for an adult is typically between 18.5 and 25 for most adults, so the output is reasonable."
  },
  {
    "objectID": "ml-models-exercise/ml-models-exercise.html#model-building",
    "href": "ml-models-exercise/ml-models-exercise.html#model-building",
    "title": "ml-models-exercise",
    "section": "Model Building",
    "text": "Model Building\n\nLinear model with all predictors\nLASSO regression\nRandom forest model (RF)\n\n\nlibrary(glmnet)  # For LASSO\n\nLoading required package: Matrix\n\n\n\nAttaching package: 'Matrix'\n\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n\n\nLoaded glmnet 4.1-8\n\nlibrary(ranger)  # For Random Forest\n\nCreate the “recipe”\n\ncleaned_data$SEX &lt;- as.numeric(as.character(cleaned_data$SEX))\nrecipe_spec &lt;- recipe(Y ~ ., data = cleaned_data) %&gt;%\n  step_normalize(all_numeric(), -all_outcomes())   \n\nDefining the models, using the glmnet package for the LASSO model and the ranger packages of the random forest model.\n\n# Define the LASSO model with penalty = 0.1\nlasso_spec &lt;- linear_reg(penalty = 0.1) %&gt;%\n  set_engine(\"glmnet\")\n\n# Define the Random Forest model with random seed\nrf_spec &lt;- rand_forest(mode = \"regression\") %&gt;%\n  set_engine(\"ranger\", seed = 1234)\n\n# Random Forest workflow \nrf_workflow &lt;- workflow() %&gt;%\n  add_recipe(recipe_spec) %&gt;%\n  add_model(rf_spec)\n\n# Lasso workflow \nlasso_workflow &lt;- workflow() %&gt;%\n  add_recipe(recipe_spec) %&gt;%\n  add_model(lasso_spec)\n\nFitting the models\n\n# Fit the Linear model \nlm_fit &lt;- lm(Y ~ ., data = cleaned_data)\n\n# Fit the LASSO model\nlasso_fit &lt;- fit(lasso_workflow, data = cleaned_data)\n\n# Fit the Random Forest model\nrf_fit &lt;- fit(rf_workflow, data = cleaned_data)\n\nPredicting with each model and assessing the models’ quality via rmse\n\n# Make predictions for the Linear Model\nlm_predictions &lt;- predict(lm_fit, cleaned_data) %&gt;%\n  as_tibble() %&gt;%\n  bind_cols(cleaned_data %&gt;% select(Y)) %&gt;%\n  rename(predicted_Y = value)\n\n# Make predictions for the Linear Model\nlm_predictions &lt;- predict(lm_fit, cleaned_data) %&gt;%\n  as_tibble() %&gt;%\n  bind_cols(cleaned_data %&gt;% select(Y)) %&gt;%\n  rename(predicted_Y = value)\n\n# Make predictions for the LASSO model\nlasso_predictions &lt;- predict(lasso_fit, cleaned_data) %&gt;%\n  bind_cols(cleaned_data %&gt;% select(Y))\n\n# Make predictions for the Random Forest model\nrf_predictions &lt;- predict(rf_fit, cleaned_data) %&gt;%\n  bind_cols(cleaned_data %&gt;% select(Y))\n\n# Calculate RMSE for the Linear Model\nlm_rmse &lt;- rmse(lm_predictions, truth = Y, estimate = predicted_Y)\nlm_rmse\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard        581.\n\n# Calculate RMSE for the LASSO model\nlasso_rmse &lt;- rmse(lasso_predictions, truth = Y, estimate = .pred)\nlasso_rmse\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard        581.\n\n# Calculate RMSE for the Random Forest model\nrf_rmse &lt;- rmse(rf_predictions, truth = Y, estimate = .pred)\nrf_rmse\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard        359.\n\n\nPlotting the observed v. predicted for each model\n\n# Plotting: Observed vs Predicted for Linear Model\nplot1 = ggplot(lm_predictions, aes(x = Y, y = predicted_Y)) +\n  geom_point() +\n  geom_abline(intercept = 0, slope = 1, color = \"red\") +\n  labs(title = \"Observed vs Predicted: Linear Model\", \n       x = \"Observed Values\", \n       y = \"Predicted Values\") +\n  theme_minimal()\n\n# Plot observed vs. predicted for LASSO\nggplot(lasso_predictions, aes(x = Y, y = .pred)) +\n  geom_point() +\n  geom_abline(intercept = 0, slope = 1, color = \"red\") +  # Add 45-degree line\n  labs(title = \"Observed vs. Predicted (LASSO)\", x = \"Observed\", y = \"Predicted\")\n\n\n\n\n\n\n\n# Plot observed vs. predicted for Random Forest\nggplot(rf_predictions, aes(x = Y, y = .pred)) +\n  geom_point() +\n  geom_abline(intercept = 0, slope = 1, color = \"red\") +  # Add 45-degree line\n  labs(title = \"Observed vs. Predicted (Random Forest)\", x = \"Observed\", y = \"Predicted\")\n\n\n\n\n\n\n\n\n\nWhy Do the Linear Model and LASSO Give Almost the Same Results?\n\nLinear model: This is a standard regression model where the relationship between the predictors and the outcome is modeled as a linear function.\nLASSO (Least Absolute Shrinkage and Selection Operator): LASSO is a type of regularized linear regression where it applies a penalty (the penalty value is set to 0.1 in this case) to the regression coefficients, shrinking some of them toward zero to reduce overfitting.\n\nSince the penalty for LASSO (0.1) isn’t very large, it’s not shrinking the coefficients significantly. Therefore, the model behaves very similarly to a standard linear regression. In practice, this means that LASSO and linear regression will give nearly identical results unless the penalty is increased enough to cause significant shrinkage (which is typically done during model tuning).\nI wanted to try the penalty increase.\n\n# Define the second LASSO model with a larger penalty (penalty = 1)\nlasso_spec2 &lt;- linear_reg(penalty = 5) %&gt;%\n  set_engine(\"glmnet\")\n\n# Lasso workflow for the second model\nlasso_workflow2 &lt;- workflow() %&gt;%\n  add_recipe(recipe_spec) %&gt;%\n  add_model(lasso_spec2)\n\n# Fit the second LASSO model\nlasso_fit2 &lt;- fit(lasso_workflow2, data = cleaned_data)\n\n# Get predictions for the second LASSO model\nlasso_pred2 &lt;- predict(lasso_fit2, cleaned_data)\n\n# Add observed and predicted values to the data\nlasso_pred2 &lt;- cleaned_data %&gt;%\n  bind_cols(lasso_pred2) %&gt;%\n  rename(predicted_Y = .pred)\n\n# Plot observed vs predicted for the second LASSO model\nplot2 = ggplot(lasso_pred2, aes(x = Y, y = predicted_Y)) +\n  geom_point() +\n  geom_abline(intercept = 0, slope = 1, color = \"red\") +\n  labs(title = \"Observed vs Predicted: LASSO (Penalty = 2)\", \n       x = \"Observed Values\", \n       y = \"Predicted Values\") +\n  theme_minimal()\n\n\nplot1\n\n\n\n\n\n\n\nplot2\n\n\n\n\n\n\n\n\n\n\nRandom Forest Outperforms LASSO and Linear Models\n\nRandom Forest: Random forest is an ensemble method that constructs many decision trees and aggregates their predictions. It is more flexible than linear regression models and can capture complex relationships between the predictors and the outcome. In this case, the random forest model has a lower RMSE, indicating it is performing better by making predictions that are closer to the true values.\nOverfitting Risk: Although random forests perform well in terms of predictive accuracy, they can be prone to overfitting if the trees are too deep or if there are too many trees in the forest. This overfitting means the model might perform exceptionally well on training data but might struggle to generalize to unseen data."
  },
  {
    "objectID": "ml-models-exercise/ml-models-exercise.html#tuning-the-models---something-that-is-normally-bad-bad-without-the-cross-validation",
    "href": "ml-models-exercise/ml-models-exercise.html#tuning-the-models---something-that-is-normally-bad-bad-without-the-cross-validation",
    "title": "ml-models-exercise",
    "section": "Tuning the Models - Something that is Normally BAD BAD (without the cross validation)",
    "text": "Tuning the Models - Something that is Normally BAD BAD (without the cross validation)\nFor the LASSO model, we will tune the penalty parameter.\n\n# Define the LASSO tuning grid (penalty values between 1E-5 and 1E2)\nlasso_grid &lt;- tibble(penalty = 10^seq(-5, 2, length.out = 50))\n\n# Define LASSO model with a tunable penalty\nlasso_spec &lt;- linear_reg(penalty = tune()) %&gt;%\n  set_engine(\"glmnet\")\n\n# Define the workflow with the LASSO model\nlasso_workflow &lt;- workflow() %&gt;%\n  add_recipe(recipe_spec) %&gt;%\n  add_model(lasso_spec)\n\n# Fit the LASSO model for each tuning parameter directly without cross-validation\nlasso_tune_res &lt;- tune_grid(\n  object = lasso_workflow,\n  resamples = vfold_cv(cleaned_data),  \n  grid = lasso_grid,\n  control = control_grid(save_pred = TRUE)\n)\n\n# View the tuning results (only the penalty parameter will be varied)\nlasso_tune_res\n\n# Tuning results\n# 10-fold cross-validation \n# A tibble: 10 × 5\n   splits           id     .metrics           .notes           .predictions\n   &lt;list&gt;           &lt;chr&gt;  &lt;list&gt;             &lt;list&gt;           &lt;list&gt;      \n 1 &lt;split [108/12]&gt; Fold01 &lt;tibble [100 × 5]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n 2 &lt;split [108/12]&gt; Fold02 &lt;tibble [100 × 5]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n 3 &lt;split [108/12]&gt; Fold03 &lt;tibble [100 × 5]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n 4 &lt;split [108/12]&gt; Fold04 &lt;tibble [100 × 5]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n 5 &lt;split [108/12]&gt; Fold05 &lt;tibble [100 × 5]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n 6 &lt;split [108/12]&gt; Fold06 &lt;tibble [100 × 5]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n 7 &lt;split [108/12]&gt; Fold07 &lt;tibble [100 × 5]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n 8 &lt;split [108/12]&gt; Fold08 &lt;tibble [100 × 5]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n 9 &lt;split [108/12]&gt; Fold09 &lt;tibble [100 × 5]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n10 &lt;split [108/12]&gt; Fold10 &lt;tibble [100 × 5]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n\n# Plot the tuning results\nautoplot(lasso_tune_res)\n\n\n\n\n\n\n\n\n\nNotes for myself on LASSO RMSE Behavior\nAbove, you’ll see RMSE as function of penalty parameter. You should see that the LASSO does best (lowest RMSE) for low penalty values and gets worse if the penalty parameter increases. At the lowest penalty, the RMSE is the same as for the linear model. This makes sense to me because at smaller penalties, the LASSO model behaves like a standard linear regression model, with minimal shrinkage of the coefficients.\nAs the penalty increases, the coefficients are forced toward zero, which reduces the model’s complexity and prevents overfitting. However, if the penalty is too high, it will shrink the coefficients too much, leading to underfitting. Thus, the RMSE increases as the penalty grows because the model becomes too simple to capture the underlying relationships in the data.\n\nAt the lowest penalty: The LASSO model behaves like a standard linear regression, giving the same result as if no regularization were applied.\nAs the penalty increases: The model becomes more regularized, reducing the complexity and leading to worse performance on the training data (higher RMSE).\n\nThis is a classic case of the bias-variance tradeoff: higher regularization reduces variance but introduces bias, leading to higher RMSE.\n\nlibrary(dials)  # Make sure dials is loaded for parameter tuning\n\n# Define the tuning grid for Random Forest (mtry between 1 and 7, min_n between 1 and 21)\nrf_grid &lt;- grid_regular(\n  mtry(range = c(1, 7)),   # mtry parameter with values from 1 to 7\n  min_n(range = c(1, 21)),  # min_n parameter with values from 1 to 21\n  levels = 7  # 7 levels for each parameter\n)\n\n# Define Random Forest model with mtry and min_n as tuning parameters and fixed trees = 300\nrf_spec &lt;- rand_forest(mode = \"regression\", trees = 300) %&gt;%\n  set_engine(\"ranger\", mtry = tune(), min_n = tune())\n\n# Define the workflow with the Random Forest model\nrf_workflow &lt;- workflow() %&gt;%\n  add_recipe(recipe_spec) %&gt;%\n  add_model(rf_spec)\n\n# Tune the Random Forest model\nrf_tune_res &lt;- tune_grid(\n  object = rf_workflow,\n  resamples = vfold_cv(cleaned_data), \n  grid = rf_grid,\n  control = control_grid(save_pred = TRUE)\n)\n\n→ A | warning: The argument `mtry` cannot be manually modified and was removed., Unused arguments: min_n\n\n\nThere were issues with some computations   A: x1\n\n\nThere were issues with some computations   A: x7\n\n\nThere were issues with some computations   A: x62\n\n\nThere were issues with some computations   A: x112\n\n\nThere were issues with some computations   A: x167\n\n\nThere were issues with some computations   A: x223\n\n\nThere were issues with some computations   A: x279\n\n\nThere were issues with some computations   A: x335\n\n\nThere were issues with some computations   A: x390\n\n\nThere were issues with some computations   A: x445\n\n\nThere were issues with some computations   A: x490\n\n\n\n\n# View the tuning results\nprint(rf_tune_res)\n\n# Tuning results\n# 10-fold cross-validation \n# A tibble: 10 × 5\n   splits           id     .metrics          .notes            .predictions\n   &lt;list&gt;           &lt;chr&gt;  &lt;list&gt;            &lt;list&gt;            &lt;list&gt;      \n 1 &lt;split [108/12]&gt; Fold01 &lt;tibble [98 × 6]&gt; &lt;tibble [49 × 3]&gt; &lt;tibble&gt;    \n 2 &lt;split [108/12]&gt; Fold02 &lt;tibble [98 × 6]&gt; &lt;tibble [49 × 3]&gt; &lt;tibble&gt;    \n 3 &lt;split [108/12]&gt; Fold03 &lt;tibble [98 × 6]&gt; &lt;tibble [49 × 3]&gt; &lt;tibble&gt;    \n 4 &lt;split [108/12]&gt; Fold04 &lt;tibble [98 × 6]&gt; &lt;tibble [49 × 3]&gt; &lt;tibble&gt;    \n 5 &lt;split [108/12]&gt; Fold05 &lt;tibble [98 × 6]&gt; &lt;tibble [49 × 3]&gt; &lt;tibble&gt;    \n 6 &lt;split [108/12]&gt; Fold06 &lt;tibble [98 × 6]&gt; &lt;tibble [49 × 3]&gt; &lt;tibble&gt;    \n 7 &lt;split [108/12]&gt; Fold07 &lt;tibble [98 × 6]&gt; &lt;tibble [49 × 3]&gt; &lt;tibble&gt;    \n 8 &lt;split [108/12]&gt; Fold08 &lt;tibble [98 × 6]&gt; &lt;tibble [49 × 3]&gt; &lt;tibble&gt;    \n 9 &lt;split [108/12]&gt; Fold09 &lt;tibble [98 × 6]&gt; &lt;tibble [49 × 3]&gt; &lt;tibble&gt;    \n10 &lt;split [108/12]&gt; Fold10 &lt;tibble [98 × 6]&gt; &lt;tibble [49 × 3]&gt; &lt;tibble&gt;    \n\nThere were issues with some computations:\n\n  - Warning(s) x490: The argument `mtry` cannot be manually modified and was removed.,...\n\nRun `show_notes(.Last.tune.result)` for more information.\n\n# Plot the tuning results (RMSE vs mtry and min_n)\nautoplot(rf_tune_res)\n\n\n\n\n\n\n\n\nAbove, I see that the RSME varies with the tuning. I think there is a slight bug here regarding mtry but I could not figure it out.  \n\n\nNotes for myself on Random Forest Tuning\nFor Random Forest, the tuning of mtry and min_n affects how the tree splits. A higher value of mtry means the model is considering more features at each split, potentially leading to better model performance. A lower value of min_n allows more splits, making the model more flexible and potentially overfitting. The combination of these two parameters determines the complexity of the model.\n\n\nKey Points about mtry:\nIn the context of Random Forests, mtry is a hyperparameter that specifies the number of variables (or features) to be randomly selected at each split when growing a tree.\n\nmtry controls the number of features considered for splitting at each node of the decision tree.\nThe default value of mtry in many implementations of Random Forests is usually the square root of the total number of features (for classification problems), or the number of features divided by 3 (for regression problems). This is a good starting point to reduce overfitting.\nSmaller values of mtry (i.e., fewer features per split) lead to more randomness and less correlation between trees, potentially improving generalization but reducing accuracy.\nLarger values of mtry (i.e., more features per split) make the model more deterministic and could lead to over fitting because the trees are more correlated.\n\nIt influences how “random” each tree in the forest is.\n\nHigher mtry and lower min_n usually lead to a more flexible model that captures more patterns in the data, which is why they perform better in this case.\n\n\n\nKey Points about min_n:\nIn the context of Random Forests, min_n is a hyperparameter that specifies the minimum number of data points (or observations) required to create a leaf node (the final decision-making point) in a decision tree.\n\nmin_n controls how “deep” or “complex” a decision tree can grow. If the number of observations in a node is less than min_n, the tree will stop growing further in that branch.\nSmaller values of min_n allow the trees to grow deeper, as the algorithm will be willing to split nodes even if there are only a few data points left. This can lead to overfitting because the model can perfectly fit to the noise in the data.\nLarger values of min_n lead to shallower trees because the algorithm will stop splitting earlier, which can help prevent overfitting by limiting the depth of the tree. However, if set too large, the model might become too simple and underfit the data."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "",
    "text": "The structure below is one possible setup for a data analysis project (including the course project). For a manuscript, adjust as needed. You don’t need to have exactly these sections, but the content covering those sections should be addressed.\nThis uses MS Word as output format. See here for more information. You can switch to other formats, like html or pdf. See the Quarto documentation for other formats."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#general-background-information",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#general-background-information",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "2.1 General Background Information",
    "text": "2.1 General Background Information\nProvide enough background on your topic that others can understand the why and how of your analysis"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#description-of-data-and-data-source",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#description-of-data-and-data-source",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "2.2 Description of data and data source",
    "text": "2.2 Description of data and data source\nThe dataset originally contained a collection of personal and preference-related information, primarily focused on physical attributes. The variables in the dataset are as follows: height, weight, and gender. The variable attributes of shoe size in men’s US standard sizing and favorite pizza flavors among the four options of sausage, cheese, supreme, and pepperoni were added to the file."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#questionshypotheses-to-be-addressed",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#questionshypotheses-to-be-addressed",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "2.3 Questions/Hypotheses to be addressed",
    "text": "2.3 Questions/Hypotheses to be addressed\nState the research questions you plan to answer with this analysis.\nTo cite other work (important everywhere, but likely happens first in introduction), make sure your references are in the bibtex file specified in the YAML header above (here dataanalysis_template_references.bib) and have the right bibtex key. Then you can include like this:\nExamples of reproducible research projects can for instance be found in (McKay, Ebell, Billings, et al., 2020; McKay, Ebell, Dale, Shen, & Handel, 2020)"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#data-aquisition",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#data-aquisition",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "3.1 Data aquisition",
    "text": "3.1 Data aquisition\nAs applicable, explain where and how you got the data. If you directly import the data from an online source, you can combine this section with the next."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#data-import-and-cleaning",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#data-import-and-cleaning",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "3.2 Data import and cleaning",
    "text": "3.2 Data import and cleaning\nWrite code that reads in the file and cleans it so it’s ready for analysis. Since this will be fairly long code for most datasets, it might be a good idea to have it in one or several R scripts. If that is the case, explain here briefly what kind of cleaning/processing you do, and provide more details and well documented code somewhere (e.g. as supplement in a paper). All materials, including files that contain code, should be commented well so everyone can follow along."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#statistical-analysis",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#statistical-analysis",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "3.3 Statistical analysis",
    "text": "3.3 Statistical analysis\nExplain anything related to your statistical analyses."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#exploratorydescriptive-analysis",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#exploratorydescriptive-analysis",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "4.1 Exploratory/Descriptive analysis",
    "text": "4.1 Exploratory/Descriptive analysis\nUse a combination of text/tables/figures to explore and describe your data. Show the most important descriptive results here. Additional ones should go in the supplement. Even more can be in the R and Quarto files that are part of your project.\nTable 1 shows a summary of the data.\nNote the loading of the data providing a relative path using the ../../ notation. (Two dots means a folder up). You never want to specify an absolute path like C:\\ahandel\\myproject\\results\\ because if you share this with someone, it won’t work for them since they don’t have that path. You can also use the here R package to create paths. See examples of that below. I recommend the here package, but I’m showing the other approach here just in case you encounter it.\n\n\n\n\nTable 1: Data summary table.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_type\nskim_variable\nn_missing\ncomplete_rate\nfactor.ordered\nfactor.n_unique\nfactor.top_counts\nnumeric.mean\nnumeric.sd\nnumeric.p0\nnumeric.p25\nnumeric.p50\nnumeric.p75\nnumeric.p100\nnumeric.hist\n\n\n\n\nfactor\nGender\n0\n1\nFALSE\n3\nM: 4, F: 3, O: 2\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nnumeric\nHeight\n0\n1\nNA\nNA\nNA\n165.66667\n15.97655\n133\n156\n166\n178\n183\n▂▁▃▃▇\n\n\nnumeric\nWeight\n0\n1\nNA\nNA\nNA\n70.11111\n21.24526\n45\n55\n70\n80\n110\n▇▂▃▂▂"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#basic-statistical-analysis",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#basic-statistical-analysis",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "4.2 Basic statistical analysis",
    "text": "4.2 Basic statistical analysis\nTo get some further insight into your data, if reasonable you could compute simple statistics (e.g. simple models with 1 predictor) to look for associations between your outcome(s) and each individual predictor variable. Though note that unless you pre-specified the outcome and main exposure, any “p&lt;0.05 means statistical significance” interpretation is not valid.\nFigure 1 shows a scatterplot figure produced by one of the R scripts.\n\n\n\n\n\n\n\n\nFigure 1: Height and weight stratified by gender."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#full-analysis",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#full-analysis",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "4.3 Full analysis",
    "text": "4.3 Full analysis\nUse one or several suitable statistical/machine learning methods to analyze your data and to produce meaningful figures, tables, etc. This might again be code that is best placed in one or several separate R scripts that need to be well documented. You want the code to produce figures and data ready for display as tables, and save those. Then you load them here.\nExample Table 2 shows a summary of a linear model fit.\n\n\n\n\nTable 2: Linear model fit table.\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n149.2726967\n23.3823360\n6.3839942\n0.0013962\n\n\nWeight\n0.2623972\n0.3512436\n0.7470519\n0.4886517\n\n\nGenderM\n-2.1244913\n15.5488953\n-0.1366329\n0.8966520\n\n\nGenderO\n-4.7644739\n19.0114155\n-0.2506112\n0.8120871\n\n\n\n\n\n\n\n\n\n\n\n\nTable 3: Linear model fit table.\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n148.192308\n33.858903\n4.3767605\n0.0071764\n\n\nFavorite.Pizza.FlavorPepperoni\n7.884615\n17.777558\n0.4435151\n0.6759271\n\n\nFavorite.Pizza.FlavorSupreme\n21.346154\n22.542078\n0.9469470\n0.3871485\n\n\nShoe.Size\n1.692308\n4.311691\n0.3924928\n0.7108678\n\n\n\n\n\n\n\n\n(resulttable3?) shows a linear model fit table of height as predicted by one’s favorite pizza flavor and the individual’s shoe size\n\n\n\n\n\n\n\n\nFigure 2: Boxplot of Height as a Function of Favorite Pizza Flavors.\n\n\n\n\n\nFigure 2 displays a box plot of height as a function of one’s favorite pizza flavor\n\n\n\n\n\n\n\n\nFigure 3: Scatterplot of Weight v. Shoe Size.\n\n\n\n\n\nFigure 3 displays a scatter plot of weight vs. shoe size"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#summary-and-interpretation",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#summary-and-interpretation",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "5.1 Summary and Interpretation",
    "text": "5.1 Summary and Interpretation\nSummarize what you did, what you found and what it means."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#strengths-and-limitations",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#strengths-and-limitations",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "5.2 Strengths and Limitations",
    "text": "5.2 Strengths and Limitations\nDiscuss what you perceive as strengths and limitations of your analysis."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#conclusions",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#conclusions",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "5.3 Conclusions",
    "text": "5.3 Conclusions\nWhat are the main take-home messages?\nInclude citations in your Rmd file using bibtex, the list of references will automatically be placed at the end\nThis paper (Leek & Peng, 2015) discusses types of analyses.\nThese papers (McKay, Ebell, Billings, et al., 2020; McKay, Ebell, Dale, et al., 2020) are good examples of papers published using a fully reproducible setup similar to the one shown in this template.\nNote that this cited reference will show up at the end of the document, the reference formatting is determined by the CSL file specified in the YAML header. Many more style files for almost any journal are available. You also specify the location of your bibtex reference file in the YAML. You can call your reference file anything you like, I just used the generic word references.bib but giving it a more descriptive name is probably better.\n\nprint"
  },
  {
    "objectID": "starter-analysis-exercise/code/readme.html",
    "href": "starter-analysis-exercise/code/readme.html",
    "title": "Hope Grismer's Data Analysis Portfolio",
    "section": "",
    "text": "Place your various R or Quarto files in the appropriate folders.\nYou can either have fewer large scripts, or multiple scripts that do only specific actions. Those can be R or Quarto files. In either case, document the scripts and what goes on in them so well that someone else (including future you) can easily figure out what is happening.\nThe scripts should load the appropriate data (e.g. raw or processed), perform actions, and save results (e.g. processed data, figures, computed values) in the appropriate folders. Document somewhere what inputs each script takes and where output is placed.\nIf scripts need to be run in a specific order, document this. Either as comments in the script, or in a separate text file such as this readme file. Ideally of course in both locations.\nDepending on your specific project, you might want to have further folders/sub-folders."
  },
  {
    "objectID": "starter-analysis-exercise/code/eda-code/eda.html",
    "href": "starter-analysis-exercise/code/eda-code/eda.html",
    "title": "An example exploratory analysis script",
    "section": "",
    "text": "This Quarto file loads the cleaned data and does some exploring.\nI’m only showing it the way where the code is included in the file. As described in the processing_code materials, I currently prefer the approach of having R code in a separate file and pulling it in.\nBut I already had this written and haven’t yet re-done it that way. Feel free to redo and send a pull request on GitHub :)\nAgain, it is largely a matter of preference and what makes the most sense to decide if one wants to have code inside Quarto files, or as separate R files. And sometimes, an R script with enough comments is good enough and one doesn’t need a Quarto file.\nAlso note that while here I split cleaning and exploring, this is iterative. You saw that as part of the processing, we already had to explore the data somewhat to understand how to clean it. In general, as you explore, you’ll find things that need cleaning. As you clean, you can explore more. Therefore, at times it might make more sense to combine the cleaning and exploring code parts into a single R or Quarto file. Or split things in any other logical way.\nAs part of the exploratory analysis, you should produce plots or tables or other summary quantities for the most interesting/important quantities in your data. Depending on the total number of variables in your dataset, explore all or some of the others. Figures produced here might be histograms or density plots, correlation plots, etc. Tables might summarize your data.\nStart by exploring one variable at a time. Then continue by creating plots or tables of the outcome(s) of interest and the predictor/exposure/input variables you are most interested in. If your dataset is small, you can do that for all variables.\nPlots produced here can be scatterplots, boxplots, violinplots, etc. Tables can be simple 2x2 tables or larger ones.\n\nSetup\n\n#load needed packages. make sure they are installed.\nlibrary(here) #for data loading/saving\n\nhere() starts at /Users/hopegrismercomputer/Desktop/MADA-course/hopegrismer-MADA-portfolio\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(skimr)\nlibrary(ggplot2)\n\nLoad the data.\n\n#Path to data. Note the use of the here() package and not absolute paths\ndata_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"processed-data\",\"processeddata.rds\")\n#load data\nmydata &lt;- readRDS(data_location)\n\n\n\nData exploration through tables\nShowing a bit of code to produce and save a summary table.\n\nsummary_df = skimr::skim(mydata)\nprint(summary_df)\n\n── Data Summary ────────────────────────\n                           Values\nName                       mydata\nNumber of rows             9     \nNumber of columns          3     \n_______________________          \nColumn type frequency:           \n  factor                   1     \n  numeric                  2     \n________________________         \nGroup variables            None  \n\n── Variable type: factor ───────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate ordered n_unique top_counts      \n1 Gender                0             1 FALSE          3 M: 4, F: 3, O: 2\n\n── Variable type: numeric ──────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate  mean   sd  p0 p25 p50 p75 p100 hist \n1 Height                0             1 166.  16.0 133 156 166 178  183 ▂▁▃▃▇\n2 Weight                0             1  70.1 21.2  45  55  70  80  110 ▇▂▃▂▂\n\n# save to file\nsummarytable_file = here(\"starter-analysis-exercise\",\"results\", \"tables-files\", \"summarytable.rds\")\nsaveRDS(summary_df, file = summarytable_file)\n\nWe are saving the results to the results/tables folder. Structure the folders inside results such that they make sense for your specific analysis. Provide enough documentation that someone can understand what you are doing and what goes where. readme.md files inside each folder are a good idea.\n\n\nData exploration through figures\nHistogram plots for the continuous outcomes.\nHeight first.\n\np1 &lt;- mydata %&gt;% ggplot(aes(x=Height)) + geom_histogram() \nplot(p1)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-distribution.png\")\nggsave(filename = figure_file, plot=p1) \n\nSaving 7 x 5 in image\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nNow weights.\n\np2 &lt;- mydata %&gt;% ggplot(aes(x=Weight)) + geom_histogram() \nplot(p2)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"weight-distribution.png\")\nggsave(filename = figure_file, plot=p2) \n\nSaving 7 x 5 in image\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nNow height as function of weight.\n\np3 &lt;- mydata %&gt;% ggplot(aes(x=Height, y=Weight)) + geom_point() + geom_smooth(method='lm')\nplot(p3)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-weight.png\")\nggsave(filename = figure_file, plot=p3) \n\nSaving 7 x 5 in image\n`geom_smooth()` using formula = 'y ~ x'\n\n\nOnce more height as function of weight, stratified by gender. Note that there is so little data, it’s a bit silly. But we’ll plot it anyway.\n\np4 &lt;- mydata %&gt;% ggplot(aes(x=Height, y=Weight, color = Gender)) + geom_point() + geom_smooth(method='lm')\nplot(p4)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning in qt((1 - level)/2, df): NaNs produced\n\n\nWarning in max(ids, na.rm = TRUE): no non-missing arguments to max; returning\n-Inf\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-weight-stratified.png\")\nggsave(filename = figure_file, plot=p4) \n\nSaving 7 x 5 in image\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning in qt((1 - level)/2, df): NaNs produced\nWarning in qt((1 - level)/2, df): no non-missing arguments to max; returning\n-Inf\n\n\n\n\nNotes\nFor your own explorations, tables and figures can be “quick and dirty”. As long as you can see what’s going on, there is no need to polish them. That’s in contrast to figures you’ll produce for your final products (paper, report, presentation, website, etc.). Those should look as nice, polished and easy to understand as possible."
  },
  {
    "objectID": "starter-analysis-exercise/code/processing-code/processingfile.html",
    "href": "starter-analysis-exercise/code/processing-code/processingfile.html",
    "title": "An example cleaning script",
    "section": "",
    "text": "Processing script\nThis Quarto file contains a mix of code and explanatory text to illustrate a simple data processing/cleaning setup.\n\n\nSetup\nLoad needed packages. make sure they are installed.\n\nlibrary(readxl) #for loading Excel files\nlibrary(dplyr) #for data processing/cleaning\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyr) #for data processing/cleaning\nlibrary(skimr) #for nice visualization of data \nlibrary(here) #to set paths\n\nhere() starts at /Users/hopegrismercomputer/Desktop/MADA-course/hopegrismer-MADA-portfolio\n\n\n\n\nData loading\nNote that for functions that come from specific packages (instead of base R), I often specify both package and function like so: package::function() that’s not required one could just call the function specifying the package makes it clearer where the function “lives”, but it adds typing. You can do it either way.\n\n# path to data\n# note the use of the here() package and not absolute paths\ndata_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"raw-data\",\"exampledata.xlsx\")\nrawdata &lt;- readxl::read_excel(data_location)\n\n\n\nCheck data\nFirst we can look at the codebook\n\ncodebook &lt;- readxl::read_excel(data_location, sheet =\"Codebook\")\nprint(codebook)\n\n# A tibble: 3 × 3\n  `Variable Name` `Variable Definition`                 `Allowed Values`      \n  &lt;chr&gt;           &lt;chr&gt;                                 &lt;chr&gt;                 \n1 Height          height in centimeters                 numeric value &gt;0 or NA\n2 Weight          weight in kilograms                   numeric value &gt;0 or NA\n3 Gender          identified gender (male/female/other) M/F/O/NA              \n\n\nSeveral ways of looking at the data\n\ndplyr::glimpse(rawdata)\n\nRows: 14\nColumns: 3\n$ Height &lt;chr&gt; \"180\", \"175\", \"sixty\", \"178\", \"192\", \"6\", \"156\", \"166\", \"155\", …\n$ Weight &lt;dbl&gt; 80, 70, 60, 76, 90, 55, 90, 110, 54, 7000, NA, 45, 55, 50\n$ Gender &lt;chr&gt; \"M\", \"O\", \"F\", \"F\", \"NA\", \"F\", \"O\", \"M\", \"N\", \"M\", \"F\", \"F\", \"M…\n\nsummary(rawdata)\n\n    Height              Weight          Gender         \n Length:14          Min.   :  45.0   Length:14         \n Class :character   1st Qu.:  55.0   Class :character  \n Mode  :character   Median :  70.0   Mode  :character  \n                    Mean   : 602.7                     \n                    3rd Qu.:  90.0                     \n                    Max.   :7000.0                     \n                    NA's   :1                          \n\nhead(rawdata)\n\n# A tibble: 6 × 3\n  Height Weight Gender\n  &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt; \n1 180        80 M     \n2 175        70 O     \n3 sixty      60 F     \n4 178        76 F     \n5 192        90 NA    \n6 6          55 F     \n\nskimr::skim(rawdata)\n\n\nData summary\n\n\nName\nrawdata\n\n\nNumber of rows\n14\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nHeight\n0\n1\n1\n5\n0\n13\n0\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nWeight\n1\n0.93\n602.69\n1922.25\n45\n55\n70\n90\n7000\n▇▁▁▁▁\n\n\n\n\n\n\n\nCleaning\nBy inspecting the data as done above, we find some problems that need addressing:\nFirst, there is an entry for height which says “sixty” instead of a number. Does that mean it should be a numeric 60? It somehow doesn’t make sense since the weight is 60kg, which can’t happen for a 60cm person (a baby). Since we don’t know how to fix this, we might decide to remove the person. This “sixty” entry also turned all Height entries into characters instead of numeric. That conversion to character also means that our summary function isn’t very meaningful. So let’s fix that first.\n\nd1 &lt;- rawdata %&gt;% dplyr::filter( Height != \"sixty\" ) %&gt;% \n                  dplyr::mutate(Height = as.numeric(Height))\nskimr::skim(d1)\n\n\nData summary\n\n\nName\nd1\n\n\nNumber of rows\n13\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1.00\n151.62\n46.46\n6\n154.00\n165\n175\n192\n▁▁▁▂▇\n\n\nWeight\n1\n0.92\n647.92\n2000.48\n45\n54.75\n73\n90\n7000\n▇▁▁▁▁\n\n\n\n\nhist(d1$Height)\n\n\n\n\n\n\n\n\nNow we see that there is one person with a height of 6. That could be a typo, or someone mistakenly entered their height in feet. Since we unfortunately don’t know, we might need to remove this person, which we’ll do here.\n\nd2 &lt;- d1 %&gt;% dplyr::mutate( Height = replace(Height, Height==\"6\",round(6*30.48,0)) )\nskimr::skim(d2)\n\n\nData summary\n\n\nName\nd2\n\n\nNumber of rows\n13\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1.00\n165.23\n16.52\n133\n155.00\n166\n178\n192\n▂▇▆▆▃\n\n\nWeight\n1\n0.92\n647.92\n2000.48\n45\n54.75\n73\n90\n7000\n▇▁▁▁▁\n\n\n\n\n\nHeight values seem ok now.\nNow let’s look at the Weight variable. There is a person with weight of 7000, which is impossible, and one person with missing weight. To be able to analyze the data, we’ll remove those individuals as well.\n\nd3 &lt;- d2 %&gt;%  dplyr::filter(Weight != 7000) %&gt;% tidyr::drop_na()\nskimr::skim(d3)\n\n\nData summary\n\n\nName\nd3\n\n\nNumber of rows\n11\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n167.09\n16.81\n133\n155.5\n166\n179\n192\n▂▇▅▇▅\n\n\nWeight\n0\n1\n70.45\n20.65\n45\n54.5\n70\n85\n110\n▇▂▃▃▂\n\n\n\n\n\nNow checking the Gender variable. Gender should be a categorical/factor variable but is loaded as character. We can fix that with simple base R code to mix things up.\n\nd3$Gender &lt;- as.factor(d3$Gender)  \nskimr::skim(d3)\n\n\nData summary\n\n\nName\nd3\n\n\nNumber of rows\n11\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n1\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nGender\n0\n1\nFALSE\n5\nM: 4, F: 3, O: 2, N: 1\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n167.09\n16.81\n133\n155.5\n166\n179\n192\n▂▇▅▇▅\n\n\nWeight\n0\n1\n70.45\n20.65\n45\n54.5\n70\n85\n110\n▇▂▃▃▂\n\n\n\n\n\nNow we see that there is another NA, but it’s not NA from R, instead it was loaded as character and is now considered as a category. Well proceed here by removing that individual with that NA entry. Since this keeps an empty category for Gender, I’m also using droplevels() to get rid of it.\n\nd4 &lt;- d3 %&gt;% dplyr::filter( !(Gender %in% c(\"NA\",\"N\")) ) %&gt;% droplevels()\nskimr::skim(d4)\n\n\nData summary\n\n\nName\nd4\n\n\nNumber of rows\n9\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n1\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nGender\n0\n1\nFALSE\n3\nM: 4, F: 3, O: 2\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n165.67\n15.98\n133\n156\n166\n178\n183\n▂▁▃▃▇\n\n\nWeight\n0\n1\n70.11\n21.25\n45\n55\n70\n80\n110\n▇▂▃▂▂\n\n\n\n\n\nAll done, data is clean now.\nLet’s assign at the end to some final variable, this makes it easier to add further cleaning steps above.\n\nprocesseddata &lt;- d4\n\n\n\nSave data\nFinally, we save the clean data as RDS file. I suggest you save your processed and cleaned data as RDS or RDA/Rdata files. This preserves coding like factors, characters, numeric, etc. If you save as CSV, that information would get lost. However, CSV is better for sharing with others since it’s plain text. If you do CSV, you might want to write down somewhere what each variable is.\nSee here for some suggestions on how to store your processed data: http://www.sthda.com/english/wiki/saving-data-into-r-data-format-rds-and-rdata\n\nsave_data_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"processed-data\",\"processeddata.rds\")\nsaveRDS(processeddata, file = save_data_location)\n\nNote the use of the here package and here command to specify a path relative to the main project directory, that is the folder that contains the .Rproj file. Always use this approach instead of hard-coding file paths that only exist on your computer.\n\n\nNotes\nRemoving anyone observation with “faulty” or missing data is one approach. It’s often not the best. based on your question and your analysis approach, you might want to do cleaning differently (e.g. keep observations with some missing information)."
  },
  {
    "objectID": "starter-analysis-exercise/code/processing-code/processingfile2.html",
    "href": "starter-analysis-exercise/code/processing-code/processingfile2.html",
    "title": "An example cleaning script",
    "section": "",
    "text": "Processing script\nThis Quarto file contains a mix of code and explanatory text to illustrate a simple data processing/cleaning setup.\n\n\nSetup\nLoad needed packages. make sure they are installed.\n\nlibrary(readxl) #for loading Excel files\nlibrary(dplyr) #for data processing/cleaning\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyr) #for data processing/cleaning\nlibrary(skimr) #for nice visualization of data \nlibrary(here) #to set paths\n\nhere() starts at /Users/hopegrismercomputer/Desktop/MADA-course/hopegrismer-MADA-portfolio\n\n\n\n\nData loading\nNote that for functions that come from specific packages (instead of base R), I often specify both package and function like so: package::function() that’s not required one could just call the function specifying the package makes it clearer where the function “lives”, but it adds typing. You can do it either way.\n\n# path to data\n# note the use of the here() package and not absolute paths\ndata_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"raw-data\",\"exampledata2.xlsx\")\nrawdata &lt;- readxl::read_excel(data_location)\n\n\n\nCheck data\nFirst we can look at the codebook\n\ncodebook &lt;- readxl::read_excel(data_location, sheet =\"Codebook\")\nprint(codebook)\n\n# A tibble: 5 × 3\n  `Variable Name`               `Variable Definition`           `Allowed Values`\n  &lt;chr&gt;                         &lt;chr&gt;                           &lt;chr&gt;           \n1 Height                        height in centimeters           numeric value &gt;…\n2 Weight                        weight in kilograms             numeric value &gt;…\n3 Gender                        identified gender (male/female… M/F/O/NA        \n4 Shoe Size                     shoe size in US sizing (mens/w… numeric value &gt;0\n5 Favorite Typical Pizza Flavor favorite popular pizza flavors  pepperoni, chee…\n\n\nSeveral ways of looking at the data\n\ndplyr::glimpse(rawdata)\n\nRows: 14\nColumns: 5\n$ Height                          &lt;chr&gt; \"180\", \"175\", \"sixty\", \"178\", \"192\", \"…\n$ Weight                          &lt;dbl&gt; 80, 70, 60, 76, 90, 55, 90, 110, 54, 7…\n$ Gender                          &lt;chr&gt; \"M\", \"O\", \"F\", \"F\", \"NA\", \"F\", \"O\", \"M…\n$ `Shoe Size`                     &lt;dbl&gt; 9, 7, 6, 5, 7, 7, 10, 11, 9, 10, 6, 5,…\n$ `Favorite Typical Pizza Flavor` &lt;chr&gt; \"Pepperoni\", \"Cheese\", \"Sausage\", \"Sup…\n\nsummary(rawdata)\n\n    Height              Weight          Gender            Shoe Size     \n Length:14          Min.   :  45.0   Length:14          Min.   : 5.000  \n Class :character   1st Qu.:  55.0   Class :character   1st Qu.: 6.250  \n Mode  :character   Median :  70.0   Mode  :character   Median : 7.500  \n                    Mean   : 602.7                      Mean   : 7.786  \n                    3rd Qu.:  90.0                      3rd Qu.: 9.000  \n                    Max.   :7000.0                      Max.   :11.000  \n                    NA's   :1                                           \n Favorite Typical Pizza Flavor\n Length:14                    \n Class :character             \n Mode  :character             \n                              \n                              \n                              \n                              \n\nhead(rawdata)\n\n# A tibble: 6 × 5\n  Height Weight Gender `Shoe Size` `Favorite Typical Pizza Flavor`\n  &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;        &lt;dbl&gt; &lt;chr&gt;                          \n1 180        80 M                9 Pepperoni                      \n2 175        70 O                7 Cheese                         \n3 sixty      60 F                6 Sausage                        \n4 178        76 F                5 Supreme                        \n5 192        90 NA               7 Sausage                        \n6 6          55 F                7 Cheese                         \n\nskimr::skim(rawdata)\n\n\nData summary\n\n\nName\nrawdata\n\n\nNumber of rows\n14\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n3\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nHeight\n0\n1\n1\n5\n0\n13\n0\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\nFavorite Typical Pizza Flavor\n0\n1\n6\n9\n0\n4\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nWeight\n1\n0.93\n602.69\n1922.25\n45\n55.00\n70.0\n90\n7000\n▇▁▁▁▁\n\n\nShoe Size\n0\n1.00\n7.79\n1.93\n5\n6.25\n7.5\n9\n11\n▇▆▂▆▆\n\n\n\n\n\n\n\nCleaning\nBy inspecting the data as done above, we find some problems that need addressing:\nFirst, there is an entry for height which says “sixty” instead of a number. Does that mean it should be a numeric 60? It somehow doesn’t make sense since the weight is 60kg, which can’t happen for a 60cm person (a baby). Since we don’t know how to fix this, we might decide to remove the person. This “sixty” entry also turned all Height entries into characters instead of numeric. That conversion to character also means that our summary function isn’t very meaningful. So let’s fix that first.\n\nd1 &lt;- rawdata %&gt;% dplyr::filter( Height != \"sixty\" ) %&gt;% \n                  dplyr::mutate(Height = as.numeric(Height))\nskimr::skim(d1)\n\n\nData summary\n\n\nName\nd1\n\n\nNumber of rows\n13\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\nFavorite Typical Pizza Flavor\n0\n1\n6\n9\n0\n4\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1.00\n151.62\n46.46\n6\n154.00\n165\n175\n192\n▁▁▁▂▇\n\n\nWeight\n1\n0.92\n647.92\n2000.48\n45\n54.75\n73\n90\n7000\n▇▁▁▁▁\n\n\nShoe Size\n0\n1.00\n7.92\n1.93\n5\n7.00\n8\n9\n11\n▇▇▂▇▇\n\n\n\n\nhist(d1$Height)\n\n\n\n\n\n\n\n\nNow we see that there is one person with a height of 6. That could be a typo, or someone mistakenly entered their height in feet. Since we unfortunately don’t know, we might need to remove this person, which we’ll do here.\n\nd2 &lt;- d1 %&gt;% dplyr::mutate( Height = replace(Height, Height==\"6\",round(6*30.48,0)) )\nskimr::skim(d2)\n\n\nData summary\n\n\nName\nd2\n\n\nNumber of rows\n13\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\nFavorite Typical Pizza Flavor\n0\n1\n6\n9\n0\n4\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1.00\n165.23\n16.52\n133\n155.00\n166\n178\n192\n▂▇▆▆▃\n\n\nWeight\n1\n0.92\n647.92\n2000.48\n45\n54.75\n73\n90\n7000\n▇▁▁▁▁\n\n\nShoe Size\n0\n1.00\n7.92\n1.93\n5\n7.00\n8\n9\n11\n▇▇▂▇▇\n\n\n\n\n\nHeight values seem ok now.\nNow let’s look at the Weight variable. There is a person with weight of 7000, which is impossible, and one person with missing weight. To be able to analyze the data, we’ll remove those individuals as well.\n\nd3 &lt;- d2 %&gt;%  dplyr::filter(Weight != 7000) %&gt;% tidyr::drop_na()\nskimr::skim(d3)\n\n\nData summary\n\n\nName\nd3\n\n\nNumber of rows\n11\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\nFavorite Typical Pizza Flavor\n0\n1\n6\n9\n0\n4\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n167.09\n16.81\n133\n155.5\n166\n179\n192\n▂▇▅▇▅\n\n\nWeight\n0\n1\n70.45\n20.65\n45\n54.5\n70\n85\n110\n▇▂▃▃▂\n\n\nShoe Size\n0\n1\n7.91\n1.92\n5\n7.0\n8\n9\n11\n▅▇▂▇▅\n\n\n\n\n\nNow checking the Gender variable. Gender should be a categorical/factor variable but is loaded as character. We can fix that with simple base R code to mix things up.\n\nd3$Gender &lt;- as.factor(d3$Gender)  \nskimr::skim(d3)\n\n\nData summary\n\n\nName\nd3\n\n\nNumber of rows\n11\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nfactor\n1\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nFavorite Typical Pizza Flavor\n0\n1\n6\n9\n0\n4\n0\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nGender\n0\n1\nFALSE\n5\nM: 4, F: 3, O: 2, N: 1\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n167.09\n16.81\n133\n155.5\n166\n179\n192\n▂▇▅▇▅\n\n\nWeight\n0\n1\n70.45\n20.65\n45\n54.5\n70\n85\n110\n▇▂▃▃▂\n\n\nShoe Size\n0\n1\n7.91\n1.92\n5\n7.0\n8\n9\n11\n▅▇▂▇▅\n\n\n\n\n\nNow we see that there is another NA, but it’s not NA from R, instead it was loaded as character and is now considered as a category. Well proceed here by removing that individual with that NA entry. Since this keeps an empty category for Gender, I’m also using droplevels() to get rid of it.\n\nd4 &lt;- d3 %&gt;% dplyr::filter( !(Gender %in% c(\"NA\",\"N\")) ) %&gt;% droplevels()\nskimr::skim(d4)\n\n\nData summary\n\n\nName\nd4\n\n\nNumber of rows\n9\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nfactor\n1\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nFavorite Typical Pizza Flavor\n0\n1\n6\n9\n0\n3\n0\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nGender\n0\n1\nFALSE\n3\nM: 4, F: 3, O: 2\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n165.67\n15.98\n133\n156\n166\n178\n183\n▂▁▃▃▇\n\n\nWeight\n0\n1\n70.11\n21.25\n45\n55\n70\n80\n110\n▇▂▃▂▂\n\n\nShoe Size\n0\n1\n7.89\n2.09\n5\n7\n8\n9\n11\n▇▇▃▇▇\n\n\n\n\n\nAll done, data is clean now.\nLet’s assign at the end to some final variable, this makes it easier to add further cleaning steps above.\n\nprocesseddata2 &lt;- d4\n\n\n\nSave data\nFinally, we save the clean data as RDS file. I suggest you save your processed and cleaned data as RDS or RDA/Rdata files. This preserves coding like factors, characters, numeric, etc. If you save as CSV, that information would get lost. However, CSV is better for sharing with others since it’s plain text. If you do CSV, you might want to write down somewhere what each variable is.\nSee here for some suggestions on how to store your processed data: http://www.sthda.com/english/wiki/saving-data-into-r-data-format-rds-and-rdata\n\nsave_data_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"processed-data\",\"processeddata2.rds\")\nsaveRDS(processeddata2, file = save_data_location)\n\nNote the use of the here package and here command to specify a path relative to the main project directory, that is the folder that contains the .Rproj file. Always use this approach instead of hard-coding file paths that only exist on your computer.\n\n\nNotes\nRemoving anyone observation with “faulty” or missing data is one approach. It’s often not the best. based on your question and your analysis approach, you might want to do cleaning differently (e.g. keep observations with some missing information)."
  },
  {
    "objectID": "starter-analysis-exercise/results/readme.html",
    "href": "starter-analysis-exercise/results/readme.html",
    "title": "Hope Grismer's Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains results produced by the code, such as figures and tables.\nDepending on the size and type of your project, you can either place it all in a single folder or create sub-folders. For instance you could create a folder for figures, another for tables. Or you could create a sub-folder for dataset 1, another for dataset 2. Or you could have a subfolder for exploratory analysis, another for final analysis. The options are endless, choose whatever makes sense for your project. For this template, there is just a a single folder, but having sub-folders is often a good idea."
  },
  {
    "objectID": "starter-analysis-exercise/data/raw-data/readme.html",
    "href": "starter-analysis-exercise/data/raw-data/readme.html",
    "title": "Hope Grismer's Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains a simple made-up data-set in an Excel file.\nIt contains the variables Height, Weight and Gender of a few imaginary individuals.\nThe dataset purposefully contains some faulty entries that need to be cleaned.\nGenerally, any dataset should contain some meta-data explaining what each variable in the dataset is. (This is often called a Codebook.) For this simple example, the codebook is given as a second sheet in the Excel file.\nThis raw data-set should generally not be edited by hand. It should instead be loaded and processed/cleaned using code."
  },
  {
    "objectID": "tidytuesday-exercise/tidytuesday-exercise.html",
    "href": "tidytuesday-exercise/tidytuesday-exercise.html",
    "title": "tidytuesday-exercise",
    "section": "",
    "text": "Getting the Data\nHere is the codebook for what the different variables in the table mean:\n\ncare_state.csv\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nstate\ncharacter\nThe two-letter code for the state (or territory, etc) where the hospital is located.\n\n\ncondition\ncharacter\nThe condition for which the patient was admitted. Six categories of conditions are included in the data.\n\n\nmeasure_id\ncharacter\nThe ID of the thing being measured. Note that there are 22 unique IDs but only 21 unique names.\n\n\nmeasure_name\ncharacter\nThe name of the thing being measured. Note that there are 22 unique IDs but only 21 unique names.\n\n\nscore\ndouble\nThe score of the measure.\n\n\nfootnote\ncharacter\nFootnotes that apply to this measure: 5 = “Results are not available for this reporting period.”, 25 = “State and national averages include Veterans Health Administration (VHA) hospital data.”, 26 = “State and national averages include Department of Defense (DoD) hospital data.”.\n\n\nstart_date\ndate\nThe date on which measurement began for this measure.\n\n\nend_date\ndate\nThe date on which measurement ended for this measure.\n\n\n\n\n\nQuestions in Mind Based on Suggestions on GitHub\n\nIs there a connection between state populations and wait times?\nWhich conditions have the longest wait times? The shortest?\n\n\n\nExploratory Data Analysis\n\nWhat was done & why:\n\nI noticed that the measures in the measure_name column were all very different. Without creating subsets of the data, you would be comparing unlabeled scores that are not measuring the same thing. In this data set especially, this would be very adverse as some of the scores are ideally higher and some ideally are lower.\n\n\nI subsetted the data as a result in measures I was curious about.\n\n\nPerfect. Now I wanted to save these sets to further my EDA.\n\nlibrary(readr)\n\n# Save Subset 1\nwrite_csv(time_b4_leaving, here::here(\"tidytuesday-exercise\", \"data\", \"time_b4_leaving_care_state.csv\"))\n\n# Save Subset 2\nwrite_csv(time_b4_leaving_psych, here::here(\"tidytuesday-exercise\", \"data\", \"time_b4_leaving_psych_care_state.csv\"))\n\n# Save Subset 3\nwrite_csv(time_b4_discharge, here::here(\"tidytuesday-exercise\", \"data\", \"time_b4_discharge.csv\"))\n\n\n\nTime to take a look at this subsets but in the context of states as well.\n\n\nVery smushed, let’s try again.\n\n\nThe state variation is glaring in whole with some states being firmly in the 100 minutes and some states being up into the 300 minute range (not good). The time before discharge graphic also showed not only large interstate differences but also large intrastate differences. For example, NM (New Mexico) had a glaringly large range. The same is true with IN, DC, ME, DE. Curious to know if these states range most highly in SES disparity or insurance coverage??\n\n\nThis table was helpful to print out the largest range states that we saw in the graph. But I included another iteration of this table because I saw that some missing data created negative values (-Inf). This meant that the smallest range ones in the table did not necessary show the smallest ranges in discharge time but just negative calculation.\n\n\nSo, next, I thought it would be neat to try it out on a map.\n\n\nTo do some modeling for my question on whether income inequality is directly the reason for the variablity in ranges between states, I found some Gini Index data to compare to my Time to Discharge Data.\n\n# Load necessary libraries\nlibrary(dplyr)\nlibrary(readr)\nlibrary(stringr)\n\n# Load Gini Index data\ngini_data &lt;- read_csv(here(\"tidytuesday-exercise\", \"data\", \"gini-index-data.csv\"))\n\nRows: 1 Columns: 105\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (53): Label (Grouping), Alabama!!Margin of Error, Alaska!!Margin of Erro...\ndbl (52): Alabama!!Estimate, Alaska!!Estimate, Arizona!!Estimate, Arkansas!!...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ncolnames(gini_data)\n\n  [1] \"Label (Grouping)\"                     \n  [2] \"Alabama!!Estimate\"                    \n  [3] \"Alabama!!Margin of Error\"             \n  [4] \"Alaska!!Estimate\"                     \n  [5] \"Alaska!!Margin of Error\"              \n  [6] \"Arizona!!Estimate\"                    \n  [7] \"Arizona!!Margin of Error\"             \n  [8] \"Arkansas!!Estimate\"                   \n  [9] \"Arkansas!!Margin of Error\"            \n [10] \"California!!Estimate\"                 \n [11] \"California!!Margin of Error\"          \n [12] \"Colorado!!Estimate\"                   \n [13] \"Colorado!!Margin of Error\"            \n [14] \"Connecticut!!Estimate\"                \n [15] \"Connecticut!!Margin of Error\"         \n [16] \"Delaware!!Estimate\"                   \n [17] \"Delaware!!Margin of Error\"            \n [18] \"District of Columbia!!Estimate\"       \n [19] \"District of Columbia!!Margin of Error\"\n [20] \"Florida!!Estimate\"                    \n [21] \"Florida!!Margin of Error\"             \n [22] \"Georgia!!Estimate\"                    \n [23] \"Georgia!!Margin of Error\"             \n [24] \"Hawaii!!Estimate\"                     \n [25] \"Hawaii!!Margin of Error\"              \n [26] \"Idaho!!Estimate\"                      \n [27] \"Idaho!!Margin of Error\"               \n [28] \"Illinois!!Estimate\"                   \n [29] \"Illinois!!Margin of Error\"            \n [30] \"Indiana!!Estimate\"                    \n [31] \"Indiana!!Margin of Error\"             \n [32] \"Iowa!!Estimate\"                       \n [33] \"Iowa!!Margin of Error\"                \n [34] \"Kansas!!Estimate\"                     \n [35] \"Kansas!!Margin of Error\"              \n [36] \"Kentucky!!Estimate\"                   \n [37] \"Kentucky!!Margin of Error\"            \n [38] \"Louisiana!!Estimate\"                  \n [39] \"Louisiana!!Margin of Error\"           \n [40] \"Maine!!Estimate\"                      \n [41] \"Maine!!Margin of Error\"               \n [42] \"Maryland!!Estimate\"                   \n [43] \"Maryland!!Margin of Error\"            \n [44] \"Massachusetts!!Estimate\"              \n [45] \"Massachusetts!!Margin of Error\"       \n [46] \"Michigan!!Estimate\"                   \n [47] \"Michigan!!Margin of Error\"            \n [48] \"Minnesota!!Estimate\"                  \n [49] \"Minnesota!!Margin of Error\"           \n [50] \"Mississippi!!Estimate\"                \n [51] \"Mississippi!!Margin of Error\"         \n [52] \"Missouri!!Estimate\"                   \n [53] \"Missouri!!Margin of Error\"            \n [54] \"Montana!!Estimate\"                    \n [55] \"Montana!!Margin of Error\"             \n [56] \"Nebraska!!Estimate\"                   \n [57] \"Nebraska!!Margin of Error\"            \n [58] \"Nevada!!Estimate\"                     \n [59] \"Nevada!!Margin of Error\"              \n [60] \"New Hampshire!!Estimate\"              \n [61] \"New Hampshire!!Margin of Error\"       \n [62] \"New Jersey!!Estimate\"                 \n [63] \"New Jersey!!Margin of Error\"          \n [64] \"New Mexico!!Estimate\"                 \n [65] \"New Mexico!!Margin of Error\"          \n [66] \"New York!!Estimate\"                   \n [67] \"New York!!Margin of Error\"            \n [68] \"North Carolina!!Estimate\"             \n [69] \"North Carolina!!Margin of Error\"      \n [70] \"North Dakota!!Estimate\"               \n [71] \"North Dakota!!Margin of Error\"        \n [72] \"Ohio!!Estimate\"                       \n [73] \"Ohio!!Margin of Error\"                \n [74] \"Oklahoma!!Estimate\"                   \n [75] \"Oklahoma!!Margin of Error\"            \n [76] \"Oregon!!Estimate\"                     \n [77] \"Oregon!!Margin of Error\"              \n [78] \"Pennsylvania!!Estimate\"               \n [79] \"Pennsylvania!!Margin of Error\"        \n [80] \"Rhode Island!!Estimate\"               \n [81] \"Rhode Island!!Margin of Error\"        \n [82] \"South Carolina!!Estimate\"             \n [83] \"South Carolina!!Margin of Error\"      \n [84] \"South Dakota!!Estimate\"               \n [85] \"South Dakota!!Margin of Error\"        \n [86] \"Tennessee!!Estimate\"                  \n [87] \"Tennessee!!Margin of Error\"           \n [88] \"Texas!!Estimate\"                      \n [89] \"Texas!!Margin of Error\"               \n [90] \"Utah!!Estimate\"                       \n [91] \"Utah!!Margin of Error\"                \n [92] \"Vermont!!Estimate\"                    \n [93] \"Vermont!!Margin of Error\"             \n [94] \"Virginia!!Estimate\"                   \n [95] \"Virginia!!Margin of Error\"            \n [96] \"Washington!!Estimate\"                 \n [97] \"Washington!!Margin of Error\"          \n [98] \"West Virginia!!Estimate\"              \n [99] \"West Virginia!!Margin of Error\"       \n[100] \"Wisconsin!!Estimate\"                  \n[101] \"Wisconsin!!Margin of Error\"           \n[102] \"Wyoming!!Estimate\"                    \n[103] \"Wyoming!!Margin of Error\"             \n[104] \"Puerto Rico!!Estimate\"                \n[105] \"Puerto Rico!!Margin of Error\"         \n\n# Select only columns ending with '!!Estimate'\ngini_estimates_only &lt;- gini_data %&gt;%\n  select(matches(\"!!Estimate$\"))\n\n#Extract the first (and only) row containing the Gini Index values\ngini_values &lt;- gini_estimates_only[1, ]\n\n#Pivot the data to long format\ngini_long &lt;- gini_values %&gt;%\n  pivot_longer(\n    cols = everything(),\n    names_to = \"state\",\n    values_to = \"gini_estimate\"\n  ) %&gt;%\n  mutate(\n    # Remove the '!!Estimate' suffix to get clean state names\n    state = str_remove(state, \"!!Estimate$\")\n  )\n\n# View the resulting tidy data\nprint(gini_long)\n\n# A tibble: 52 × 2\n   state                gini_estimate\n   &lt;chr&gt;                        &lt;dbl&gt;\n 1 Alabama                      0.477\n 2 Alaska                       0.449\n 3 Arizona                      0.465\n 4 Arkansas                     0.474\n 5 California                   0.487\n 6 Colorado                     0.458\n 7 Connecticut                  0.495\n 8 Delaware                     0.456\n 9 District of Columbia         0.516\n10 Florida                      0.483\n# ℹ 42 more rows\n\n\n\n\nSo now I have my Gini Index data where I want it with my Estimates of 2023 Gini Index as a proxy for income inequality and I want to merge it with my range data I had before for each state.\n\n# Load necessary libraries\nlibrary(dplyr)\nlibrary(tidyr)\n\n# Create a state reference table using built-in vectors\nstate_reference &lt;- data.frame(\n  state_name = state.name,\n  state_abbr = state.abb,\n  stringsAsFactors = FALSE\n)\n\n# Print the state reference table to confirm it exists\nhead(state_reference)\n\n  state_name state_abbr\n1    Alabama         AL\n2     Alaska         AK\n3    Arizona         AZ\n4   Arkansas         AR\n5 California         CA\n6   Colorado         CO\n\n# Join the state reference to gini_long to add state abbreviations\ngini_data &lt;- gini_long %&gt;%\n  left_join(state_reference, by = c(\"state\" = \"state_name\"))\n\n# Check the resulting gini_data with the state abbreviation added\nhead(gini_data)\n\n# A tibble: 6 × 3\n  state      gini_estimate state_abbr\n  &lt;chr&gt;              &lt;dbl&gt; &lt;chr&gt;     \n1 Alabama            0.477 AL        \n2 Alaska             0.449 AK        \n3 Arizona            0.465 AZ        \n4 Arkansas           0.474 AR        \n5 California         0.487 CA        \n6 Colorado           0.458 CO        \n\n\n\n\nMerging the range (difference between the minimum and maximum time to discharge) data and the Gini coefficent data.\n\n\nTo quickly check whether there’s a correlation between range (the intrastate difference between the highest and lowest times before discharge - the outcome) and Gini_Estimate (predictor), here is a scatterplot with a trend line.\n\nggplot(combined_data, aes(x = gini_estimate, y = range)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = TRUE, color = \"blue\") +\n  labs(\n    title = \"Relationship between Gini Estimate and Range\",\n    x = \"Gini Estimate\",\n    y = \"Range (Time Before Discharge)\"\n  ) +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 1 row containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nHypothesis: “States with higher Gini coefficients (indicating more economic inequality) will have longer average times before discharge in hospitals due to economic barriers affecting healthcare access.”\nAssuming that range is my “outcome” and that Gini coefficents act as a predictor.\n\n\n\n\nSetting up some models\n\nNull model\nLinear regression model\nRandom forest model\n\nNull Model\nLinear Regression Model\nRandom Forest\n\n#For cross validation and workflow set up \n\nset.seed(234)\ncv_folds &lt;- vfold_cv(train_data, v = 5)\nnull_workflow &lt;- workflow() %&gt;%\n  add_model(null_model) %&gt;%\n  add_recipe(range_recipe)\n\nlm_workflow &lt;- workflow() %&gt;%\n  add_model(lm_model) %&gt;%\n  add_recipe(range_recipe)\n\nrf_workflow &lt;- workflow() %&gt;%\n  add_model(rf_model) %&gt;%\n  add_recipe(range_recipe)\n\nnull_res &lt;- fit_resamples(null_workflow, resamples = cv_folds)\n\n→ A | warning: A correlation computation is required, but `estimate` is constant and has 0\n               standard deviation, resulting in a divide by 0 error. `NA` will be returned.\n\n\nThere were issues with some computations   A: x1\n\n\nThere were issues with some computations   A: x5\n\n\n\n\nlm_res &lt;- fit_resamples(lm_workflow, resamples = cv_folds)\nrf_res &lt;- fit_resamples(rf_workflow, resamples = cv_folds)\n\ncollect_metrics(null_res)\n\n# A tibble: 2 × 6\n  .metric .estimator  mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard    22.1     5    6.45 Preprocessor1_Model1\n2 rsq     standard   NaN       0   NA    Preprocessor1_Model1\n\ncollect_metrics(lm_res)\n\n# A tibble: 2 × 6\n  .metric .estimator   mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard   23.0       5   6.79  Preprocessor1_Model1\n2 rsq     standard    0.253     5   0.135 Preprocessor1_Model1\n\ncollect_metrics(rf_res)\n\n# A tibble: 2 × 6\n  .metric .estimator   mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard   28.3       5   5.61  Preprocessor1_Model1\n2 rsq     standard    0.199     5   0.136 Preprocessor1_Model1\n\nprint(null_res)\n\n# Resampling results\n# 5-fold cross-validation \n# A tibble: 5 × 4\n  splits         id    .metrics         .notes          \n  &lt;list&gt;         &lt;chr&gt; &lt;list&gt;           &lt;list&gt;          \n1 &lt;split [27/7]&gt; Fold1 &lt;tibble [2 × 4]&gt; &lt;tibble [1 × 3]&gt;\n2 &lt;split [27/7]&gt; Fold2 &lt;tibble [2 × 4]&gt; &lt;tibble [1 × 3]&gt;\n3 &lt;split [27/7]&gt; Fold3 &lt;tibble [2 × 4]&gt; &lt;tibble [1 × 3]&gt;\n4 &lt;split [27/7]&gt; Fold4 &lt;tibble [2 × 4]&gt; &lt;tibble [1 × 3]&gt;\n5 &lt;split [28/6]&gt; Fold5 &lt;tibble [2 × 4]&gt; &lt;tibble [1 × 3]&gt;\n\nThere were issues with some computations:\n\n  - Warning(s) x5: A correlation computation is required, but `estimate` is constant...\n\nRun `show_notes(.Last.tune.result)` for more information.\n\nprint(lm_res)\n\n# Resampling results\n# 5-fold cross-validation \n# A tibble: 5 × 4\n  splits         id    .metrics         .notes          \n  &lt;list&gt;         &lt;chr&gt; &lt;list&gt;           &lt;list&gt;          \n1 &lt;split [27/7]&gt; Fold1 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n2 &lt;split [27/7]&gt; Fold2 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n3 &lt;split [27/7]&gt; Fold3 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n4 &lt;split [27/7]&gt; Fold4 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n5 &lt;split [28/6]&gt; Fold5 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n\nprint(rf_res)\n\n# Resampling results\n# 5-fold cross-validation \n# A tibble: 5 × 4\n  splits         id    .metrics         .notes          \n  &lt;list&gt;         &lt;chr&gt; &lt;list&gt;           &lt;list&gt;          \n1 &lt;split [27/7]&gt; Fold1 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n2 &lt;split [27/7]&gt; Fold2 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n3 &lt;split [27/7]&gt; Fold3 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n4 &lt;split [27/7]&gt; Fold4 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n5 &lt;split [28/6]&gt; Fold5 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n\n\n\nTakeaways\n\nRMSE (Root Mean Squared Error): Lower values indicate better model performance.\n\nThe null model has an RMSE of 22.1.\nThe linear mode (lm_res) has an RMSE of 23.0.\nThe random forest model (rf_res) has an RMSE of 28.3.\n\nR-squared (rsq): R-squared values range from 0 to 1, with values closer to 1 indicating a better fit. In your case:\n\nThe null model has no R-squared value (NaN).\nThe linear model has an R-squared of 0.253, which means about 25.3% of the variance in range can be explained by the Gini_Estimate.\nThe random forest model has an R-squared of 0.199, indicating that this model explains about 19.9% of the variance in range.\n\n\n\n\nModel Comparison\n\nNull Model: This is typically a baseline model with no predictors. It’s interesting to see that your null model has the lowest RMSE (22.1), which suggests that the other models are not doing better in terms of RMSE.\nLinear Model (lm_res): The linear model explains 25.3% of the variance, but its RMSE is slightly worse than the null model (23.0).\nRandom Forest (rf_res): The random forest has the highest RMSE (28.3) and also explains the least amount of variance (19.9%).\nThe Linear Model (lm_model):\n\nThe linear model (lm_model) incorporates the Gini Estimate as the sole predictor.\nPerformance: The RMSE of 23.0 is relatively close to the null model. However, the R² of 0.253 suggests that there is some weak relationship between Gini Estimate and time before discharge.\nReason for inclusion: This model is simple, interpretable, and provides some insight into the relationship, but there is room for improvement in terms of performance.\n\n\n\n\nEvaluation on Test Data\nNow that the model has been chosen, we will evaluate it on the test data (which was set aside earlier). This is the final evaluation that will provide an honest assessment of the model’s ability to generalize to new, unseen data.\n\n# Fit the final linear model on the full training data\nfinal_lm_fit &lt;- lm_workflow %&gt;%\n  fit(data = train_data)\n\n# Predict on the test set\ntest_data_pred &lt;- predict(final_lm_fit, new_data = test_data)$.pred\n\n# Calculate RMSE and R-squared\ntest_rmse &lt;- sqrt(mean((test_data_pred - test_data$range)^2))\ntest_rsq &lt;- 1 - sum((test_data_pred - test_data$range)^2) / sum((mean(test_data$range) - test_data$range)^2)\n\ntest_rmse\n\n[1] 18.93827\n\ntest_rsq\n\n[1] -0.01457361\n\n# Plot residuals\nlm_residuals &lt;- test_data$range - test_data_pred\nplot(lm_residuals)\n\n\n\n\n\n\n\n# Create a tibble for plotting\nresidual_df &lt;- tibble(\n  predicted = test_data_pred,\n  residuals = lm_residuals\n)\n\n# Residual plot\nresidualsplot &lt;- ggplot(residual_df, aes(x = predicted, y = residuals)) +\n  geom_point() +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\") +\n  labs(\n    title = \"Residual Plot\",\n    x = \"Predicted Range\",\n    y = \"Residuals\"\n  ) +\n  theme_minimal()\n\nNo clear pattern in the residuals which is a good sign for the model.\nSource: Data Science Learning Community (2024). Tidy Tuesday: A weekly social data project. https://tidytues.day"
  }
]